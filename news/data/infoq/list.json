{
    "data": [
        {
            "id": "DWyEL9LsfxEV8JW5eZWU",
            "title": "火热报名中！2025 龙蜥操作系统大会亮点速递",
            "image": "https://static001.infoq.cn/resource/image/61/89/614eb2533cf3f6c55d1438ed9cf7f389.jpg",
            "description": "<div><p>Hi，小伙伴们，好久不见，很高兴再次与大家相聚在北京！</p><p></p><p>2025 龙蜥操作系统大会（简称“2025龙蜥大会”）将于 11 月 17 日在北京·星地艺术中心盛大召开，主题为“生态共融·智驱未来”，汇聚全球操作系统领域的前沿探索与最佳实践，诚邀产业共建者一起，洞悉行业机遇，把握产业脉动，领略技术跃迁，聆听生态蝶变。</p><p></p><p>作为全程蹲守筹备一线的“情报官”小龙，今天就带着满满诚意，为大家抢先剧透一波：更硬核的技术论坛、更沉浸的动手体验、更闪耀的生态阵容……统统安排上！</p><p></p><p>那 2025 龙蜥大会到底有哪些新亮点？小龙沉浸式您你揭晓：</p><p></p><p><strong>● 大咖齐聚</strong>：20+ 院士学者、企业领袖、业界翘楚，超 300 家企业和逾千位行业开发者。</p><p></p><p>●<strong> 前沿探索</strong>：1 个产业主论坛、5 个技术分论坛，深入探讨 AI 时代下操作系统产业发展的新趋势、新挑战与新路径。</p><p></p><p>● <strong>生态共荣</strong>：10+ 个特色展区/开发者活动，推动技术融合，构建更加开放、协同、繁荣的产业生态</p><p></p><p>● <strong>干货满满</strong>：60+ 场前沿技术分享，聚焦技术融合、生态共建、人才培养等议题。</p><p></p><p><img>https://static001.geekbang.org/infoq/e9/e9050dc316ea470d706b4dcb898e9e8e.jpeg<img></p><p></p><p><img>https://static001.geekbang.org/infoq/23/23b0589b80b4f2db74e809e2f1ce7b74.jpeg<img></p><p></p><p><img>https://static001.geekbang.org/infoq/dc/dc0f175c747cbd9d78bd2e920b7d6cf2.jpeg<img></p><p></p><p><img>https://static001.geekbang.org/infoq/52/5208077ab26f832866bd56c66211f721.jpeg<img></p><p>前 100 名报名且现场签到的小伙伴可在现场“龙蜥礼物岛”领取限量社区周边一份，五周年定制冰箱贴、帆布袋等可选。欢迎大家速速报名，共赴操作系统产业的年度盛会！</p><p></p><p>报名链接（复制链接至浏览器打开）：</p><p>https://hd.aliyun.com/form/6795</p><p></p><p>2025 龙蜥操作系统大会官网链接（复制链接至浏览器打开或点击阅读原文直达）：</p><p>https://openanolis.cn/openanolisconference2025</p></div>",
            "link": "https://www.infoq.cn/article/DWyEL9LsfxEV8JW5eZWU",
            "pub_date": "2025-10-31 04:07:41",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "EMmU8fChp9qiNjRCLjQZ",
            "title": "深度探讨“云+智能计算”，智算新基础设施分论坛议程揭晓 | 2025 龙蜥大会",
            "image": "https://static001.infoq.cn/resource/image/73/01/737fc57061fb38c67ac345b1c2beb501.jpg",
            "description": "<div><p><strong>2025 龙蜥操作系统大会将于 11 月 17 日在北京·星地艺术中心举办，由中国计算机学会开源发展技术委员会、泛在操作系统开放社区、中关村科技园区朝阳园管理委员会（北京市朝阳区科学技术和信息化局）、中国开源软件推进联盟指导，龙蜥社区主办，中关村互联网 3.0 产业园（星地中心）协办，阿里云、中兴通讯、海光信息、Intel、浪潮信息、Arm 等 24 家理事单位共同承办，主题为“生态共融·智驱未来”， 云集逾千位全球技术领袖、业界精英和行业开发者，深入探讨 AI 时代下操作系统产业发展的新趋势、新挑战与新路径。大会将聚焦技术融合、生态共建、人才培养等核心议题，致力于推动操作系统与AI、云计算、大数据等前沿技术的深度融合，打造政、产、学、研多方深度对话的年度盛会，构建更加开放、协同、繁荣的产业生态体系。</strong></p><p></p><p>当前，人工智能正加速演进，智算基础设施作为支撑人工智能技术发展和产业落地的底座已成为驱动技术创新与产业变革的核心引擎。本论坛以<strong>“共建智算新基础设施”</strong>为主题，汇聚企业、科研机构及产业生态各方力量，共同探讨云+智能计算的产业进展、前沿技术创新与生态建设，旨在凝聚共识、激发创新。</p><p></p><p>本论坛由龙蜥社区主办，龙蜥社区智算基础设施联盟、Intel 、中国智算产业联盟、浪潮信息、大普微企业等联合承办，欢迎来自产业、高校的智算系统领域工程师、科研人员、专家和架构师分享精彩的技术演讲与生态实践经验，共同探索和构建高效、普惠的智能算力新基础设施。</p><p></p><p><strong>时间：</strong>2025.11.17 13:30-16:35</p><p></p><p><strong>地点：</strong>北京·星地艺术中心 D 座 5F 会议室 2</p><p></p><p><strong>报名链接</strong>（复制链接至浏览器打开）：</p><p><italic>https://hd.aliyun.com/form/6795</italic></p><p></p><p>会议议程详见下方海报：</p><p></p><p><img>https://static001.geekbang.org/infoq/7d/7d9d50ea577717b348a49542b813902d.webp<img></p><p></p><p>相关分论坛议程链接：</p><p></p><p>更多 2025 龙蜥大会详情，请点击大会官网链接查看（复制链接至浏览器打开或点击阅读原文）：</p><p><italic>https://openanolis.cn/openanolisconference225</italic></p></div>",
            "link": "https://www.infoq.cn/news/EMmU8fChp9qiNjRCLjQZ",
            "pub_date": "2025-10-31 02:42:12",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "WzV23TlWBq6F7Qsi2zz4",
            "title": "AWS ALB 现已原生支持 URL 与主机头重写功能",
            "image": "https://static001.infoq.cn/resource/image/9a/1e/9a5eab18f719b8160fc42319ab45531e.jpg",
            "description": "<div><p><color>亚马逊云（AWS）近日宣布，</color><color> ALB 现已全面支持原生的 URL 与 Host Header（主机头）重写功能。有了这一新特性，用户无不再需要依赖自定义的应用逻辑，也不用部署和维护额外的第三方代理解决方案（例如 NGINX Ingress Controller）来管理 Layer 7 流量的修改。</color></p><p><color> </color></p><p><color>在这项功能推出之前，如果企业需要复杂的请求路由功能，比如在</color><color>，就必须在架构中引入额外的代理层。这些额外的组件不仅增加了维护负担，往往也会带来不必要的延迟。</color></p><p><color> </color></p><p><color>在 Reddit 上的</color><color>中，这一变化得到了开发者们的肯定。有用户评论指出，最大的亮点是：</color></p><p><color> </color></p><p><color>修改 URL 路径和主机头的功能通过正则表达式（regex）来匹配条件，在 ALB 内部有原生的支持。用户可以在</color><color>或</color><color> ALB 上进行配置，无论是通过 </color><color>、</color><color> </color><color>还是</color><color> </color><color> 都可以实现，对流向后端服务（如 EC2 实例、容器或 Lambda 函数）的流量进行精细化控制。</color></p><p><color> </color></p><p></p><p><img>https://static001.geekbang.org/infoq/6c/6c46068f88a9f9c8d0ddfd8df44d0bcb.png<img></p><p></p><p><color>图源：AWS Networking & Content Delivery </color></p><p><color> </color></p><p><color>此外，ALB 规则中新增的 Transforms（转换）部分，允许在请求到达目标组之前，对进入的请求进行修改。一个常见的使用场景就是调整路径前缀，例如将旧的 API 版本路径 </color><color> 替换为新的 </color><color>。</color></p><p><color> </color></p><p></p><p><img>https://static001.geekbang.org/infoq/79/79ade952a074c05a2dcc3a750dc50c5c.png<img></p><p></p><p><color>图源：AWS Networking & Content Delivery </color></p><p></p><p><color>AWS 的 Serverless Hero Luc van Donkersgoed 在</color><color>上评论道：</color></p><p></p><p><color>随着这一功能的上线，AWS 也正式加入了其他已提供类似 </color><color> 原生能力的主要云服务商行列。例如，谷歌云（GCP）的</color><color> 提供强大的 URL 和头修改功能，支持在请求进入后端前重写主机和路径；而微软 Azure 则在区域范围内通过 Azure Application Gateway 提供</color><color>，在全球范围则通过 </color><color>（Rule Engine）支持更复杂、基于模式的流量控制。</color></p><p></p><p><color>用户“A Snark bot from lastweekinaws”在 Bluesky 上</color><color>道：</color></p><p><color> </color></p><p><color>最后，这项功能目前已在所有 AWS 商业区、AWS GovCloud（美国）以及 AWS 中国区域全面上线。关于 AWS ALB 的更多详细信息，可在</color><color>查看。</color></p><p></p><p>原文链接：</p></div>",
            "link": "https://www.infoq.cn/article/WzV23TlWBq6F7Qsi2zz4",
            "pub_date": "2025-10-31 01:45:28",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "e7QqfyYa9GebHrSC7IYt",
            "title": "重磅！华为开源业界首个Serverless分布式计算引擎openYuanrong，单机体验编程、极致分布式运行性能",
            "image": "https://static001.infoq.cn/resource/image/5d/66/5dd649e659e79158f6ca0ea5d6ae7366.jpg",
            "description": "<div><p>作者 | openYuanrong 团队</p><p>策划 | 华卫</p><p></p><p>近期，华为开源了自研的 Serverless 分布式计算引擎 openYuanrong。openYuanrong 已经在华为 MetaERP、小艺、华为云、终端云、ICT、海思等核心产品和平台广泛使用，是华为面向分布式计算领域长期积累下来的核心竞争力产品，也是业界首个统一支持通用计算和 AI 智能计算、灵衢超节点多种分布式应用场景的开源 Serverless 分布式计算引擎。同时，工商银行参与了openYuanrong建设，在交易对手风险计量场景应用。该场景用蒙特卡洛模拟，基于该引擎实现CPU+GPU异构算力融合计算，对比Ray性能有一定提升。下阶段华为与工行围绕分布式推理、强化学习场景深化联创。</p><p></p><p>开源地址：</p><p></p><p></p><p>Serverless 计算以其单机体验编程、自动弹性扩缩、免运维、和屏蔽复杂基础设施等优点广受业界关注，但传统的 Serverless 产品（以 FaaS 为代表）大多是面向特定垂直领域定制、缺乏通用性。因此，面对复杂的应用，往往需要和现有微服务（容器）集群混合部署。如此便会造成多个基于异构软件技术栈的异构集群之间的互操作效率低下、集成和管理复杂。不仅如此，随着 AI 技术的普及，以及 AI Agent 新应用的兴起，通算和智算集群之间也需要相互集成，当前的各个集群软件之间缺乏协同，也带来开发运维复杂、集群资源割裂、跨系统数据流转开销大等严重问题。而 openYuanrong 则致力于以统一的 Serverless 技术栈支持上述各类通算和智算负载在统一资源池上的细粒度融合部署，实现各负载间的高效协同和资源复用，同时提高系统性能，提升计算资源的使用效率。</p><p></p><h2>经过华为核心场景锻造的 Serverless 分布式计算引擎</h2><p></p><ul><li><p>通用计算场景。华为 MetaERP 以 openYuanrong 为分布式底座构建业界首个 Serverless 的 ERP 系统。MetaERP 为华为公司整体提供 ERP 服务，包含上千个微服务、日处理百 TB 级数据，是目前国内最大的 ERP 系统之一。MetaERP 使用 openYuanrong 提供零代码修改方案将大量使用 Spring 开发的标准微服务直接进行 Serverless 化，和 FaaS 原生的扩展服务实现共集群部署。openYuanrong 的快照极速冷启动等方案使得大型 Java 微服务启动时延从 90s 缩短至 1.4s，满足了 Serverless 自动水平弹性，以及无请求缩容到 0 的需求。同时，MetaERP 还采用了 openYuanrong 提供的 Serverless 流处理方案，实现性能优于 Flink 的大数据处理流水线。通过 openYuanrong 提供的 Serverless 技术，MetaERP 整体提升开发效率 60%，资源成本节省 30％。</p></li></ul><p></p><p><img>https://static001.geekbang.org/wechat/images/22/2222dfc7fe575481ab2cc70caea955e8.jpeg<img></p><p></p><ul><li><p>智能计算场景。华为小艺服务广泛采用 openYuanrong 作为其云端 AI 训 - 推 - 用的 Serverless AI 基础设施。传统方案下，SFT 训练、强化学习、大模型推理、AI Agent 分别采用不同的软件技术栈，独立集群部署（如在训推场景中使用 Ray，而 Agent 实例则运行在 K8S 管理的容器中），带来了大量的开发运维成本。小艺基于 openYuanrong 在单一资源池上构建了统一的 Serverless AI 平台，同时支持了 SFT 训练、强化学习、模型推理、AI Agent 四个领域负载的融合部署和资源灵活调度，在极大简化了分布式系统的开发运维的同时提升了资源利用率。利用 openYuanrong 的分布式内核技术，小艺的强化学习训推任务调度端到端时延减半；大模型推理实例启动速度从分钟级下降到秒级（如 Llama2-70B 水平弹性时延从 571 秒缩短至 4.55 秒）。基于 openYuanrong 的异构分布式内存数据系统，分布式 KV Cache 整体性能提升 1 倍以上，同时强化学习过程中，训推转换时实例间参数同步时延也从分钟级缩短至秒级。相较于业界类似产品（如 Ray），openYuanrong 提供了更高的性能和弹性调度能力，更成熟稳定，同时也支持通算和智算负载的融合部署，构建全流程的 Serverless AI 平台。</p></li></ul><p></p><p><img>https://static001.geekbang.org/wechat/images/fa/fa82b171fc3c2cf7d40938b811d46fe4.png<img></p><p></p><h2>openYuanrong 的整体架构</h2><p></p><p>openYuanrong 从架构上看主要由三部分组成：</p><p></p><ul><li><p>多语言函数运行时：一套以函数、状态、数据对象、数据流为核心抽象的分布式编程接口和高效实现，其中“函数”类似单机 OS“进程”，可以表达任意分布式应用的运行实例，并天然支持相互间直连互调或通过分布式内存共享传递数据，无需经过网关中转或外部存储。支持 Python、Java、C++ 等主流编程语言及相互间跨语言调用，使能用户以单机体验的编程方式高效开发各类分布式应用。</p></li></ul><ul><li><p>函数系统：Serverless 大规模分布式动态调度，支持函数动态生命周期管理（函数实例可在运行过程中动态创建 / 删除、长时间运行、休眠 / 唤醒），支持包括 Java 微服务、大模型推理在内的各类应用实例秒级冷启动和快速水平弹性，支持实例自动垂直弹性和跨节点迁移，实现节点内高密部署和集群资源高效利用。</p></li></ul><ul><li><p>数据系统：近计算的异构分布式内存数据缓存，提供 Object、Stream、KV 等语义，实现函数实例间高性能数据共享及传递。通过 HBM/DDR/SSD 多级缓存，支持节点内基于共享内存的免拷贝数据访问、节点间基于 D2D/H2H/D2H/H2D 高速传输的高效分布式数据访问。</p></li></ul><p></p><p><img>https://static001.geekbang.org/wechat/images/8c/8c030ac0c51d87d5ec74e664738a0710.png<img></p><p></p><p>本质上 openYuanrong 提供了一套统一 Serverless 架构支持各类分布式计算场景，通过提供多语言函数编程接口，以单机编程体验简化分布式应用开发；通过提供分布式动态调度和数据共享等能力，实现分布式应用的高性能运行和集群的高效资源利用。</p><p></p><h2>openYuanrong 与业界同类系统的异同点</h2><p></p><p>业界同类系统有 Ray、AWS Lambda 等。和 Ray 相比，Ray 和 openYuanrong 都支持面向 Python 的“单机体验编程、分布式运行”；但和 Ray 主要面向 Python 编程语言不同，openYuanrong 采用多语言运行时和分布式内核分离的架构，能够支持更丰富的编程语言，也支持更大规模集群调度，以及极致水平 / 垂直弹性和跨节点迁移。同时，openYuanrong 也构建了比 Ray Object Store 性能更高、功能更全的分布式内存数据系统，实现高性能数据共享 / 流转。此外，openYuanrong 的分布式管理架构也支持分布式高可靠运行，消除了单点故障。和 AWS Lambda 等 FaaS 系统相比，FaaS 只适用于特定的工作负载，而 openYuanrong 是业界首个将 Serverless 从专用场景扩展到通用分布式场景的系统。</p><p></p><p>openYuanrong 已在 openEuler 社区全面开源，采用 Apache 2.0 License，期待更多优秀的开发者参与，共同定义智能时代的分布式计算新范式。</p></div>",
            "link": "https://www.infoq.cn/article/e7QqfyYa9GebHrSC7IYt",
            "pub_date": "2025-10-29 09:30:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "ZCe0w3QBY1NZPbrAja8E",
            "title": "IDC 最新AI Infra报告解读｜AI 云爆发的背后：AI 应用全球化势不可挡",
            "image": "https://static001.infoq.cn/resource/image/89/43/89450fe1ca9656ee2b00404548888943.jpg",
            "description": "<div><p><img>https://static001.infoq.cn/resource/image/ef/a8/efbc7726f884035803f978c44b350aa8.png<img></p><p>近期 IDC 发布了一份《AI 原生云 / 新型云厂商重构 Agentic 基础设施》报告，报告中的调研数据显示：87% 的亚太企业在 2024 年至少部署了 10 个 GenAI 场景，预计到 2025 年这一数字将上升到近 94%。此外，IDC 调查发现，亚太地区日常使用 GenAI 的消费者从 2024 年的 19% 增加到 2025 年的 30%，企业 GenAI 的采用率也激增，65% 的亚太区企业将有超过 50 个 GenAI 场景投入生产，预计到 2025 年，26% 的企业将拥有超过 100 个应用。IDC 预测，到 2028 年中等以上的规模企业当中，至少会有上百个智能体在运转。</p><p></p><p>在此背景下，企业需要重新思考其 AI 基础设施，是否足以应对即将到来的智能体协同时代。这场变革不仅关乎技术升级，更是一场关于商业模式、市场格局和全球战略的深度博弈。</p><p></p><p>为了帮助更多 AI 从业者、企业决策者理解这份报告的核心内容，InfoQ 特别邀请了 IDC 中国研究总监卢言霞、GMI Cloud 创始人 & CEO Alex Yeh，从数据洞察与实战视角拆解报告，解读亚太地区 AI 基础设施的新趋势。</p><p></p><p><img>https://static001.geekbang.org/infoq/26/2613c5b8ac674be323f251cb41b816db.webp<img></p><p></p><p><italic>以下为经整理的直播内容精要。</italic></p><p></p><p><img>https://static001.infoq.cn/resource/image/dc/2b/dc0ac9e3ef27ba6384c00f6ac063802b.png<img></p><p></p><p>AI 应用构建浪潮扑面而来，但技术栈深海处的「链式反应」，可能比表面热浪更具颠覆性。</p><p>IDC 指出，自 ChatGPT 2022 年发布以来，大型互联网公司以及部分初创企业聚焦在大模型训练，以期在基础模型领域占据市场领先地位。因此在 2022-2024 年间，AI 基础设施的投资更多聚焦在模型训练侧。进入 2025 年，大模型的预训练开始收敛，市场的焦点更多在于模型推理侧，AI 推理专用基础设施需求激增。</p><p></p><p>IDC 报告指出，在亚太市场， 2023 年只有 40% 的组织使用人工智能推理基础设施，但在 2025 年，这一数字增长到 84%。这一数据标志着 AI 产业正在从模型开发阶段进入大规模应用落地阶段。</p><p></p><p><strong>这种变化一方面正在模糊传统技术栈的界限，另一方面也催生出专为 AI 工作负载优化的新型云服务——AI Native Cloud.</strong></p><p></p><p><strong>什么是 AI Native Cloud（AI 原生云）？</strong>IDC 报告里定义：“需要同时满足 GPU 高密度算力、超低延迟网络、以及面向 GenAI 的编排与冷却等需求”。</p><p></p><p>卢言霞分析道：“未来企业可能有多个智能体，成千上万个智能体之间并行大规模交互，对分布式算力和模型间传输的要求已经与传统 AI 时代有很大不同。”更关键的是，从通用模型到行业定制化的转型中，模型调优、RAG 推理环节的增加催生了训推一体需求。</p><p></p><p>Alex 从技术角度分析了 AI Native Cloud 的核心技术壁垒：“<strong>首先是 GPU 集群的高效调度能力。</strong> 比如在泰国、越南、马来西亚等亚太区域间的算力调度，关键不在于硬件扩容，而是通过 K8s 等动态资源调配技术，将算力利用率稳定维持在 98% 以上。我们自研的 Cluster Engine 技术能在亚太四个节点间实时调度，甚至能利用时区差异 —— 当亚洲进入夜间时，美国客户可调用亚洲节点算力，让整体利用率持续攀升。这种调度稳定性还能避免训练任务中断，原本 10 天的训练周期可提前完成，这是高效调度能力的核心价值。</p><p></p><p><strong>其次是算力的适配能力。</strong> 不同 AI 场景的算力需求差异极大：量化训练的逻辑如同骨架般具有严格时序节点，需高算力密度支持，而视频扩散模型或图片处理可能用中低端显卡就能完成。因此，能否打造统一框架适配多元场景至关重要。我们的第二个产品 Inference Engine 正是为此设计 —— 它打通硬件适配层，让客户无需关注底层硬件，直接通过 API token 按调用量付费。不是按卡计费，而是根据文本、图像、语音等不同模态动态调配算力，用灵活的算力资源支撑‘按 token 计价’模式，这是算力适配的核心逻辑。</p><p></p><p><strong>再次是全链路的优化能力。</strong> 这正是 GMI Cloud 与传统云的差异所在：传统云仅提供‘多少张卡 + 多少存储’的资源组合，而我们会做分散式推理架构设计，效率远高于传统方案。基于 Inference Engine 的模型调度能力，我们的模型吞吐率、TTFT（首 token 生成时间）等指标均优于传统云厂商 —— 这源于从模型侧到硬件侧的深度调优，不再是单纯提供资源，而是将算力转化为直接可用的模型服务接口。”</p><p></p><p>他进一步补充了三点关键技术洞察，阐释了 AI Cloud 与传统云的根本区别：<strong>第一，架构范式转变：从虚拟化到裸金属。</strong> Alex 指出，传统云厂商受制于过去二十年的虚拟化架构，通常以虚拟机形式提供算力。然而，AI 计算，尤其是训练和低延迟推理，需要直接掌控底层硬件资源以避免虚拟化带来的性能损耗。</p><p>Alex 提到与其他云厂商合作时的体验，“但现在 AI Native 的新创公司，常常会需要 BareMetal（裸金属），因为需要控制到整个架构。” 这种对底层硬件的直接访问和控制，对于实现极致的性能优化和稳定性至关重要。</p><p></p><p><strong>第二，服务模式变革：从远程支持到陪伴式服务。</strong> AI 时代云服务的深度正在发生本质变化。“我们服务了很多训练类的客户，基本需要陪伴式服务，因为训练集群随时可能出现各种问题”，Alex 描述道。这种 “长期陪伴的服务能力” 要求云厂商的工程师团队几乎驻扎在客户现场，与客户共同调试和优化，这与传统云时代 “开个网站、基本不会坏” 的远程、标准化服务模式截然不同。GMI Cloud 为此建立专属 SLA 团队，承诺 10 分钟响应、1 小时问题诊断、2 小时系统恢复。</p><p></p><p><strong>第三，核心竞争壁垒</strong>：<strong>全球化合规与运营。</strong> 这一点在当前的国际环境下显得尤为关键。亚太地区数据法规碎片化，GPU 资源也相对抢手，这要求云厂商不仅要在技术上过硬，还必须具备在全球复杂的地缘政治和监管环境中安全、合规运营的能力。GMI Cloud 已在亚太建立多个合规节点，通过本地化集群 + 动态调度，满足不同区域的合规与延迟需求。</p><p></p><p>这三项要求共同构成了传统云厂商转型的壁垒。“很多传统云厂商或者 GPU 集群供应商很容易被过去的架构给限制住，而不能提供给客户更敏捷的产品，”Alex 总结道。而这恰恰为没有历史包袱、从一开始就围绕 AI 工作负载构建技术栈的新兴云厂商创造了巨大的市场机会，进而推动了 AI 云厂商的快速崛起。目前 GMI Cloud 正持续推进 “AI Factory” 计划，即将落地全亚洲最大的万卡液冷 GB300 集群，未来还将在东南亚、日本、中东、美国等区域布局，以支撑超大规模算力需求等。</p><p></p><p><img>https://static001.infoq.cn/resource/image/a8/ae/a8fecdc311439d80e61442fa7bd867ae.png<img></p><p></p><p>任何技术的革新，最终落地到企业的视角，除了提效，能否节省成本则是技术选型的另一考核要素。</p><p>亚太地区 AI 企业普遍采用多云策略，以规避供应商锁定、追求最佳性价比或满足数据本地化要求。然而，<strong>“算力资源分散在不同云平台、管理规则与接口五花八门” 的局面，构成了一个巨大的 “隐性成本黑洞”。</strong></p><p></p><p>卢言霞详细剖析了其中的挑战：“企业的管理成本变得非常高，这涉及到完全不同厂商的技术栈，它们的定价模式、服务水准协议（SLA）、技术支持方式都存在巨大差异。要实现这些异构技术栈的融合、保证不同平台间的兼容性，其整体的运营复杂度和成本是相当可观的。”</p><p></p><p>她进一步指出了更棘手的数据问题 —— 生成式 AI 应用往往需要从多个异构数据源读取数据。当企业设想一个核心智能体与内部成千上万的其他智能体进行并行交互时，这些数据和系统可能分布在不同的公有云、甚至私有的本地化基础设施中，其间的数据同步与协同成为了巨大的工程挑战。更关键的是，不同系统接口标准化程度低，多数定制开发系统接口不统一，进一步抬高了技术门槛。</p><p></p><p>面对这一行业痛点，Alex 阐述了 GMI Cloud 提供的 “统一算力纳管” 解决方案：</p><p></p><ul><li><p><strong>底层 GPU 硬件架构：提供高端 GPU 云与裸金属服务</strong>。通过顶级生态协作获取英伟达高端硬件资源，并为高性能和高控制权限要求的客户提供直接开放硬件层访问的裸金属方案，消除虚拟化损耗，适配泛互联网、自动驾驶等对性能与控制权要求严苛的场景。</p></li></ul><p></p><ul><li><p><strong>IaaS 层：Cluster Engine 平台。</strong> 基于 K8s 架构实现全球算力弹性调度，支持跨区域负载均衡与错峰复用，资源利用率达 98%+，并通过可视化工具实现实时监控与智能管理。</p></li></ul><p></p><ul><li><p><strong>MaaS 层：Inference Engine 推理引擎平台。</strong> 底层搭载 H200 芯片，集成 DeepSeek、Qwen 等近百个大模型，平台通过自研推理优化技术提升模型调用效率，提供统一 API 接口，支持文本 / 图像 / 视频多模态模型调用，实现 “按 token 用量付费” 的弹性服务。</p></li></ul><p></p><p>三层架构的协同形成了完整的算力价值闭环：底层硬件提供性能基础，Cluster Engine 实现资源高效流转，Inference Engine 交付即用模型能力，最终帮助企业破解算力分散、管理复杂、成本高企等核心痛点。</p><p></p><p><strong>除了多云管理带来的复杂度和成本挑战，企业在算力投入上还面临一个两难困境：“前期投入巨大，但后期利用率难以保证，导致闲置率高企”。</strong> 卢言霞观察到，这一问题在 2025 年上半年的中国市场尤为典型。“尤其是一些大型企业，之前投入了大量的一体机方案。在大模型浪潮爆发之前，中国市场对 AI 和 IT 的投入就比较重视算力基础设施的采购，但往往未能与最终的应用场景和效率紧密挂钩。”</p><p></p><p>传统 AI 时代，企业 IT 投入中硬件常作为固定资产，但技术迭代快（如几年前的芯片型号如今可能过时），加上 AI 应用未大规模落地，导致前期投入易形成浪费；而互联网企业因业务波峰波谷明显，新兴 AIGC APP 试点新功能时，也不适合过早投入硬件，否则可能因功能未留存造成资源闲置。</p><p></p><p>针对这一核心痛点，Alex 分享了 GMI Cloud 给客户的方案。“GPU 的迭代速度正在变得越来越快，从过去的 5-6 年缩短到现在的 3 年甚至更短。技术迭代的加速意味着硬件贬值的风险急剧增加。因此，我们提供了 ‘Rent versus Buy’（租用而非购买） 的服务方式。客户可以与我们签订三年的合同，以租用的方式获得顶尖的算力，并在合同结束后，可以根据需要轻松升级到最新的硬件，从而彻底避免了技术迭代带来的资产贬值风险。” 这种模式对于现金流敏感的新创 AI 应用公司尤其具有吸引力，因为它将沉重的固定资产投入转化为了灵活的运营成本。</p><p></p><p>Alex 强调:“这种深度合作模式也使得 GMI Cloud 与客户的关系从传统的 ‘供应商 — 采购方’转变为了 ‘战略伙伴、共同成长’。”实际上，当 AI 算力需求从 “标准化采购” 转向 “场景化定制”，传统云厂商 “卖算力资源” 的供应商模式也将发生改变。随着 AI 基础设施进入 “效果为王” 的深水区，云厂商的竞争力不再取决于 “有多少算力”，而在于 “能为客户的每一分算力投入创造多少商业价值”。</p><p></p><p><img>https://static001.infoq.cn/resource/image/6f/a2/6f883ce41dfdfe1b720d2745a21d39a2.png<img></p><p></p><p>GenAI 场景应用的加速，除了带来技术栈、需求、模式等变革外，也在深层次影响产业发展的风向和竞争格局。</p><p></p><p>报告数据显示，95% 的亚太企业正在同时部署训练和推理基础设施。<strong>从行业分布来看，泛互联网、制造业和具身智能成为推理设施投入增速最快的三大领域。</strong> 其中，泛互联网既包括传统大型互联网企业，也包含当下火热的 AIGC 应用，特别是中国企业出海的重点方向 ——AI 社交、内容生成等 to C 应用；制造业则涵盖高端器械、医疗器械、重工业设备等领域的出海企业，这些企业在海外建设智能制造工厂，带动了大模型和 AIGC 应用需求；具身智能领域的机器人企业，无论是新秀还是老牌厂商，在 AI 推理算力基础设施上的投入也呈指数级增长。</p><p></p><p>Alex 通过实战观察验证了这一趋势：“我们看到的最大需求来自泛互联网，接下来是制造业。这些需求可以细分为三种模态：语音、视频加图像、文本。” 他进一步解释道，语音包括语音转换、呼叫中心、陪伴应用；视频主要是电商领域，需大量图像与视频制作广告素材；文本则是 Copilot、会议摘要等工具。</p><p></p><p><strong>技术应用层面，多模态融合正成为场景爆发的核心方向。</strong>Alex 预判视频领域将迎来 “DeepSeek 时刻”，B200 相比 H100 速度提升两倍，原本生成 5 秒视频需要耗时 30 秒，未来可能缩短至 400 毫秒，实现即时生成，这将彻底改变内容生产方式。另外，电商、影片生成、短视频、动画、广告都是亚洲市场的热门领域，庞大的用户基数与场景红利，为 AI 技术提供了天然的试验场与商业化土壤。而开源与闭源格局的变化更是降低了入场门槛 ，中尾部企业无需自建大模型，通过 Finetuning 即可快速落地场景。</p><p></p><p><strong>推理需求的快速增长，也带动了 AI 基础设施市场的竞争格局重塑</strong>。传统公有云厂商与 AI Cloud/GPU Cloud 新型云厂商之间的市场份额变化呈现出明显趋势。卢言霞透露：“2024 年到 2025 年间，GPU Cloud 和新兴云厂商在整个生成式 AI 基础设施市场上可能占到 15% 左右的市场份额。不要小瞧这 15%，对基础设施这么庞大的市场来说已经是非常大的进展。”</p><p></p><p><strong>一个反常识的转变也在发生：亚太市场的算力玩家们正在从 “零和博弈” 走向 “竞合共生”。</strong>Alex 提到，不同于传统 IT 行业的 “要么我卖进、要么你卖进”，AI 赛道因算力普遍短缺，“合作潜力非常多，大家都不够用，就互相借卡、租卡”，泛互联网超大型企业、公有云甚至会与新兴 AI 云厂商合作，“他们不想持续砸钱买卡，直接向我们租，我们能在小地方快速建立集群，速度比他们更快”。</p><p></p><p>这种资源互补的模式，打破了传统市场的竞争壁垒，让算力资源流动更高效，为中国企业提供了更多合作机遇，也为中企 AI 应用出海提供了更多的支撑。</p><p></p><p>对于计划出海的中国 AI 企业，在直播最后，卢言霞给出了三点核心战略建议：</p><p></p><ul><li><p><strong>第一，建立负责任的 AI 体系</strong>，“现阶段对整个行业参与者非常重要”。随着生成式 AI 能力增强，伦理风险、内容合规等问题已引发全球监管关注，头部企业需优先构建全流程的 AI 治理框架，这不仅是准入门槛，更是长期信任的基础；</p></li></ul><p></p><ul><li><p><strong>第二，紧盯大模型能力进化</strong>，“大模型迭代快，要判断哪些能力可能由大模型直接提供，无需开发工具重复投入”。避免在通用能力上浪费资源，聚焦行业定制化的差异化价值；</p></li></ul><p></p><ul><li><p><strong>第三，重视 AI 专用基础设施建设</strong>，“传统 AI 时代企业对基础设施重视不足，如今生成式 AI 广泛部署，必须关注面向 AI 工作负载优化的基础设施”，尤其是训推一体、低延迟网络等核心能力，这是业务落地的技术基石。</p></li></ul><p></p><p>对中国企业而言，只有抓住推理市场新机遇，在性能、合规、成本间找到平衡点，才有机会在算力变革的浪潮中抢占先机，从 “AI 应用追随者” 稳步进阶为 “区域规则的共建者”，进而在全球市场竞争中筑牢优势。</p><p></p><p><img>https://static001.infoq.cn/resource/image/5c/95/5c1ea66f943e5888a5dbb8dba7085595.png<img></p><p></p><p>卢言霞与 Alex 指出，亚太 AI 基础设施的变革本质是技术与产业话语权的双重重构。随着推理基础设施渗透率稳步提升，传统云“卖资源”的模式正在失效，取而代之的是“技术栈 + 服务模式 + 全球布局”的综合实力较量。</p><p></p><p>从技术底层看，AI 原生云正通过裸金属架构、K8s 弹性调度等关键技术，将 GPU 算力利用率提升至行业领先水平，并实现训推一体的闭环优化。这一能力直接推动了 AI 应用（如智能体）从单一任务执行向复杂多场景协同的演进。以 GMI Cloud 的分布式推理架构为例，其通过 PD 分离、跨区域动态扩缩容等技术，稳定支撑了高并发实时推理，满足了多区域用户访问与智能体并行决策的需求。</p><p></p><p>对行业而言，AI 原生云的核心价值在于构建了一个高效、智能的“能力底座”。它通过将算力精准转化为业务迭代的直接生产力，助力企业驾驭高并发推理与智能体协同等复杂场景。亚太市场正在经历的，正是一场从“资源上云”到“智能用云”的深刻效能革命。</p><p></p><p><italic>点击</italic><italic>获取完整报告。</italic></p><p></p></div>",
            "link": "https://www.infoq.cn/article/ZCe0w3QBY1NZPbrAja8E",
            "pub_date": "2025-10-25 08:36:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "WhRl2cJktHCN5wrxAcvg",
            "title": "亚马逊云科技推出EC2容量管理器，实现集中式跨账户容量优化",
            "image": "https://static001.infoq.cn/resource/image/78/7a/78237cd7b0d794dddf959163da05dc7a.jpg",
            "description": "<div><p><color>最近，亚马逊云科技新推出了一个集中式解决方案——</color><color>，旨在通过单个界面监控、分析和管理客户所有账户和AWS区域内的EC2容量使用情况。</color></p><p><color> </color></p><p><color>以前，在跨多个可用区协调按需实例、竞价实例和容量预留（CR）等类型的数百个实例时，大规模运营</color><color>（Amazon EC2）的组织会面临高度复杂的管理开销。底层容量数据分散在AWS Management Console、Cost and Usage Reports(CUR)、Amazon CloudWatch和各种EC2 API中，造成了明显的运营障碍。</color></p><p><color> </color></p><p><color>为了应对这一挑战，EC2容量管理器将所有容量数据聚合到了一个统一的、跨账户、跨区域的仪表板中。此外，该服务每小时刷新一次容量信息，并且初始设置保留14天的历史数据用于即时分析。</color></p><p></p><p><img>https://static001.geekbang.org/infoq/00/003b246124752afb25da42b37588a9df.png<img></p><p></p><p><color>（图片来源：</color><color>）</color></p><p><color> </color></p><p><color>亚马逊云科技安全英雄Sena Yakut</color><color>了这种整合的迫切需求：</color></p><p><color> </color></p><p><color> </color></p><p><color>主仪表板提供了一个综合视图，上面显示了所有实例类型的利用率，提供了基于vCPU、实例计数或估计成本（使用按需费率计算）的指标。其核心功能包括：</color></p><p><color> </color></p><ul><li><p><span style=\"color:#494949\">预留指标：可视化跟踪已使用与未使用预留容量的比例，可直接反映预留效率。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">竞价分析：竞价标签聚焦使用模式，显示关键指标，如竞价实例在被中断前的平均运行时长。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">直接管理：对于操作人员来说，一个特性是能够直接从容量管理器界面修改按需容量预留（ODCR），当预留资源位于同一个账户下时，这可以减少上下文切换并简化响应性更改。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">数据导出和集成：容量管理器支持将数据导出到Amazon S3，这使得组织可以保留超过标准保留期限（90天）的容量数据，用于长期趋势分析以及与外部商业智能（BI）工具集成。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">AWS Organizations集成：设置部分原生支持集中化的企业范围的容量可见性，以及跨多个账户的委托访问控制，从而简化治理流程。</span></p></li></ul><p></p><p><img>https://static001.geekbang.org/infoq/86/86ea5eacafd71e6dd59b3a7c3675f7f8.png<img></p><p></p><p><color>（图片来源：</color><color>）</color></p><p><color> </color></p><p><color>该公告在开发者和财务运营社区中引发了褒贬不一的反应。许多人对此表示欢迎，认为这减轻了运营压力，尤其是对成本管理专业人员而言。在</color><color>中，一位参与者评论道：</color></p><p><color> </color></p><p><color> </color></p><p><color>在LinkedIn上，首席云架构师Ivo Pinto</color><color>了安全和访问控制方面的好处：</color></p><p><color> </color></p><p><color> </color></p><p><color>不过，也有人对这一功能的长期云价值主张表示了怀疑。在同一个Reddit讨论帖中，另一位评论者</color><color>，集中式工具并不能解决云弹性成本的核心问题：</color></p><p><color> </color></p><p><color> </color></p><p><color>在推特上，首席云工程师Jack Hendy</color><color>：</color></p><p><color> </color></p><p><color> </color></p><p><color>最后，Amazon EC2容量管理器在所有商业AWS区域中默认启用，并且不额外收取任何费用。</color></p><p><color> </color></p><p><color> </color></p><p><color>声明：本文为InfoQ翻译，未经许可禁止转载。</color></p><p><color> </color></p><p><color>原文链接：</color></p></div>",
            "link": "https://www.infoq.cn/article/WhRl2cJktHCN5wrxAcvg",
            "pub_date": "2025-10-29 04:00:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "FDc37HIdrfyWGv3UqXVA",
            "title": "黄仁勋凌晨炸场：6G、量子计算、物理AI、机器人、自动驾驶全来了！AI芯片订单已达3.5万亿｜2025GTC最全指南",
            "image": "https://static001.infoq.cn/resource/image/96/e6/96e4789d23f9486d3b5de16a4e4101e6.jpg",
            "description": "<div><p>本周可谓<strong>美国科技行业的“超级周”</strong>，而号称“AI风向标”的<strong>英伟达GTC</strong>（GPU Technology Conference，GPU技术大会）也于美东时间10月27日<strong>开幕</strong>。</p><p></p><p>10月28日，黄仁勋依旧身着皮衣登场。但与以往有明确重点的发布会不同，老黄此次演讲<strong>几乎把全球科技圈的热词悉数点了一遍：6G、量子计算、物理AI、机器人、自动驾驶、核聚变...</strong>一个没落下。</p><p></p><p>他还官宣了一个<strong>跨次元般的</strong>重磅消息：</p><p></p><p>英伟达和<strong>诺基亚</strong>合作了，用AI提高无线通信的速度，共同建造面向AI原生的移动网络，搭建6G AI平台；为此，英伟达推出新品<strong>NVIDIA Arc（Aerial Radio Network Computer）</strong>，还将对诺基亚投资10亿美元（约合人民71亿元）。</p><p></p><p><img>https://static001.geekbang.org/infoq/96/962bde9f6657f45727124ebfc87d63c3.png<img></p><p></p><p>也就是说，AI不再只是网络的使用者，还成为了网络本身的“智能中枢”。</p><p></p><p>黄仁勋直言：“我们将采用这项新技术，升级全球数百万个基站。”</p><p></p><p>值得一提的是，老黄这次（可能下次血本了）不仅演讲话题涉猎甚广，还组建了一支<strong>超级“嘉宾足球队”</strong>，在一众大佬中，具身智能当红新星<strong>Figure AI创始人兼CEO Brett Adcock</strong>（第二行左四）也赫然在列。</p><p></p><p></p><p>话说回来，<strong>英伟达AI芯片</strong>的成绩也着实亮眼：Blackwell和Rubin芯片订单总额，已达<strong>5000亿美元</strong>（约合人民币35,000亿元）。</p><p></p><p>在今年3月的春季GTC大会上，老黄曾秀出<strong>最强AI芯片GB300 NVL72</strong>，其中<strong>“G”</strong>是该芯片中的CPU架构<strong>Grace</strong>，<strong>“B”</strong>是GPU架构<strong>Blackwell</strong>，也是英伟达现在主推的数据中心GPU架构。</p><p></p><p>截至当日收盘，<strong>英伟达</strong>股价涨约5%，<strong>总市值逼近5万亿美元（约合人民币35万亿元）</strong>，创下新高。</p><p><img>https://static001.geekbang.org/infoq/b3/b3c03b0c2ab8ac8383c78cbafb996e04.png<img></p><p></p><p></p><h2>次元壁破了：和老牌手机霸主一起搞通信</h2><p>在这次GTC上，老黄官宣的第一个合作协议，就是和诺基亚携手共建6G AI平台。</p><p></p><p>二者此次<strong>合作重点</strong>不在“造芯片”，而在<strong>“让网络学会思考”</strong>：英伟达把其加速计算平台<strong>Aerial RAN Computer Pro（ARC-Pro）</strong>，带进诺基亚的无线通信系统<strong>AirScale</strong>中，推动运营商向AI原生的5G与6G网络过渡。</p><p></p><p>话说，<strong>诺基亚</strong>这个零几年的全球手机霸主、听起来好像是个“上古战神”，现在<strong>为什么会出现在英伟达的合作名单上，而且还被放在超级显眼位置？</strong></p><p></p><p><strong>首先，其实诺基亚“没死”</strong>，它只是卖掉了手机业务，<strong>退回了“根部”的通信设备业务</strong>。</p><p></p><p>2013年，在卖掉手机部门给微软之后，诺基亚彻底转向<strong>电信基础设施</strong>：基站、天线、光纤网络、核心网软件——全球运营商的底层网络都离不开这些。</p><p></p><p>2016年，诺基亚收购了阿尔卡特朗讯——这是由传奇的<strong>“贝尔实验室”</strong>（曾获9项诺贝尔奖、4项图灵奖）改组重建的公司，然后就自然而然地继承了贝尔实验室一堆领先的技术和专利；收购后，贝尔实验室也更名为Nokia Bell Labs。</p><p></p><p>如今的诺基亚和华为、爱立信并列，是全球三大通信设备厂商之一。</p><p></p><p><strong>第二，英伟达做芯片的终极目标</strong>，其实不是“更强的GPU”，而是<strong>让所有计算都发生在“靠近数据产生的地方”——也就是网络边缘，</strong>而这正是诺基亚的主场。</p><p></p><p>英伟达提供 ARC-Pro平台：让通信基站不仅传信号，还能执行 AI 推理；诺基亚则提供AirScale 无线系统和RAN软件栈：让这些“AI 基站”融入现有的5G网络，并能平滑升级到 6G。另外，T-Mobile是他们的首个运营商合作方，将在 2026年开始实地测试。</p><p></p><p>简单来说，就是英伟达把GPU算力塞进基站，诺基亚负责让它能在真实的网络中跑起来。</p><p></p><p>正如黄仁勋在发布会上所说的：“基于NVIDIA CUDA和AI的AI-RAN，将彻底改变电信行业，这是一次跨时代的平台变革。”</p><p></p><p>英伟达的官方新闻稿也指出，此次合作标志着行业的转折点，通过在全球范围内推动AI-RAN的创新和商业化，为AI 原生6G铺平了道路。</p><p></p><p>在诺基亚之外，黄仁勋还一口气官宣了几家合作伙伴：从自动驾驶巨头Uber、政府AI供应商Palantir到与美国能源部、甲骨文达成战略合作——几乎覆盖了AI产业的每一个关键环节，暗含其野心。</p><p></p><p>先是Uber。黄仁勋认为“机器人出租车的拐点即将到来”，双方计划在全球铺开10万辆自动驾驶汽车。这不仅是自动驾驶的突破，更是AI硬件与智能算法的深度融合——英伟达正试图让GPU成为Robotaxi时代的“车载大脑”，推动出行生态进入商业化阶段。</p><p></p><p>接着是Palantir。这家擅长处理政府和军情数据的公司，将把英伟达的CUDA-X和Nemotron模型嵌入自家系统，让AI学会“看懂”世界。Lowe’s已经在用这套组合调度它的供应链——AI代理成了企业的隐形决策官。</p><p></p><p>不过更大胆的布局在科研领域，与美国能源部、甲骨文联手打造七台AI超级计算机。其中阿贡实验室的Solstice与Equinox系统，将以2,200 exaFLOP的惊人算力，成为“美国的探索引擎”。</p><p></p><p>从Robotaxi到政府级AI决策，再到超级计算，英伟达不再“造芯片”，而是在“造智能的地基”。</p><p></p><h2>前沿科技话题“大点兵”</h2><p></p><p>前文提到，老黄这次的演讲涉及多个话题，除了6G，还有<strong>量子计算、物理AI、机器人、自动驾驶等</strong>；下面来具体看几个。</p><h2>1、量子计算</h2><p>自理查德·费曼提出量子计算概念起，40年后，业界终于在去年实现了关键突破——创造出可相干、稳定、且具纠错能力的<strong>逻辑量子比特（logical qubit）</strong>。</p><p></p><p>英伟达此前推出了开放式量子GPU计算平台<strong>CUDA-Q</strong>，这次又开发了一个基于CUDA-Q核心构建的<strong>NVQLink，</strong>是一种能把传统GPU和量子处理器连接起来的互联架构。</p><p></p><p>当下的量子计算仍处在“易碎”阶段——对环境噪声异常敏感，且算力利用率有限。为了让量子比特保持稳定运行，往往要借助 GPU 超算系统承担控制与纠错计算，这让量子计算暂时还离不开经典计算的“辅助臂”。</p><p>黄仁勋分享称，英伟达将与美国能源部合作建设<strong>7台AI超级计算机</strong>，这些超算将使用Blackwell和下一代Vera Rubin架构芯片，利用 AI、量子计算等最前技术投入研究。</p><p></p><p>老黄还化身“AI赛道的美国队长”，举起了他的“芯片盾牌”：由NVLink连接的72块GPU构成。</p><p></p><p><img>https://static001.geekbang.org/infoq/ba/bab53b874868d7722e6f77cea2a837ca.png<img></p><p></p><h2>2、具身智能和物理AI（Physical AI）</h2><p>对于具身智能与机器人计算，英伟达的理念是，如果要让AI真正进入物理世界、具备感知和行动能力，必须依托一个“三计算机”体系：</p><p></p><p>一是用于模型训练的<strong>Grace Blackwell AI 计算机</strong>，负责生成大规模智能模型；二是用于仿真和虚拟验证的 <strong>Omniverse 数字孪生计算机</strong>，在虚拟环境中模拟机器人行为与物理交互；三是用于实际执行的 <strong>Jetson Thor 机器人计算机</strong>，让智能在真实世界中运行。</p><p></p><p>这三者都基于CUDA平台运行，形成从训练、仿真到执行的完整“物理智能”闭环，使AI能够真正连接虚拟与现实世界。</p><p></p><p>英伟达投资过多家具体智能公司，和美国具体智能新独角兽Figure AI也在开展合作，加速下一代机器人的研发。</p><p></p><p><img>https://static001.geekbang.org/infoq/3e/3ea8fd1b7156268885bc9b96f0803704.png<img></p><p></p><p></p><h2>3、开源模型与生态合作</h2><p>英伟达在开源模型和产业生态上正展开双线布局。</p><p></p><p>一方面，得益于推理、多模态与知识蒸馏等能力的提升，开源模型已经足够强大，成为初创企业和科研机构进行灵活定制与创新的基础。英伟达作为开源社区的重要贡献者，已有23个模型登上各类性能榜单，并承诺将持续投入。</p><p></p><p>另一方面，英伟达正加速与云计算和行业伙伴的深度集成：其模型与库已嵌入AWS、Google Cloud、Microsoft Azure等主流云平台，以及ServiceNow、SAP等 SaaS系统，使用户能够在不同生态中无缝调用AI能力。</p><p></p><p>同时，英伟达还与 CrowdStrike（网络安全）、Palantir（数据处理）和 Synopsys（芯片设计）等行业巨头合作，以 AI 提升垂直领域生产力，推动从安全到设计的智能化变革。</p><p></p><h2>4、AI在聊天机器人之外的更多应用</h2><p>AI 被广泛用于基础科学研究，远不止聊天或生成内容。在医疗、基因组学、企业计算等领域都有应用。不同类型的模型（卷积神经网络 CNN、图神经网络 GNN、状态空间模型等）被用于不同任务。</p><p></p><p>AI 不仅是工具，也能成为“数字员工”。例如英伟达内部的 <strong>Cursor</strong> 系统可帮助工程师自动生成代码；AI 驾驶员（AI Chauffeur）则被用于自动驾驶出租车。</p><p></p><h2>老黄还亲自详解：AI到底是什么？</h2><p>在GTC大会上，除了抛出一个又一个爆炸性的技术新动向，黄仁勋还特意拿出20分钟，讲了一堂“AI是什么”的深度课。</p><p></p><p>在他看来，这件事十分必要——如果没弄清AI的定义，就无法判断下一场产业潮水将流向何处。</p><p></p><p>黄仁勋首先澄清一个误区：<strong>AI的世界</strong>远不止ChatGPT所代表的聊天机器人，那只是大众心中的AI形象。<strong>真正关键的，是以AGI为代表的深层计算机科学，以及支撑它的惊人算力</strong>，“AI不是某个应用，而是一种新的计算方式。”</p><p></p><p>过去的计算世界，是程序员写规则、计算机执行命令；现在，机器靠数据自己学习规律。</p><p></p><p>比如，以前要教电脑识别猫，你得写几十条规则；如今只需给它十万张猫的照片，它自己就能学会什么是“猫”。</p><p></p><p>这场转变包含三层逻辑：计算方式变了——从写代码变成喂数据；计算工具变了——从CPU到GPU；计算目标也变了——从执行任务到生成智能。AI由此彻底重构了计算栈。</p><p></p><p>换句话说，AI正在从“螺丝刀”变成“工人”。</p><p></p><p>它不再是被动的工具，而是主动的执行者——会使用浏览器、写代码、制定计划、理解需求。</p><p></p><p>当技术第一次具备了“做事”的能力，机器也第一次进入了生产力的核心。</p><p></p><p>有了学习能力，AI看世界的方式也彻底不同。</p><p>在它的眼中，万物都被拆解成可学习的片段：文字、图片、声音、分子、蛋白质……这些最小的信息单位叫作token。</p><p>AI通过这些“语言颗粒”去理解、模仿、重建世界。谁能以更低成本、更高速度生成和操控token，谁就能主导下一代计算。</p><p></p><p>这也催生了一种全新的基础设施——AI工厂。</p><p></p><p>如果传统数据中心像多功能仓库，负责存储文件、运行程序，那么AI工厂就像一条生产线，只做一件事：生产token。</p><p></p><p>能源流入，驱动GPU；GPU通过NVLink和Spectrum-X网络连接成超级系统；软件与模型协同工作，批量产出token。</p><p></p><p><strong>黄仁勋把这条链概括成一句话：</strong></p><p><strong>“能源 → GPU → 算法 → 模型 → Token → 智能。”</strong></p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/f4/f4c48f7bceaa07695874eaefc8b24e0a.png<img></p><p></p><p></p><p>AI的成长轨迹，也像一个“吃电的天才”。</p><p></p><p><strong>它先在预训练阶段“学语言、记知识”，像学前班；</strong></p><p><strong>后在后训练阶段“学技能、学推理”；</strong></p><p><strong>最后进入“思考阶段”，在与你互动时实时规划、推理——那是最消耗算力的时刻。</strong></p><p></p><p>AI越聪明，就越费电。这是第一条指数——AI使用指数：智能与算力的爆炸式增长。</p><p></p><p>但还有第二条曲线——AI采用指数：AI越好用，越多人使用；使用越多，算力需求又被推高。</p><p></p><p>两条指数叠加，形成了一个强劲的正反馈——AI越聪明，体验越好；越好用，用户越多；越多的使用带来更多利润，又反过来推动AI变得更聪明。</p><p></p><p>面对算力需求如此巨大，要让这台“永动机”持续转动，黄仁勋的答案是“极致协同设计（Extreme Co-Design）”。</p><p></p><p>摩尔定律的线性提升早已赶不上AI的指数爆炸，必须在芯片、封装、互联、系统、编译器、模型、算法、应用等每一层同时创新。</p><p></p><p>他说：“要让AI继续前进，不能只造更快的芯片，而要重新设计整座工厂。”</p><p></p><p>这座“工厂”，就是未来的AI工厂。</p><p></p><p>在黄仁勋眼中，它不再是通用数据中心，而是一条智能的生产线——能源是燃料，GPU是引擎，模型是模具，Token是产品。</p><p></p><p>算力不再是辅助资源，而是新的生产资料。只有不断压低成本、扩大产能，AI的良性循环才能继续。</p><p></p><p>这就是未来计算的形态。</p><p></p><p>AI工厂将成为现代经济的新基础设施，从科学、医疗、制造到娱乐，所有行业都将围绕AI工厂重构。</p><p></p><p>从Arvin在Perplexity的工作、软件开发中的Cursor，到机器人出租车中的AI司机。人工智能正快速渗透到过去难以触及的经济领域，占据越来越广阔的版图。</p><p></p><p>那20分钟里，他讲的其实不只是AI，而是人类第一次拥有一种能把能量直接转化为智能的机器。</p><p></p><p>未来的计算，不再是让电脑执行命令，而是让世界自己学会思考。</p><p></p><p>老黄演讲视频回看地址：</p><p>https://www.youtube.com/watch?v=lQHK61IDFH4&list=TLGG23pf8VjteXoyODEwMjAyNQ</p><p>参考链接：</p><p>https://nvidianews.nvidia.com/news/nvidia-nokia-ai-telecommunications</p><p>https://www.reuters.com/world/asia-pacific/nvidias-huang-speak-washington-investors-look-hints-china-2025-10-28/</p></div>",
            "link": "https://www.infoq.cn/article/FDc37HIdrfyWGv3UqXVA",
            "pub_date": "2025-10-29 00:57:13",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "UF3guSh5REtRDy36ddwL",
            "title": "自带密钥（BYOK）：亚马逊云科技采用CMK以满足企业合规性的需要",
            "image": "https://static001.infoq.cn/resource/image/73/52/737c7e440457648e49b4ba2aa39e4552.jpeg",
            "description": "<div><p><color>亚马逊云科技最近宣布，其IAM身份中心（IAM Identity Center）服务支持</color><color>以用于静态加密。组织可以使用自己的密钥来加密身份中心的身份数据。</color></p><p><color> </color></p><p><color>IAM身份中心是一项云服务，它集中管理对多个AWS账户和云应用程序的单点登录（SSO）访问。虽然身份中心的数据一直使用AWS持有的KMS密钥进行静态加密，但新的CMK支持允许组织使用自己的密钥来加密他们的员工身份数据，例如用户和组属性。</color></p><p><color> </color></p><p><color>与</color><color>的集成至关重要，因为它将加密密钥的生命周期（创建、轮换和删除）的控制权直接转移到客户手中。</color></p><p><color> </color></p><p></p><p><img>https://static001.geekbang.org/infoq/c9/c90db2616af255f4c3d84b4ae27f41b6.png<img></p><p></p><p><italic>(图片来源：</italic><italic>)</italic></p><p><color> </color></p><p><color>亚马逊云科技的IAM身份中心的高级产品经理Alex Milanovic在LinkedIn的</color><color>中总结了它的核心优势：</color></p><ul><li><p><span style=\"color:#494949\">完全控制他们自己的加密密钥。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">通过KMS和IAM策略对身份数据进行细粒度访问管理，确保只有授权主体才能访问他们的加密数据。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">通过详细的AWS CloudTrail密钥使用日志增强审计能力。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">对于需要数据主权的受监管行业，这能够加强合规性。</span></p></li></ul><p><color> </color></p><p><color>亚马逊云科技的开发布道师Sébastien Stormacq进一步详细说明了它所支持的控制级别：</color></p><p><color> </color></p><p><color>基于审计和监管的需求，整个过程会通过</color><color>进行记录，提供密钥使用的详细记录。这种对加密密钥的细粒度控制通常是在高度受监管行业运营的企业的一个先决条件。</color></p><p><color> </color></p><p><color>由于合规性或安全策略，使用CMK对静态数据进行加密是企业的标准要求，如</color><color>。其他超大规模的提供商和产品也通过它们各自的密钥管理服务广泛支持它。</color></p><p><color> </color></p><p><color>微软Azure通过</color><color>实现了这一点，使客户能够在各种服务中加密敏感数据，并通过</color><color>验证访问。同样，谷歌云通过</color><color>提供CMK，为Cloud Storage和BigQuery等服务提供加密边界和完整的密钥生命周期控制。</color></p><p><color> </color></p><p><color>身份中心服务支持单区域和多区域密钥，以满足用户的部署需求。但是，目前，身份中心实例只能在单个区域部署。尽管如此，公司建议使用多区域AWS KMS密钥，除非公司政策要求用户使用单区域密钥。它指出，多区域密钥会在区域间提供一致的密钥材料，同时在每个区域保持独立的密钥基础设施。</color></p><p><color> </color></p><p><color>最后，该功能目前在所有AWS商业区域、AWS GovCloud（美国）和AWS中国区域均可用。此外，在定价方面，用户需要为身份IAM中心付费，对于标准AWS KMS，密钥存储和API使用会产生费用。</color></p><p><color> </color></p><p><color>查看英文原文：</color></p></div>",
            "link": "https://www.infoq.cn/article/UF3guSh5REtRDy36ddwL",
            "pub_date": "2025-10-27 05:00:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "OGyUaIfDwSHzByidNlM0",
            "title": "当 Kafka 架构显露“疲态”：共享存储领域正迎来创新变革",
            "image": "https://static001.infoq.cn/resource/image/0b/11/0b8fd03b29352dea29305a6ayy4d4311.jpg",
            "description": "<div><p></p><p></p><h2>Kafka：数据运营与数据分析之间的桥梁</h2><p></p><p>我已经使用 Apache Kafka 多年，并且非常喜欢这个工具。作为一名数据工程师，我主要将它用作连接数据运营端与数据分析端的桥梁。凭借优雅的设计和强大的功能，Kafka 长期以来一直是流处理领域的标杆。</p><p></p><p><img>https://static001.geekbang.org/wechat/images/b0/b0441c7183e351481db9fb74463b6898<img></p><p></p><p>Kafka 扮演着连接数据运营端与数据分析端的桥梁角色。</p><p></p><p>自问世以来，Kafka 就凭借独特的分布式日志抽象，塑造了现代流处理架构。它不仅为实时数据流处理提供了无可比拟的能力，还围绕自身构建了完整的生态系统。</p><p></p><p>Kafka 的成功源于其核心优势：能够大规模地实现高吞吐量与低延迟处理。这一特性使其成为各类规模企业的可靠选择，并最终确立了其在流处理领域的行业标准地位。</p><p></p><p>但 Kafka 的发展之路并非一帆风顺。它的成本可能急剧攀升，而在流量高峰时段进行分区重分配等运维难题，更是令人头疼不已。</p><p></p><p>我至今还记得在沃尔玛工作时的经历：曾花费数小时排查一次恰逢流量高峰发生的分区重分配问题，那次经历几乎让我心力交瘁。</p><p></p><p>尽管成本居高不下，Kafka 在流处理领域的主导地位依然稳固。在如今云优先的大环境下，一个多年前基于本地磁盘存储设计的系统，至今仍是众多企业的核心支撑，这着实令人意外。</p><p></p><p>深入研究后我发现，背后的原因并非 Kafka “完美无缺”，而是长期以来缺乏合适的替代方案。其最大的卖点 —— 速度、持久性与可靠性，至今仍具有重要价值。</p><p></p><p>但只要使用过 Kafka，你就会知道：它将所有数据都存储在本地磁盘上。这一设计暗藏着一系列成本与挑战，包括磁盘故障、扩展难题、突发流量应对，以及受限于本地或私有部署存储容量等问题。</p><p></p><p>几个月前，我偶然发现了一个名为 AutoMQ 的开源项目。起初只是随意研究，后来却深入探索，彻底改变了我对流处理架构的认知。</p><p></p><p>因此，在本文中，我们希望分享两方面内容：一是 Kafka 传统存储模型面临的挑战，二是以 AutoMQ 为代表的现代解决方案如何通过云对象存储（而非本地磁盘）另辟蹊径解决这些问题。这一转变在保留 Kafka 熟悉的 API 与生态系统的同时，让 Kafka 具备更强的扩展性、更高的成本效益与更优的云适配性。</p><p></p><h2>不容忽视的问题：Kafka 为何停滞不前</h2><p></p><p>坦白说，Kafka 十分出色，它彻底改变了我们对数据流的认知。但每当我配置昂贵的 EBS 卷、看着分区重分配进程缓慢推进数小时，或是凌晨 3 点因某个 Broker 磁盘空间耗尽而被惊醒时，我总会忍不住思考：一定有更好的解决方案。</p><p></p><p>这些问题的根源何在？答案是 Kafka 的 shared-nothing 架构。每个 Broker 都像一个 “隐士”：独自拥有数据，将其小心翼翼地存储在本地磁盘上，拒绝与其他 Broker 共享。这种设计在 2011 年合情合理，当时我们使用私有部署服务器，本地磁盘是唯一的存储选择。但在如今的云时代，这就好比在所有人都使用谷歌云盘（Google Drive）的情况下，仍坚持使用文件柜存储数据。</p><p></p><p>这种架构实际带来了以下成本负担：</p><p></p><ul><li><p>9 倍的数据冗余（没错，你没看错 ——Kafka 3 倍副本 × EBS 3 倍副本）。</p></li></ul><ul><li><p>分区重分配进程极其缓慢，如同看着油漆变干。</p></li></ul><ul><li><p>完全缺乏弹性 —— 尝试对 Kafka 进行自动扩展，你会发现整个周末都要耗费在这上面。</p></li></ul><ul><li><p>跨可用区（AZ）流量费用高到让首席财务官（CFO）头疼。</p></li></ul><p></p><p><img>https://static001.geekbang.org/wechat/images/39/39a35a1a9805884f3b4857bcbfc2615d<img></p><p></p><h2>Kafka 的运维成本：Shared-Nothing 架构的代价</h2><p></p><p>我想通过一个故事，直观展现 Kafka 的成本问题。</p><p></p><p>假设你运营着一个小型电商网站，每小时仅摄入 1GB 数据，包括用户点击、订单信息、库存更新等，数据量并不算大。在过去，你只需将这些数据存储在一台服务器上即可。但如今是 2025 年，为确保高可用性，你选择部署 Kafka。</p><p></p><p>而 Shared-Nothing 架构在此刻开始让你付出高昂代价。</p><p></p><p><strong>Shared-Nothing 的真正含义</strong></p><p></p><p>在 Kafka 的体系中，“Shared-Nothing” 意味着每个 Broker 都像一个 “多疑的隐士”，彼此之间不共享任何资源 —— 无论是存储、数据，还是其他任何东西。每个 Broker 都拥有独立的本地磁盘，自行管理数据，本质上把其他 Broker 当作 “恰好共事的陌生人”。</p><p></p><p>这就好比三个室友拒绝共享 Netflix 账号，反而各自付费订阅，将相同的节目下载到自己的设备上，并小心翼翼地守护着自己的密码。听起来成本很高？事实确实如此。</p><p></p><p><strong>三重（甚至更严重的）打击</strong></p><p></p><p>接下来，让我们看看成本问题有多棘手。</p><p></p><p><img>https://static001.geekbang.org/wechat/images/cc/ccf09819d26f4490ebfe903e79217460<img></p><p></p><p>请仔细观察上图。</p><p></p><p>现在，让我们跟踪 1GB / 小时的数据在 Kafka 副本机制中的流转过程：</p><p></p><ul><li><p>第 1 小时：应用产生 1GB 数据。</p></li></ul><ul><li><p>Kafka 副本（副本因子 RF=3）：1GB 数据在 Broker 间复制为 3GB。</p></li></ul><ul><li><p>EBS 副本：这 3GB 数据的每个副本又被 AWS 复制 3 份，最终变为 9GB。</p></li></ul><ul><li><p>预留空间：为避免午夜告警，需额外预留 30%-40% 的缓冲空间，最终需配置约 12GB 存储。</p></li></ul><p></p><p>也就是说，每摄入 1GB 数据，你需要为约 12GB 的存储付费</p><p></p><p><strong>一周的数据流转（与费用消耗）</strong></p><p></p><p>若设置 7 天的数据保留期（常见配置）：</p><p></p><ul><li><p>第 1 天：实际数据 24GB，需配置 288GB 存储。</p></li></ul><ul><li><p>第 3 天：实际数据 72GB，需配置 864GB 存储。</p></li></ul><ul><li><p>第 7 天：实际数据 168GB，需配置约 2016GB 存储。</p></li></ul><p></p><p>更关键的是：即便你只需要消费最近 1 小时的数据，仍需为整整 7 天的数据存储与复制付费。</p><p></p><p>以上仅是粗略计算，旨在说明 Apache Kafka 的高成本问题。</p><p></p><p><strong>雪上加霜的跨可用区成本</strong></p><p></p><p>跨可用区复制让成本问题进一步恶化：</p><p></p><p>当数据摄入速率为 1GB / 小时（RF=3）时：</p><p></p><ul><li><p>每小时有 2GB 数据跨可用区传输。</p></li></ul><ul><li><p>每月约产生 1460GB 跨区流量，按每 GB 约 0.02 美元计算（双向传输各按每 GB 约 0.01 美元计费），每月费用约 29 美元。</p></li></ul><p></p><p>当数据摄入速率为 100MB / 秒（RF=3）时：</p><p></p><ul><li><p>副本机制新增 200MB / 秒的跨可用区流量。</p></li></ul><ul><li><p>生产者向其他可用区的 Leader 节点写入数据，又新增约 67MB / 秒的跨区流量。</p></li></ul><ul><li><p>总跨区流量约为 267MB / 秒，每月流量达 700800GB。</p></li></ul><ul><li><p>仅跨可用区副本流量与生产者流量的月度费用就约为 1.4 万美元。</p></li></ul><ul><li><p>若消费者也跨可用区拉取数据，月度费用将攀升至约 1.75 万美元。</p></li></ul><p></p><p><img>https://static001.geekbang.org/wechat/images/f7/f736c35f0c16704e33922aa556ae0921<img></p><p></p><p><strong>核心结论</strong></p><p></p><p>在 2011 年，Shared-Nothing 架构合情合理。当时我们使用物理服务器与本地磁盘，存储区域网络（SAN）的性能无法与本地磁盘相比。</p><p></p><p>但在云时代，你需要为相同的数据支付 12 倍的存储费用，再加上网络费用与管理大量磁盘的运维成本。这就好比在 Netflix 时代仍购买 DVD，不仅如此，还为每张 DVD 购买 3 份副本，存放在 3 个不同的地方，并雇人确保这些副本同步更新。</p><p></p><p>如今情况已然不同。S3 已成为云存储的事实标准，具备低成本、高持久性与全局可用性的特点。正因如此，包括数据库、数据仓库乃至如今的流处理平台在内的各类系统，都在围绕共享存储架构进行重新设计。</p><p></p><p>AutoMQ、Aiven、Redpanda 等项目顺应这一趋势，将存储与计算解耦。它们不再在 Broker 间无休止地复制数据，而是利用 S3 保障数据持久性与可用性，既减少了基础设施重复建设，又降低了跨可用区网络成本。</p><p></p><p>这些项目均致力于减少资源重复、降低跨可用区成本，并采用云原生设计。目前，大多数试图降低 Apache Kafka 成本的新兴项目，实际上都采用了以下两种方案之一：</p><p></p><ul><li><p>部分项目推动 Kafka 向全共享存储模型演进 ——Broker 变为无状态，存储完全依托 S3。</p></li></ul><ul><li><p>另一些项目则采用分层存储方案 —— 将旧数据段迁移至 S3/GCS 等远程存储，减少本地磁盘占用，但仍保留热数据层。</p></li></ul><p></p><p>当然，在 S3 上运行 Kafka 也面临自身挑战，例如延迟、一致性与元数据管理等问题。我们将在后续内容中深入探讨这些挑战，并重点分析 AutoMQ 等开源新项目如何高效解决这些问题。</p><p></p><p>一定有更好的方案，对吧？</p><p></p><p>（剧透：答案是肯定的 —— 这正是我们深入探索的起点……）</p><p></p><h2>Kafka 分层存储（Tiered Storage）方案的提出</h2><p></p><p>Kafka 社区一直在积极讨论并开发分层存储功能（参见 KIP-405）。</p><p></p><p>在阐述我认为该设计可能存在缺陷的原因之前，先让我们用通俗的语言解释一下什么是分层存储。</p><p></p><p>传统上，Kafka Broker 将所有数据存储在本地磁盘中。这种方式速度快，但成本高且扩展性差 —— 一旦磁盘空间耗尽，你要么增加更多 Broker，要么更换更大容量的磁盘，这导致存储扩展与计算扩展深度绑定。</p><p></p><p>分层存储打破了这一模式，将数据分为两层：</p><p></p><p><img>https://static001.geekbang.org/wechat/images/16/164994b496290cd066334a2b06baf81c<img></p><p></p><p><strong>Kafka 分层存储的核心特点</strong></p><p></p><p>热数据 / 本地层</p><p></p><ul><li><p>该层位于 Kafka Broker 的本地磁盘中，存储最新数据，针对高吞吐量写入与低延迟读取进行优化。</p></li></ul><p></p><p>冷数据 / 远程层</p><p></p><ul><li><p>该层采用独立的、通常成本更低且扩展性更强的存储系统。旧数据段会被异步上传至这一远程层，从而释放 Broker 的本地磁盘空间。</p></li></ul><p></p><p>数据流转</p><p></p><ul><li><p>仅当日志段关闭后，才会将其上传至远程层。消费者可从任意一层读取数据；若 Broker 本地无目标数据，则 Kafka 会从远程层拉取数据。</p></li></ul><p></p><p><strong>分层存储宣称的优势</strong></p><p></p><ul><li><p>成本更低：旧数据存储在 S3/GCS 等远程存储中，而非昂贵的 Broker 本地磁盘。</p></li></ul><ul><li><p>弹性更强：存储与计算可实现更高程度的独立扩展。</p></li></ul><ul><li><p>运维更优：本地数据量减少，Broker 重启与恢复速度更快。</p></li></ul><p></p><p>从理论上看，这是一个巧妙的折中方案：将热数据就近存储以保证性能，将冷数据迁移至远程存储以降低成本。</p><p></p><h2>为何分层存储仍未真正解决问题</h2><p></p><p>接下来，我将分享我的观点：我认为分层存储只是对深层问题的 “治标不治本”。</p><p></p><p>还记得我们提到的 1GB 电商数据最终膨胀至约 12GB 的案例吗？分层存储无法解决这一根本性问题。这就好比在房屋地基开裂时，却只对厨房进行翻新。</p><p></p><p>让我们逐一分析其中原因。</p><p></p><p><strong>问题 1：难以摆脱的 “热数据长尾”</strong></p><p></p><p>Kafka 必须将活跃数据段存储在本地磁盘中，这一规则始终不变。只有当数据段 “关闭” 后，才可能被迁移至远程层。</p><p></p><p>一个活跃数据段的大小可能是 1GB，在黑色星期五等流量高峰时段甚至可能达到 50GB。若乘以 3 倍副本因子（RF=3），仅单个分区就需要在昂贵的本地磁盘中存储 150GB 数据。</p><p></p><p>因此，尽管旧数据被迁移至远程存储，但热数据长尾依然存在，且数据量可能非常庞大。</p><p></p><p><strong>问题 2：分区重分配仍令人头疼</strong></p><p></p><p>新增 Broker？重新平衡分区？分层存储仅能起到微小的缓解作用。</p><p></p><p>举例来说：</p><p></p><ul><li><p>无分层存储时：可能需要迁移 500GB 数据，耗时长达 12 小时，过程痛苦。</p></li></ul><ul><li><p>有分层存储时：可能仅需迁移 100GB 热数据，耗时缩短至 2-3 小时。</p></li></ul><p></p><p>不可否认，分层存储确实有所改善。但如果你的网站在结账高峰期出现故障，等待数小时迁移数据仍然无法接受。扩展瓶颈依然存在。</p><p></p><p><strong>问题 3：隐性的复杂性代价</strong></p><p></p><p>我的工程师思维这样总结道：</p><p></p><p>“现在我需要管理两个存储系统，而不是一个。我既要排查本地磁盘问题，又要处理 S3 相关问题。监控指标翻倍，告警数量翻倍。有时数据甚至会卡在两层之间无法流转。”</p><p></p><p>分层存储并未简化运维，反而增加了更多移动部件。这就好比为了整理凌乱的书桌，却买了一张新的书桌 —— 问题并未得到根本解决。</p><p></p><p><strong>我的结论</strong></p><p></p><p>分层存储设计巧妙，也确实能降低存储成本，但它无法解决 Kafka Shared-Nothing 架构中计算与存储深度耦合的根本问题。你仍需为热数据层成本、扩展摩擦与运维复杂性付出代价。</p><p></p><p>真正值得思考的问题并非 “如何降低 Broker 磁盘成本”，而是 “Broker 是否真的需要拥有磁盘”。</p><p></p><p>这正是 AutoMQ 等项目进一步探索的方向 —— 让 Broker 实现无状态，由共享云存储保障数据持久性。</p><p></p><h2>但是……Broker 仍是有状态的，不具备云原生特性</h2><p></p><p>随着我对 Kafka 的使用不断深入，我开始质疑其核心设计假设。</p><p></p><p>回顾我们此前讨论的 Kafka 各类缺陷，它们都指向一个缺失的关键特性：真正的云原生能力。</p><p></p><p>即便引入了分层存储，Kafka Broker 依然是有状态的，存储与计算仍紧密耦合。扩展或恢复 Broker 时，仍需进行数据迁移。</p><p></p><p>为了让 Kafka 真正实现云原生，社区开始探索 Diskless Kafka（参见 KIP-1150），实现计算与存储的完全解耦。</p><p></p><p>这就好比谷歌文档（Google Docs）：不再将文件保存到本地硬盘，而是将所有数据存储在共享云空间中。Broker 不再 “拥有” 数据，仅负责连接共享存储。</p><p></p><p>试想这样的场景：</p><p></p><ul><li><p>无需管理本地磁盘。</p></li></ul><ul><li><p>Broker 崩溃时无需恐慌 —— 不会有任何数据丢失。</p></li></ul><ul><li><p>无需再经历痛苦的分区重分配。</p></li></ul><ul><li><p>新增 Broker？只需接入集群即可。</p></li></ul><ul><li><p>移除 Broker？毫无问题 —— 数据安全地存储在其他位置。</p></li></ul><p></p><p>这不就能解决我们此前讨论的半数难题吗？以上仅为我的个人思考，你或许能提出更优的方案。欢迎在评论区分享你的想法，或通过私信与我交流。</p><p></p><h2>Diskless Kafka 才是破局之道</h2><p></p><p>尽管 Apache Kafka 尚未推出 Diskless 版本，但 AutoMQ 等开源项目已实现了这一功能 —— 而我个人最欣赏的一点是，AutoMQ 与 Kafka API 实现了 100% 兼容。</p><p></p><p>早在 2023 年，AutoMQ 团队就着手打造真正云原生的 Kafka。他们很早就意识到，Amazon S3（及兼容 S3 的对象存储）已成为耐用云存储的事实标准。</p><p></p><p>AutoMQ 与 Kafka 实现 100% 兼容，但对存储层进行了彻底重构：</p><p></p><ul><li><p>所有日志段均存储在云对象存储（如 S3）中。</p></li></ul><ul><li><p>Broker 变得轻量且无状态，仅作为协议路由器。</p></li></ul><ul><li><p>数据的可信来源不再是 Broker 磁盘，而是共享存储。</p></li></ul><p></p><p>既然云服务商已提供近乎无限的容量、跨可用区副本与 “11 个 9” 的持久性，为何还要重新构建复杂的存储系统？AutoMQ 充分利用 S3（或兼容存储）保障数据持久性，Broker 仅负责数据的传入与传出。</p><p></p><p>这一设计带来了显著优势：</p><p></p><ul><li><p>轻松扩展：计算与存储可独立扩展。新增 Broker 以提升吞吐量，存储则在云中自动扩展。</p></li></ul><ul><li><p>快速重平衡：无需进行数据迁移。新增或移除 Broker 时，仅需重新分配 Leader 即可。</p></li></ul><ul><li><p>更高持久性：云对象存储无需在 Broker 上维护 3 倍副本，即可提供数据冗余。</p></li></ul><ul><li><p>运维简化：Broker 可随时替换。若某个 Broker 故障，只需启动新的 Broker，无需进行副本同步。</p></li></ul><p></p><p>换言之，Broker 变得像 “牛群” 一样可替代，而非需要精心呵护的 “宠物”。</p><p></p><p>我最喜欢用这样的比喻来形容：这就好比谷歌文档，不再将文件保存到本地 “C 盘”，而是将所有数据存储在共享云盘中。Broker 仅提供访问能力 —— 数据本身始终安全地存储在云中。</p><p></p><p>AutoMQ 摒弃了每个 Broker 在本地磁盘囤积数据的模式，提出了共享存储理念：所有 Kafka 数据存储在一个公共云仓库中，任何 Broker 均可访问。这并非空想 ——AutoMQ 已通过与 Kafka 完全兼容的分支实现了这一设计，有效解耦了 Kafka 架构中的计算与存储。</p><p></p><p>本质上，他们选择站在 “巨人”（云服务商）的肩膀上，而非重复 “造轮子”。既然 S3 等服务已开箱即用地提供近乎无限的容量、跨可用区副本与极高的耐用性，为何还要从零构建复杂的存储系统？</p><p></p><p><img>https://static001.geekbang.org/wechat/images/24/24ed5e1e2b3098b23af126dcfa29873a<img></p><p></p><p>要理解 AutoMQ 的创新，不妨想象 Kafka 以谷歌文档的模式运行：Broker 不再将数据保存到本地 “C 盘”，而是写入一个所有人共享的云盘。具体而言，AutoMQ 的 Broker 是无状态的，仅作为轻量级 “交通警察”，解析 Kafka 协议并实现数据与存储之间的路由。Kafka 日志段不再存储在 Broker 磁盘中，而是以云对象存储（S3）作为可信来源。这一设计带来了诸多显著优势。</p><p></p><p>首先，数据持久性大幅提升 —— 你可利用 S3 内置的副本机制与可靠性，无需在不同 Broker 上维护 3 份数据副本。其次，成本显著降低 —— 大规模使用对象存储的成本远低于部署大量本地 SSD（尤其是考虑到这些 SSD 还需维护 3 倍副本）。此外，扩展变得几乎 “即插即用”。</p><p></p><p>需要更高吞吐量？只需新增更多 Broker 实例（计算资源），并将其指向同一存储即可；无需通过大规模数据迁移来重新平衡分区。Broker 变得像 “牛群” 一样可替代，而非 “宠物”—— 若某个 Broker 故障，新的 Broker 可立即启动并提供数据服务，因为数据安全地存储在其他位置。这正是 Kafka 此前一直难以实现的云弹性。正如一位 Kafka 云架构师所言：“存储在云中自动扩展，Broker 只需提供数据传入与传出的处理能力。”</p><p></p><p>最后，让我们总结 AutoMQ Diskless 架构带来的优势。</p><p></p><p><strong>Diskless 架构优势</strong></p><p></p><ul><li><p>轻松扩展：计算（Broker）与存储独立扩展。新增 Broker 以提升吞吐量，存储则在云中自动扩展。无需再过度配置磁盘空间，按实际使用付费即可。</p></li></ul><ul><li><p>快速重平衡：无需迁移分区数据。新增或移除 Broker 时，仅需重新分配 Leader，过程几乎即时完成。</p></li></ul><ul><li><p>更高持久性：对象存储提供 “11 个 9” 的耐用性，远优于 Broker 副本机制。</p></li></ul><ul><li><p>运维简化：Broker 故障无关紧要，只需替换即可。无需数据恢复或副本同步。</p></li></ul><p></p><p><strong>延迟挑战</strong></p><p></p><p>理论上，Diskless Kafka 堪称完美，但它存在一个问题：对象存储会引入延迟。</p><p></p><p>低延迟是 Kafka 的核心优势，而直接向 S3 或 GCS 写入数据会导致延迟增加，并产生 API 开销。</p><p></p><p>AutoMQ 在此处做出了明智的设计：引入预写日志（Write-Ahead Log，WAL）抽象。消息首先追加到一个小型、耐用的 WAL（基于 EBS/NVMe 等块存储）中，而长期持久性则由 S3 保障。这一设计在保持 Broker Diskless 特性的同时，有效降低了延迟。</p><p></p><h2>能否进一步优化？</h2><p></p><p>在某些场景中，延迟至关重要，例如金融系统、高频交易、低延迟分析等。对于这些场景，即便是 AutoMQ 的 WAL 方案，也需要进一步创新。</p><p></p><p>AutoMQ 已表示将推出更深入的专有 / 商业解决方案：</p><p></p><ul><li><p>直接写入 WAL：每条消息均写入耐用的云原生 WAL。</p></li></ul><ul><li><p>Broker 随后从缓存或内存中提供读取服务。</p></li></ul><ul><li><p>WAL 卷容量较小（如 10 GB），若某个 Broker 故障，可快速将其挂载到新的 Broker 上。</p></li></ul><p></p><p>这与 Kafka 的分层存储有何不同？</p><p></p><ul><li><p>分层存储：数据首先写入 Broker 磁盘，在 Broker 间复制，之后才将旧数据段迁移至 S3。</p></li></ul><ul><li><p>AutoMQ 的 Diskless 方案：完全无需 Broker 磁盘。数据持久性由云存储层直接保障，无需进行副本迁移。</p></li></ul><p></p><p>若某个 Broker 故障，只需将其 WAL 卷挂载到新的 Broker 上，新 Broker 即可无缝接续旧 Broker 的工作。存储的生命周期超越计算。</p><p></p><p>这是一个重大的思维转变：计算资源可随时替换，存储则保持稳定。</p><p></p><p>在部分场景中，延迟的影响至关重要。因此，上述方案可能并非完美适配，仍需进一步优化。深入研究后我发现，AutoMQ 已针对这类场景提供了相应解决方案，但该方案似乎属于其专有 / 商业产品范畴。</p><p></p><p>这一解决方案可能看似复杂，但彰显了真正的工程智慧，是下一代基于 S3 的 Diskless Kafka 方案。</p><p></p><p>当然，与 SSD / 本地磁盘相比，S3 的速度确实较慢。此外，还需提升向云存储（S3）写入数据的效率，以减少 API 开销。</p><p></p><p><img>https://static001.geekbang.org/wechat/images/fe/fed25258e52d849bdc0a94d5690b2d51<img></p><p></p><p>这与 Kafka 的分层存储是否相同？</p><p></p><p>我的第一反应也是如此：“等等，这难道不与 Kafka 将数据迁移至 S3 的分层存储方案一样吗？”</p><p></p><p>事实并非如此。二者的区别如下：</p><p></p><ul><li><p>在启用分层存储的 Kafka 中，数据仍需先写入 Broker 本地磁盘，Broker 间的副本复制（ISR）仍是必需步骤，之后才会将旧数据段迁移至 S3。</p></li></ul><ul><li><p>在 AutoMQ 中，完全无需本地磁盘。数据直接写入云原生存储中的 WAL，无需副本复制，因为云卷本身已具备耐用性与冗余能力。</p></li></ul><p></p><p>因此，这并非简单的优化，而是一种完全不同的设计。</p><p></p><p>若 Broker 故障怎么办？</p><p></p><p>这是一个很好的问题，也是我们接下来的 “顿悟” 时刻。</p><p></p><p>在 Kafka 中，若某个 Broker 故障，需重新分配分区并同步副本，过程十分痛苦。</p><p></p><p>而 AutoMQ 的处理方式完全不同：</p><p></p><ul><li><p>每个 Broker 本质上是一个挂载了耐用云卷（EBS 或 NVMe）的计算实例。</p></li></ul><ul><li><p>假设 Broker A 正在向其 WAL（EBS）卷写入数据，突然发生故障。</p></li></ul><ul><li><p>无需担心，数据仍安全地存储在 WAL 卷中。</p></li></ul><ul><li><p>集群会迅速将该 WAL 卷挂载到 Broker B 上，Broker B 可无缝接续 Broker A 的工作。</p></li></ul><ul><li><p>整个过程无数据丢失、无副本迁移、无需等待。</p></li></ul><p></p><p>本质上，在 AutoMQ 中，存储的生命周期超越 Broker。计算资源可随时替换，存储则保持稳定。</p><p></p><p>这与 Kafka 的设计理念存在巨大差异。AutoMQ 将计算与存储彻底解耦，这正是其设计的精妙之处。若你想深入了解，可查阅其官方文档。</p><p></p><h2>最后的思考</h2><p></p><p>若你能读到此处，感谢你的耐心阅读！</p><p></p><p>我们一直在探讨的理念简单却极具影响力：若用云存储取代本地磁盘，作为类 Kafka 系统的基础，会带来怎样的改变？</p><p></p><p>这一转变将大幅减少运维难题：</p><p></p><ul><li><p>无需再进行 Broker 重分配。</p></li></ul><ul><li><p>无需再为磁盘告警惊慌失措。</p></li></ul><ul><li><p>扩展变得 “即插即用”。</p></li></ul><p></p><p>令人振奋的是，AutoMQ 等项目正朝着这一方向探索，同时保持与 Kafka API 及工具的兼容性。</p><p></p><p><italic>今日好文推荐</italic></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p></div>",
            "link": "https://www.infoq.cn/article/OGyUaIfDwSHzByidNlM0",
            "pub_date": "2025-10-27 02:53:05",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "LcB8AsrqyOWMEGDl1BbB",
            "title": "谷歌云发布指南，阐述保护远程MCP服务器的关键策略",
            "image": "https://static001.infoq.cn/resource/image/6b/18/6b915854a68166008591cb560a325618.jpg",
            "description": "<div><p>谷歌云发布了一份，详细阐述了保护远程模型上下文协议（MCP）服务器的策略，尤其针对那些依赖外部工具、数据库和API的人工智能系统。该指南指出，尽管MCP显著提升了智能体的能力，但同时也引入了新的安全漏洞，包括工具投毒、提示词注入、动态工具操作、会话劫持、未经授权的访问以及数据泄露等风险。</p><p></p><p>在指南中，谷歌提出了一个以集中式MCP代理为中心的防御架构，这是一个介于客户端和远程MCP服务器之间的安全层。这个代理可在Cloud Run、Apigee或GKE等平台上部署，能够强制执行一致的访问控制、执行审计日志记录、应用机密和资源使用策略，并实时检测威胁，所有这些操作都不会改变各个MCP服务器的实现。</p><p></p><p>为了说明最佳实践，指南列举了组织应优先考虑的五种部署风险：因清单配置失误致使未经授权的工具意外暴露、会话劫持、伪装成合法端点的“幽灵”工具、令牌失窃或敏感数据外泄以及弱身份验证被轻易绕过。借助这种代理架构，这些潜在漏洞得以在大规模部署场景中得到有效缓解。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/76/767818a4302bab8e04f51015f018b61f.png<img></p><p></p><p></p><p>谷歌强调，由于MCP服务器通常会暴露给远程或外部访问，保障身份安全、传输安全以及架构稳固性绝非可选操作，而是必要的基础防线。集中式代理模型的引入，为安全防护、可观测性以及治理效能提供了一个集中的强制执行枢纽，使其能够在不引发多个服务器实例漏洞扩散的前提下实现高效且安全的横向拓展。</p><p></p><p>谷歌云建议采用集中式代理来统一管理客户端与远程MCP服务器之间的通信。该代理能够强制执行访问控制、审计日志记录、机密策略以及安全传输，通过设立一个集中的强制执行点，而非依赖众多分散的服务器，有效减少了潜在的攻击面。此外，谷歌着重强调了识别特定风险向量的重要性，例如未经授权的工具暴露、会话劫持以及弱身份验证等问题，并明确指出，将身份认证、传输安全以及策略强制执行视为基础性要求，而非可选操作。</p><p></p><p>相比之下，尽管AWS尚未发布专门针对MCP的指南，但它提供了，用于保障远程服务器编排以及智能体工具的安全性。例如，AWS会话管理器允许用户在无需开启SSH/RDP端口的情况下对EC2实例进行远程访问管理。它使用IAM策略进行访问控制，并通过与CloudTrail以及其他监控工具的深度集成完成日志记录与审计工作。</p><p></p><p>AWS还通过安全组、VPC端点和最低权限IAM角色限制网络访问，为智能体和管理员提供更精细的访问控制。</p><p></p><p>Azure通过提供智能体管理能力。使用Azure Arc时，无论服务器位于Azure、本地还是其他云中，都会在其上部署“连接的机器代理”。通过Azure基于角色的访问控制（RBAC）和基于身份的认证机制（例如Entra ID）来强制执行访问控制策略。Azure还支持默认禁用远程访问，只有在经过明确配置后才会启用，并且可以审计智能体行为和凭据。</p><p></p><p>谷歌云、AWS和Azure这三大云服务供应商在远程智能体/远程服务器方面有重叠的安全理念：强制实施强有力的身份认证与访问控制机制，避免服务器直接暴露于公共互联网，采用集中式代理或智能体框架来增强安全性，确保完备的日志记录与审计追踪，遵循最小权限原则，并严格限制可访问或执行的代理及工具范围。谷歌云针对MCP发布的安全指南与这些通用的安全实践高度契合，还增加了更多关于协议特定威胁（工具投毒、提示词注入等）的警告，这些内容在AWS和Azure的智能体安全文档中往往没有如此明确的说明。</p><p></p><p>【声明：本文由InfoQ翻译，未经许可禁止转载。】</p><p></p><p><strong>查看英文原文</strong>：</p></div>",
            "link": "https://www.infoq.cn/article/LcB8AsrqyOWMEGDl1BbB",
            "pub_date": "2025-10-24 04:00:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        }
    ]
}