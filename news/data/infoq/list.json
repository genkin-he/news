{
    "data": [
        {
            "id": "e7QqfyYa9GebHrSC7IYt",
            "title": "重磅！华为开源业界首个Serverless分布式计算引擎openYuanrong，单机体验编程、极致分布式运行性能",
            "image": "https://static001.infoq.cn/resource/image/5d/66/5dd649e659e79158f6ca0ea5d6ae7366.jpg",
            "description": "<div><p>作者 | openYuanrong 团队</p><p>策划 | 华卫</p><p></p><p>近期，华为开源了自研的 Serverless 分布式计算引擎 openYuanrong。openYuanrong 已经在华为 MetaERP、小艺、华为云、终端云、ICT、海思等核心产品和平台广泛使用，是华为面向分布式计算领域长期积累下来的核心竞争力产品，也是业界首个统一支持通用计算和 AI 智能计算、灵衢超节点多种分布式应用场景的开源 Serverless 分布式计算引擎。同时，工商银行参与了openYuanrong建设，在交易对手风险计量场景应用。该场景用蒙特卡洛模拟，基于该引擎实现CPU+GPU异构算力融合计算，对比Ray性能有一定提升。下阶段华为与工行围绕分布式推理、强化学习场景深化联创。</p><p></p><p>开源地址：</p><p></p><p></p><p>Serverless 计算以其单机体验编程、自动弹性扩缩、免运维、和屏蔽复杂基础设施等优点广受业界关注，但传统的 Serverless 产品（以 FaaS 为代表）大多是面向特定垂直领域定制、缺乏通用性。因此，面对复杂的应用，往往需要和现有微服务（容器）集群混合部署。如此便会造成多个基于异构软件技术栈的异构集群之间的互操作效率低下、集成和管理复杂。不仅如此，随着 AI 技术的普及，以及 AI Agent 新应用的兴起，通算和智算集群之间也需要相互集成，当前的各个集群软件之间缺乏协同，也带来开发运维复杂、集群资源割裂、跨系统数据流转开销大等严重问题。而 openYuanrong 则致力于以统一的 Serverless 技术栈支持上述各类通算和智算负载在统一资源池上的细粒度融合部署，实现各负载间的高效协同和资源复用，同时提高系统性能，提升计算资源的使用效率。</p><p></p><h2>经过华为核心场景锻造的 Serverless 分布式计算引擎</h2><p></p><ul><li><p>通用计算场景。华为 MetaERP 以 openYuanrong 为分布式底座构建业界首个 Serverless 的 ERP 系统。MetaERP 为华为公司整体提供 ERP 服务，包含上千个微服务、日处理百 TB 级数据，是目前国内最大的 ERP 系统之一。MetaERP 使用 openYuanrong 提供零代码修改方案将大量使用 Spring 开发的标准微服务直接进行 Serverless 化，和 FaaS 原生的扩展服务实现共集群部署。openYuanrong 的快照极速冷启动等方案使得大型 Java 微服务启动时延从 90s 缩短至 1.4s，满足了 Serverless 自动水平弹性，以及无请求缩容到 0 的需求。同时，MetaERP 还采用了 openYuanrong 提供的 Serverless 流处理方案，实现性能优于 Flink 的大数据处理流水线。通过 openYuanrong 提供的 Serverless 技术，MetaERP 整体提升开发效率 60%，资源成本节省 30％。</p></li></ul><p></p><p><img>https://static001.geekbang.org/wechat/images/22/2222dfc7fe575481ab2cc70caea955e8.jpeg<img></p><p></p><ul><li><p>智能计算场景。华为小艺服务广泛采用 openYuanrong 作为其云端 AI 训 - 推 - 用的 Serverless AI 基础设施。传统方案下，SFT 训练、强化学习、大模型推理、AI Agent 分别采用不同的软件技术栈，独立集群部署（如在训推场景中使用 Ray，而 Agent 实例则运行在 K8S 管理的容器中），带来了大量的开发运维成本。小艺基于 openYuanrong 在单一资源池上构建了统一的 Serverless AI 平台，同时支持了 SFT 训练、强化学习、模型推理、AI Agent 四个领域负载的融合部署和资源灵活调度，在极大简化了分布式系统的开发运维的同时提升了资源利用率。利用 openYuanrong 的分布式内核技术，小艺的强化学习训推任务调度端到端时延减半；大模型推理实例启动速度从分钟级下降到秒级（如 Llama2-70B 水平弹性时延从 571 秒缩短至 4.55 秒）。基于 openYuanrong 的异构分布式内存数据系统，分布式 KV Cache 整体性能提升 1 倍以上，同时强化学习过程中，训推转换时实例间参数同步时延也从分钟级缩短至秒级。相较于业界类似产品（如 Ray），openYuanrong 提供了更高的性能和弹性调度能力，更成熟稳定，同时也支持通算和智算负载的融合部署，构建全流程的 Serverless AI 平台。</p></li></ul><p></p><p><img>https://static001.geekbang.org/wechat/images/fa/fa82b171fc3c2cf7d40938b811d46fe4.png<img></p><p></p><h2>openYuanrong 的整体架构</h2><p></p><p>openYuanrong 从架构上看主要由三部分组成：</p><p></p><ul><li><p>多语言函数运行时：一套以函数、状态、数据对象、数据流为核心抽象的分布式编程接口和高效实现，其中“函数”类似单机 OS“进程”，可以表达任意分布式应用的运行实例，并天然支持相互间直连互调或通过分布式内存共享传递数据，无需经过网关中转或外部存储。支持 Python、Java、C++ 等主流编程语言及相互间跨语言调用，使能用户以单机体验的编程方式高效开发各类分布式应用。</p></li></ul><ul><li><p>函数系统：Serverless 大规模分布式动态调度，支持函数动态生命周期管理（函数实例可在运行过程中动态创建 / 删除、长时间运行、休眠 / 唤醒），支持包括 Java 微服务、大模型推理在内的各类应用实例秒级冷启动和快速水平弹性，支持实例自动垂直弹性和跨节点迁移，实现节点内高密部署和集群资源高效利用。</p></li></ul><ul><li><p>数据系统：近计算的异构分布式内存数据缓存，提供 Object、Stream、KV 等语义，实现函数实例间高性能数据共享及传递。通过 HBM/DDR/SSD 多级缓存，支持节点内基于共享内存的免拷贝数据访问、节点间基于 D2D/H2H/D2H/H2D 高速传输的高效分布式数据访问。</p></li></ul><p></p><p><img>https://static001.geekbang.org/wechat/images/8c/8c030ac0c51d87d5ec74e664738a0710.png<img></p><p></p><p>本质上 openYuanrong 提供了一套统一 Serverless 架构支持各类分布式计算场景，通过提供多语言函数编程接口，以单机编程体验简化分布式应用开发；通过提供分布式动态调度和数据共享等能力，实现分布式应用的高性能运行和集群的高效资源利用。</p><p></p><h2>openYuanrong 与业界同类系统的异同点</h2><p></p><p>业界同类系统有 Ray、AWS Lambda 等。和 Ray 相比，Ray 和 openYuanrong 都支持面向 Python 的“单机体验编程、分布式运行”；但和 Ray 主要面向 Python 编程语言不同，openYuanrong 采用多语言运行时和分布式内核分离的架构，能够支持更丰富的编程语言，也支持更大规模集群调度，以及极致水平 / 垂直弹性和跨节点迁移。同时，openYuanrong 也构建了比 Ray Object Store 性能更高、功能更全的分布式内存数据系统，实现高性能数据共享 / 流转。此外，openYuanrong 的分布式管理架构也支持分布式高可靠运行，消除了单点故障。和 AWS Lambda 等 FaaS 系统相比，FaaS 只适用于特定的工作负载，而 openYuanrong 是业界首个将 Serverless 从专用场景扩展到通用分布式场景的系统。</p><p></p><p>openYuanrong 已在 openEuler 社区全面开源，采用 Apache 2.0 License，期待更多优秀的开发者参与，共同定义智能时代的分布式计算新范式。</p></div>",
            "link": "https://www.infoq.cn/article/e7QqfyYa9GebHrSC7IYt",
            "pub_date": "2025-10-29 09:30:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "ZCe0w3QBY1NZPbrAja8E",
            "title": "IDC 最新AI Infra报告解读｜AI 云爆发的背后：AI 应用全球化势不可挡",
            "image": "https://static001.infoq.cn/resource/image/89/43/89450fe1ca9656ee2b00404548888943.jpg",
            "description": "<div><p><img>https://static001.infoq.cn/resource/image/ef/a8/efbc7726f884035803f978c44b350aa8.png<img></p><p>近期 IDC 发布了一份《AI 原生云 / 新型云厂商重构 Agentic 基础设施》报告，报告中的调研数据显示：87% 的亚太企业在 2024 年至少部署了 10 个 GenAI 场景，预计到 2025 年这一数字将上升到近 94%。此外，IDC 调查发现，亚太地区日常使用 GenAI 的消费者从 2024 年的 19% 增加到 2025 年的 30%，企业 GenAI 的采用率也激增，65% 的亚太区企业将有超过 50 个 GenAI 场景投入生产，预计到 2025 年，26% 的企业将拥有超过 100 个应用。IDC 预测，到 2028 年中等以上的规模企业当中，至少会有上百个智能体在运转。</p><p></p><p>在此背景下，企业需要重新思考其 AI 基础设施，是否足以应对即将到来的智能体协同时代。这场变革不仅关乎技术升级，更是一场关于商业模式、市场格局和全球战略的深度博弈。</p><p></p><p>为了帮助更多 AI 从业者、企业决策者理解这份报告的核心内容，InfoQ 特别邀请了 IDC 中国研究总监卢言霞、GMI Cloud 创始人 & CEO Alex Yeh，从数据洞察与实战视角拆解报告，解读亚太地区 AI 基础设施的新趋势。</p><p></p><p><img>https://static001.geekbang.org/infoq/26/2613c5b8ac674be323f251cb41b816db.webp<img></p><p></p><p><italic>以下为经整理的直播内容精要。</italic></p><p></p><p><img>https://static001.infoq.cn/resource/image/dc/2b/dc0ac9e3ef27ba6384c00f6ac063802b.png<img></p><p></p><p>AI 应用构建浪潮扑面而来，但技术栈深海处的「链式反应」，可能比表面热浪更具颠覆性。</p><p>IDC 指出，自 ChatGPT 2022 年发布以来，大型互联网公司以及部分初创企业聚焦在大模型训练，以期在基础模型领域占据市场领先地位。因此在 2022-2024 年间，AI 基础设施的投资更多聚焦在模型训练侧。进入 2025 年，大模型的预训练开始收敛，市场的焦点更多在于模型推理侧，AI 推理专用基础设施需求激增。</p><p></p><p>IDC 报告指出，在亚太市场， 2023 年只有 40% 的组织使用人工智能推理基础设施，但在 2025 年，这一数字增长到 84%。这一数据标志着 AI 产业正在从模型开发阶段进入大规模应用落地阶段。</p><p></p><p><strong>这种变化一方面正在模糊传统技术栈的界限，另一方面也催生出专为 AI 工作负载优化的新型云服务——AI Native Cloud.</strong></p><p></p><p><strong>什么是 AI Native Cloud（AI 原生云）？</strong>IDC 报告里定义：“需要同时满足 GPU 高密度算力、超低延迟网络、以及面向 GenAI 的编排与冷却等需求”。</p><p></p><p>卢言霞分析道：“未来企业可能有多个智能体，成千上万个智能体之间并行大规模交互，对分布式算力和模型间传输的要求已经与传统 AI 时代有很大不同。”更关键的是，从通用模型到行业定制化的转型中，模型调优、RAG 推理环节的增加催生了训推一体需求。</p><p></p><p>Alex 从技术角度分析了 AI Native Cloud 的核心技术壁垒：“<strong>首先是 GPU 集群的高效调度能力。</strong> 比如在泰国、越南、马来西亚等亚太区域间的算力调度，关键不在于硬件扩容，而是通过 K8s 等动态资源调配技术，将算力利用率稳定维持在 98% 以上。我们自研的 Cluster Engine 技术能在亚太四个节点间实时调度，甚至能利用时区差异 —— 当亚洲进入夜间时，美国客户可调用亚洲节点算力，让整体利用率持续攀升。这种调度稳定性还能避免训练任务中断，原本 10 天的训练周期可提前完成，这是高效调度能力的核心价值。</p><p></p><p><strong>其次是算力的适配能力。</strong> 不同 AI 场景的算力需求差异极大：量化训练的逻辑如同骨架般具有严格时序节点，需高算力密度支持，而视频扩散模型或图片处理可能用中低端显卡就能完成。因此，能否打造统一框架适配多元场景至关重要。我们的第二个产品 Inference Engine 正是为此设计 —— 它打通硬件适配层，让客户无需关注底层硬件，直接通过 API token 按调用量付费。不是按卡计费，而是根据文本、图像、语音等不同模态动态调配算力，用灵活的算力资源支撑‘按 token 计价’模式，这是算力适配的核心逻辑。</p><p></p><p><strong>再次是全链路的优化能力。</strong> 这正是 GMI Cloud 与传统云的差异所在：传统云仅提供‘多少张卡 + 多少存储’的资源组合，而我们会做分散式推理架构设计，效率远高于传统方案。基于 Inference Engine 的模型调度能力，我们的模型吞吐率、TTFT（首 token 生成时间）等指标均优于传统云厂商 —— 这源于从模型侧到硬件侧的深度调优，不再是单纯提供资源，而是将算力转化为直接可用的模型服务接口。”</p><p></p><p>他进一步补充了三点关键技术洞察，阐释了 AI Cloud 与传统云的根本区别：<strong>第一，架构范式转变：从虚拟化到裸金属。</strong> Alex 指出，传统云厂商受制于过去二十年的虚拟化架构，通常以虚拟机形式提供算力。然而，AI 计算，尤其是训练和低延迟推理，需要直接掌控底层硬件资源以避免虚拟化带来的性能损耗。</p><p>Alex 提到与其他云厂商合作时的体验，“但现在 AI Native 的新创公司，常常会需要 BareMetal（裸金属），因为需要控制到整个架构。” 这种对底层硬件的直接访问和控制，对于实现极致的性能优化和稳定性至关重要。</p><p></p><p><strong>第二，服务模式变革：从远程支持到陪伴式服务。</strong> AI 时代云服务的深度正在发生本质变化。“我们服务了很多训练类的客户，基本需要陪伴式服务，因为训练集群随时可能出现各种问题”，Alex 描述道。这种 “长期陪伴的服务能力” 要求云厂商的工程师团队几乎驻扎在客户现场，与客户共同调试和优化，这与传统云时代 “开个网站、基本不会坏” 的远程、标准化服务模式截然不同。GMI Cloud 为此建立专属 SLA 团队，承诺 10 分钟响应、1 小时问题诊断、2 小时系统恢复。</p><p></p><p><strong>第三，核心竞争壁垒</strong>：<strong>全球化合规与运营。</strong> 这一点在当前的国际环境下显得尤为关键。亚太地区数据法规碎片化，GPU 资源也相对抢手，这要求云厂商不仅要在技术上过硬，还必须具备在全球复杂的地缘政治和监管环境中安全、合规运营的能力。GMI Cloud 已在亚太建立多个合规节点，通过本地化集群 + 动态调度，满足不同区域的合规与延迟需求。</p><p></p><p>这三项要求共同构成了传统云厂商转型的壁垒。“很多传统云厂商或者 GPU 集群供应商很容易被过去的架构给限制住，而不能提供给客户更敏捷的产品，”Alex 总结道。而这恰恰为没有历史包袱、从一开始就围绕 AI 工作负载构建技术栈的新兴云厂商创造了巨大的市场机会，进而推动了 AI 云厂商的快速崛起。目前 GMI Cloud 正持续推进 “AI Factory” 计划，即将落地全亚洲最大的万卡液冷 GB300 集群，未来还将在东南亚、日本、中东、美国等区域布局，以支撑超大规模算力需求等。</p><p></p><p><img>https://static001.infoq.cn/resource/image/a8/ae/a8fecdc311439d80e61442fa7bd867ae.png<img></p><p></p><p>任何技术的革新，最终落地到企业的视角，除了提效，能否节省成本则是技术选型的另一考核要素。</p><p>亚太地区 AI 企业普遍采用多云策略，以规避供应商锁定、追求最佳性价比或满足数据本地化要求。然而，<strong>“算力资源分散在不同云平台、管理规则与接口五花八门” 的局面，构成了一个巨大的 “隐性成本黑洞”。</strong></p><p></p><p>卢言霞详细剖析了其中的挑战：“企业的管理成本变得非常高，这涉及到完全不同厂商的技术栈，它们的定价模式、服务水准协议（SLA）、技术支持方式都存在巨大差异。要实现这些异构技术栈的融合、保证不同平台间的兼容性，其整体的运营复杂度和成本是相当可观的。”</p><p></p><p>她进一步指出了更棘手的数据问题 —— 生成式 AI 应用往往需要从多个异构数据源读取数据。当企业设想一个核心智能体与内部成千上万的其他智能体进行并行交互时，这些数据和系统可能分布在不同的公有云、甚至私有的本地化基础设施中，其间的数据同步与协同成为了巨大的工程挑战。更关键的是，不同系统接口标准化程度低，多数定制开发系统接口不统一，进一步抬高了技术门槛。</p><p></p><p>面对这一行业痛点，Alex 阐述了 GMI Cloud 提供的 “统一算力纳管” 解决方案：</p><p></p><ul><li><p><strong>底层 GPU 硬件架构：提供高端 GPU 云与裸金属服务</strong>。通过顶级生态协作获取英伟达高端硬件资源，并为高性能和高控制权限要求的客户提供直接开放硬件层访问的裸金属方案，消除虚拟化损耗，适配泛互联网、自动驾驶等对性能与控制权要求严苛的场景。</p></li></ul><p></p><ul><li><p><strong>IaaS 层：Cluster Engine 平台。</strong> 基于 K8s 架构实现全球算力弹性调度，支持跨区域负载均衡与错峰复用，资源利用率达 98%+，并通过可视化工具实现实时监控与智能管理。</p></li></ul><p></p><ul><li><p><strong>MaaS 层：Inference Engine 推理引擎平台。</strong> 底层搭载 H200 芯片，集成 DeepSeek、Qwen 等近百个大模型，平台通过自研推理优化技术提升模型调用效率，提供统一 API 接口，支持文本 / 图像 / 视频多模态模型调用，实现 “按 token 用量付费” 的弹性服务。</p></li></ul><p></p><p>三层架构的协同形成了完整的算力价值闭环：底层硬件提供性能基础，Cluster Engine 实现资源高效流转，Inference Engine 交付即用模型能力，最终帮助企业破解算力分散、管理复杂、成本高企等核心痛点。</p><p></p><p><strong>除了多云管理带来的复杂度和成本挑战，企业在算力投入上还面临一个两难困境：“前期投入巨大，但后期利用率难以保证，导致闲置率高企”。</strong> 卢言霞观察到，这一问题在 2025 年上半年的中国市场尤为典型。“尤其是一些大型企业，之前投入了大量的一体机方案。在大模型浪潮爆发之前，中国市场对 AI 和 IT 的投入就比较重视算力基础设施的采购，但往往未能与最终的应用场景和效率紧密挂钩。”</p><p></p><p>传统 AI 时代，企业 IT 投入中硬件常作为固定资产，但技术迭代快（如几年前的芯片型号如今可能过时），加上 AI 应用未大规模落地，导致前期投入易形成浪费；而互联网企业因业务波峰波谷明显，新兴 AIGC APP 试点新功能时，也不适合过早投入硬件，否则可能因功能未留存造成资源闲置。</p><p></p><p>针对这一核心痛点，Alex 分享了 GMI Cloud 给客户的方案。“GPU 的迭代速度正在变得越来越快，从过去的 5-6 年缩短到现在的 3 年甚至更短。技术迭代的加速意味着硬件贬值的风险急剧增加。因此，我们提供了 ‘Rent versus Buy’（租用而非购买） 的服务方式。客户可以与我们签订三年的合同，以租用的方式获得顶尖的算力，并在合同结束后，可以根据需要轻松升级到最新的硬件，从而彻底避免了技术迭代带来的资产贬值风险。” 这种模式对于现金流敏感的新创 AI 应用公司尤其具有吸引力，因为它将沉重的固定资产投入转化为了灵活的运营成本。</p><p></p><p>Alex 强调:“这种深度合作模式也使得 GMI Cloud 与客户的关系从传统的 ‘供应商 — 采购方’转变为了 ‘战略伙伴、共同成长’。”实际上，当 AI 算力需求从 “标准化采购” 转向 “场景化定制”，传统云厂商 “卖算力资源” 的供应商模式也将发生改变。随着 AI 基础设施进入 “效果为王” 的深水区，云厂商的竞争力不再取决于 “有多少算力”，而在于 “能为客户的每一分算力投入创造多少商业价值”。</p><p></p><p><img>https://static001.infoq.cn/resource/image/6f/a2/6f883ce41dfdfe1b720d2745a21d39a2.png<img></p><p></p><p>GenAI 场景应用的加速，除了带来技术栈、需求、模式等变革外，也在深层次影响产业发展的风向和竞争格局。</p><p></p><p>报告数据显示，95% 的亚太企业正在同时部署训练和推理基础设施。<strong>从行业分布来看，泛互联网、制造业和具身智能成为推理设施投入增速最快的三大领域。</strong> 其中，泛互联网既包括传统大型互联网企业，也包含当下火热的 AIGC 应用，特别是中国企业出海的重点方向 ——AI 社交、内容生成等 to C 应用；制造业则涵盖高端器械、医疗器械、重工业设备等领域的出海企业，这些企业在海外建设智能制造工厂，带动了大模型和 AIGC 应用需求；具身智能领域的机器人企业，无论是新秀还是老牌厂商，在 AI 推理算力基础设施上的投入也呈指数级增长。</p><p></p><p>Alex 通过实战观察验证了这一趋势：“我们看到的最大需求来自泛互联网，接下来是制造业。这些需求可以细分为三种模态：语音、视频加图像、文本。” 他进一步解释道，语音包括语音转换、呼叫中心、陪伴应用；视频主要是电商领域，需大量图像与视频制作广告素材；文本则是 Copilot、会议摘要等工具。</p><p></p><p><strong>技术应用层面，多模态融合正成为场景爆发的核心方向。</strong>Alex 预判视频领域将迎来 “DeepSeek 时刻”，B200 相比 H100 速度提升两倍，原本生成 5 秒视频需要耗时 30 秒，未来可能缩短至 400 毫秒，实现即时生成，这将彻底改变内容生产方式。另外，电商、影片生成、短视频、动画、广告都是亚洲市场的热门领域，庞大的用户基数与场景红利，为 AI 技术提供了天然的试验场与商业化土壤。而开源与闭源格局的变化更是降低了入场门槛 ，中尾部企业无需自建大模型，通过 Finetuning 即可快速落地场景。</p><p></p><p><strong>推理需求的快速增长，也带动了 AI 基础设施市场的竞争格局重塑</strong>。传统公有云厂商与 AI Cloud/GPU Cloud 新型云厂商之间的市场份额变化呈现出明显趋势。卢言霞透露：“2024 年到 2025 年间，GPU Cloud 和新兴云厂商在整个生成式 AI 基础设施市场上可能占到 15% 左右的市场份额。不要小瞧这 15%，对基础设施这么庞大的市场来说已经是非常大的进展。”</p><p></p><p><strong>一个反常识的转变也在发生：亚太市场的算力玩家们正在从 “零和博弈” 走向 “竞合共生”。</strong>Alex 提到，不同于传统 IT 行业的 “要么我卖进、要么你卖进”，AI 赛道因算力普遍短缺，“合作潜力非常多，大家都不够用，就互相借卡、租卡”，泛互联网超大型企业、公有云甚至会与新兴 AI 云厂商合作，“他们不想持续砸钱买卡，直接向我们租，我们能在小地方快速建立集群，速度比他们更快”。</p><p></p><p>这种资源互补的模式，打破了传统市场的竞争壁垒，让算力资源流动更高效，为中国企业提供了更多合作机遇，也为中企 AI 应用出海提供了更多的支撑。</p><p></p><p>对于计划出海的中国 AI 企业，在直播最后，卢言霞给出了三点核心战略建议：</p><p></p><ul><li><p><strong>第一，建立负责任的 AI 体系</strong>，“现阶段对整个行业参与者非常重要”。随着生成式 AI 能力增强，伦理风险、内容合规等问题已引发全球监管关注，头部企业需优先构建全流程的 AI 治理框架，这不仅是准入门槛，更是长期信任的基础；</p></li></ul><p></p><ul><li><p><strong>第二，紧盯大模型能力进化</strong>，“大模型迭代快，要判断哪些能力可能由大模型直接提供，无需开发工具重复投入”。避免在通用能力上浪费资源，聚焦行业定制化的差异化价值；</p></li></ul><p></p><ul><li><p><strong>第三，重视 AI 专用基础设施建设</strong>，“传统 AI 时代企业对基础设施重视不足，如今生成式 AI 广泛部署，必须关注面向 AI 工作负载优化的基础设施”，尤其是训推一体、低延迟网络等核心能力，这是业务落地的技术基石。</p></li></ul><p></p><p>对中国企业而言，只有抓住推理市场新机遇，在性能、合规、成本间找到平衡点，才有机会在算力变革的浪潮中抢占先机，从 “AI 应用追随者” 稳步进阶为 “区域规则的共建者”，进而在全球市场竞争中筑牢优势。</p><p></p><p><img>https://static001.infoq.cn/resource/image/5c/95/5c1ea66f943e5888a5dbb8dba7085595.png<img></p><p></p><p>卢言霞与 Alex 指出，亚太 AI 基础设施的变革本质是技术与产业话语权的双重重构。随着推理基础设施渗透率稳步提升，传统云“卖资源”的模式正在失效，取而代之的是“技术栈 + 服务模式 + 全球布局”的综合实力较量。</p><p></p><p>从技术底层看，AI 原生云正通过裸金属架构、K8s 弹性调度等关键技术，将 GPU 算力利用率提升至行业领先水平，并实现训推一体的闭环优化。这一能力直接推动了 AI 应用（如智能体）从单一任务执行向复杂多场景协同的演进。以 GMI Cloud 的分布式推理架构为例，其通过 PD 分离、跨区域动态扩缩容等技术，稳定支撑了高并发实时推理，满足了多区域用户访问与智能体并行决策的需求。</p><p></p><p>对行业而言，AI 原生云的核心价值在于构建了一个高效、智能的“能力底座”。它通过将算力精准转化为业务迭代的直接生产力，助力企业驾驭高并发推理与智能体协同等复杂场景。亚太市场正在经历的，正是一场从“资源上云”到“智能用云”的深刻效能革命。</p><p></p><p><italic>点击</italic><italic>获取完整报告。</italic></p><p></p></div>",
            "link": "https://www.infoq.cn/article/ZCe0w3QBY1NZPbrAja8E",
            "pub_date": "2025-10-25 08:36:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "WhRl2cJktHCN5wrxAcvg",
            "title": "亚马逊云科技推出EC2容量管理器，实现集中式跨账户容量优化",
            "image": "https://static001.infoq.cn/resource/image/78/7a/78237cd7b0d794dddf959163da05dc7a.jpg",
            "description": "<div><p><color>最近，亚马逊云科技新推出了一个集中式解决方案——</color><color>，旨在通过单个界面监控、分析和管理客户所有账户和AWS区域内的EC2容量使用情况。</color></p><p><color> </color></p><p><color>以前，在跨多个可用区协调按需实例、竞价实例和容量预留（CR）等类型的数百个实例时，大规模运营</color><color>（Amazon EC2）的组织会面临高度复杂的管理开销。底层容量数据分散在AWS Management Console、Cost and Usage Reports(CUR)、Amazon CloudWatch和各种EC2 API中，造成了明显的运营障碍。</color></p><p><color> </color></p><p><color>为了应对这一挑战，EC2容量管理器将所有容量数据聚合到了一个统一的、跨账户、跨区域的仪表板中。此外，该服务每小时刷新一次容量信息，并且初始设置保留14天的历史数据用于即时分析。</color></p><p></p><p><img>https://static001.geekbang.org/infoq/00/003b246124752afb25da42b37588a9df.png<img></p><p></p><p><color>（图片来源：</color><color>）</color></p><p><color> </color></p><p><color>亚马逊云科技安全英雄Sena Yakut</color><color>了这种整合的迫切需求：</color></p><p><color> </color></p><p><color> </color></p><p><color>主仪表板提供了一个综合视图，上面显示了所有实例类型的利用率，提供了基于vCPU、实例计数或估计成本（使用按需费率计算）的指标。其核心功能包括：</color></p><p><color> </color></p><ul><li><p><span style=\"color:#494949\">预留指标：可视化跟踪已使用与未使用预留容量的比例，可直接反映预留效率。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">竞价分析：竞价标签聚焦使用模式，显示关键指标，如竞价实例在被中断前的平均运行时长。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">直接管理：对于操作人员来说，一个特性是能够直接从容量管理器界面修改按需容量预留（ODCR），当预留资源位于同一个账户下时，这可以减少上下文切换并简化响应性更改。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">数据导出和集成：容量管理器支持将数据导出到Amazon S3，这使得组织可以保留超过标准保留期限（90天）的容量数据，用于长期趋势分析以及与外部商业智能（BI）工具集成。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">AWS Organizations集成：设置部分原生支持集中化的企业范围的容量可见性，以及跨多个账户的委托访问控制，从而简化治理流程。</span></p></li></ul><p></p><p><img>https://static001.geekbang.org/infoq/86/86ea5eacafd71e6dd59b3a7c3675f7f8.png<img></p><p></p><p><color>（图片来源：</color><color>）</color></p><p><color> </color></p><p><color>该公告在开发者和财务运营社区中引发了褒贬不一的反应。许多人对此表示欢迎，认为这减轻了运营压力，尤其是对成本管理专业人员而言。在</color><color>中，一位参与者评论道：</color></p><p><color> </color></p><p><color> </color></p><p><color>在LinkedIn上，首席云架构师Ivo Pinto</color><color>了安全和访问控制方面的好处：</color></p><p><color> </color></p><p><color> </color></p><p><color>不过，也有人对这一功能的长期云价值主张表示了怀疑。在同一个Reddit讨论帖中，另一位评论者</color><color>，集中式工具并不能解决云弹性成本的核心问题：</color></p><p><color> </color></p><p><color> </color></p><p><color>在推特上，首席云工程师Jack Hendy</color><color>：</color></p><p><color> </color></p><p><color> </color></p><p><color>最后，Amazon EC2容量管理器在所有商业AWS区域中默认启用，并且不额外收取任何费用。</color></p><p><color> </color></p><p><color> </color></p><p><color>声明：本文为InfoQ翻译，未经许可禁止转载。</color></p><p><color> </color></p><p><color>原文链接：</color></p></div>",
            "link": "https://www.infoq.cn/article/WhRl2cJktHCN5wrxAcvg",
            "pub_date": "2025-10-29 04:00:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "FDc37HIdrfyWGv3UqXVA",
            "title": "黄仁勋凌晨炸场：6G、量子计算、物理AI、机器人、自动驾驶全来了！AI芯片订单已达3.5万亿｜2025GTC最全指南",
            "image": "https://static001.infoq.cn/resource/image/96/e6/96e4789d23f9486d3b5de16a4e4101e6.jpg",
            "description": "<div><p>本周可谓<strong>美国科技行业的“超级周”</strong>，而号称“AI风向标”的<strong>英伟达GTC</strong>（GPU Technology Conference，GPU技术大会）也于美东时间10月27日<strong>开幕</strong>。</p><p></p><p>10月28日，黄仁勋依旧身着皮衣登场。但与以往有明确重点的发布会不同，老黄此次演讲<strong>几乎把全球科技圈的热词悉数点了一遍：6G、量子计算、物理AI、机器人、自动驾驶、核聚变...</strong>一个没落下。</p><p></p><p>他还官宣了一个<strong>跨次元般的</strong>重磅消息：</p><p></p><p>英伟达和<strong>诺基亚</strong>合作了，用AI提高无线通信的速度，共同建造面向AI原生的移动网络，搭建6G AI平台；为此，英伟达推出新品<strong>NVIDIA Arc（Aerial Radio Network Computer）</strong>，还将对诺基亚投资10亿美元（约合人民71亿元）。</p><p></p><p><img>https://static001.geekbang.org/infoq/96/962bde9f6657f45727124ebfc87d63c3.png<img></p><p></p><p>也就是说，AI不再只是网络的使用者，还成为了网络本身的“智能中枢”。</p><p></p><p>黄仁勋直言：“我们将采用这项新技术，升级全球数百万个基站。”</p><p></p><p>值得一提的是，老黄这次（可能下次血本了）不仅演讲话题涉猎甚广，还组建了一支<strong>超级“嘉宾足球队”</strong>，在一众大佬中，具身智能当红新星<strong>Figure AI创始人兼CEO Brett Adcock</strong>（第二行左四）也赫然在列。</p><p></p><p></p><p>话说回来，<strong>英伟达AI芯片</strong>的成绩也着实亮眼：Blackwell和Rubin芯片订单总额，已达<strong>5000亿美元</strong>（约合人民币35,000亿元）。</p><p></p><p>在今年3月的春季GTC大会上，老黄曾秀出<strong>最强AI芯片GB300 NVL72</strong>，其中<strong>“G”</strong>是该芯片中的CPU架构<strong>Grace</strong>，<strong>“B”</strong>是GPU架构<strong>Blackwell</strong>，也是英伟达现在主推的数据中心GPU架构。</p><p></p><p>截至当日收盘，<strong>英伟达</strong>股价涨约5%，<strong>总市值逼近5万亿美元（约合人民币35万亿元）</strong>，创下新高。</p><p><img>https://static001.geekbang.org/infoq/b3/b3c03b0c2ab8ac8383c78cbafb996e04.png<img></p><p></p><p></p><h2>次元壁破了：和老牌手机霸主一起搞通信</h2><p>在这次GTC上，老黄官宣的第一个合作协议，就是和诺基亚携手共建6G AI平台。</p><p></p><p>二者此次<strong>合作重点</strong>不在“造芯片”，而在<strong>“让网络学会思考”</strong>：英伟达把其加速计算平台<strong>Aerial RAN Computer Pro（ARC-Pro）</strong>，带进诺基亚的无线通信系统<strong>AirScale</strong>中，推动运营商向AI原生的5G与6G网络过渡。</p><p></p><p>话说，<strong>诺基亚</strong>这个零几年的全球手机霸主、听起来好像是个“上古战神”，现在<strong>为什么会出现在英伟达的合作名单上，而且还被放在超级显眼位置？</strong></p><p></p><p><strong>首先，其实诺基亚“没死”</strong>，它只是卖掉了手机业务，<strong>退回了“根部”的通信设备业务</strong>。</p><p></p><p>2013年，在卖掉手机部门给微软之后，诺基亚彻底转向<strong>电信基础设施</strong>：基站、天线、光纤网络、核心网软件——全球运营商的底层网络都离不开这些。</p><p></p><p>2016年，诺基亚收购了阿尔卡特朗讯——这是由传奇的<strong>“贝尔实验室”</strong>（曾获9项诺贝尔奖、4项图灵奖）改组重建的公司，然后就自然而然地继承了贝尔实验室一堆领先的技术和专利；收购后，贝尔实验室也更名为Nokia Bell Labs。</p><p></p><p>如今的诺基亚和华为、爱立信并列，是全球三大通信设备厂商之一。</p><p></p><p><strong>第二，英伟达做芯片的终极目标</strong>，其实不是“更强的GPU”，而是<strong>让所有计算都发生在“靠近数据产生的地方”——也就是网络边缘，</strong>而这正是诺基亚的主场。</p><p></p><p>英伟达提供 ARC-Pro平台：让通信基站不仅传信号，还能执行 AI 推理；诺基亚则提供AirScale 无线系统和RAN软件栈：让这些“AI 基站”融入现有的5G网络，并能平滑升级到 6G。另外，T-Mobile是他们的首个运营商合作方，将在 2026年开始实地测试。</p><p></p><p>简单来说，就是英伟达把GPU算力塞进基站，诺基亚负责让它能在真实的网络中跑起来。</p><p></p><p>正如黄仁勋在发布会上所说的：“基于NVIDIA CUDA和AI的AI-RAN，将彻底改变电信行业，这是一次跨时代的平台变革。”</p><p></p><p>英伟达的官方新闻稿也指出，此次合作标志着行业的转折点，通过在全球范围内推动AI-RAN的创新和商业化，为AI 原生6G铺平了道路。</p><p></p><p>在诺基亚之外，黄仁勋还一口气官宣了几家合作伙伴：从自动驾驶巨头Uber、政府AI供应商Palantir到与美国能源部、甲骨文达成战略合作——几乎覆盖了AI产业的每一个关键环节，暗含其野心。</p><p></p><p>先是Uber。黄仁勋认为“机器人出租车的拐点即将到来”，双方计划在全球铺开10万辆自动驾驶汽车。这不仅是自动驾驶的突破，更是AI硬件与智能算法的深度融合——英伟达正试图让GPU成为Robotaxi时代的“车载大脑”，推动出行生态进入商业化阶段。</p><p></p><p>接着是Palantir。这家擅长处理政府和军情数据的公司，将把英伟达的CUDA-X和Nemotron模型嵌入自家系统，让AI学会“看懂”世界。Lowe’s已经在用这套组合调度它的供应链——AI代理成了企业的隐形决策官。</p><p></p><p>不过更大胆的布局在科研领域，与美国能源部、甲骨文联手打造七台AI超级计算机。其中阿贡实验室的Solstice与Equinox系统，将以2,200 exaFLOP的惊人算力，成为“美国的探索引擎”。</p><p></p><p>从Robotaxi到政府级AI决策，再到超级计算，英伟达不再“造芯片”，而是在“造智能的地基”。</p><p></p><h2>前沿科技话题“大点兵”</h2><p></p><p>前文提到，老黄这次的演讲涉及多个话题，除了6G，还有<strong>量子计算、物理AI、机器人、自动驾驶等</strong>；下面来具体看几个。</p><h2>1、量子计算</h2><p>自理查德·费曼提出量子计算概念起，40年后，业界终于在去年实现了关键突破——创造出可相干、稳定、且具纠错能力的<strong>逻辑量子比特（logical qubit）</strong>。</p><p></p><p>英伟达此前推出了开放式量子GPU计算平台<strong>CUDA-Q</strong>，这次又开发了一个基于CUDA-Q核心构建的<strong>NVQLink，</strong>是一种能把传统GPU和量子处理器连接起来的互联架构。</p><p></p><p>当下的量子计算仍处在“易碎”阶段——对环境噪声异常敏感，且算力利用率有限。为了让量子比特保持稳定运行，往往要借助 GPU 超算系统承担控制与纠错计算，这让量子计算暂时还离不开经典计算的“辅助臂”。</p><p>黄仁勋分享称，英伟达将与美国能源部合作建设<strong>7台AI超级计算机</strong>，这些超算将使用Blackwell和下一代Vera Rubin架构芯片，利用 AI、量子计算等最前技术投入研究。</p><p></p><p>老黄还化身“AI赛道的美国队长”，举起了他的“芯片盾牌”：由NVLink连接的72块GPU构成。</p><p></p><p><img>https://static001.geekbang.org/infoq/ba/bab53b874868d7722e6f77cea2a837ca.png<img></p><p></p><h2>2、具身智能和物理AI（Physical AI）</h2><p>对于具身智能与机器人计算，英伟达的理念是，如果要让AI真正进入物理世界、具备感知和行动能力，必须依托一个“三计算机”体系：</p><p></p><p>一是用于模型训练的<strong>Grace Blackwell AI 计算机</strong>，负责生成大规模智能模型；二是用于仿真和虚拟验证的 <strong>Omniverse 数字孪生计算机</strong>，在虚拟环境中模拟机器人行为与物理交互；三是用于实际执行的 <strong>Jetson Thor 机器人计算机</strong>，让智能在真实世界中运行。</p><p></p><p>这三者都基于CUDA平台运行，形成从训练、仿真到执行的完整“物理智能”闭环，使AI能够真正连接虚拟与现实世界。</p><p></p><p>英伟达投资过多家具体智能公司，和美国具体智能新独角兽Figure AI也在开展合作，加速下一代机器人的研发。</p><p></p><p><img>https://static001.geekbang.org/infoq/3e/3ea8fd1b7156268885bc9b96f0803704.png<img></p><p></p><p></p><h2>3、开源模型与生态合作</h2><p>英伟达在开源模型和产业生态上正展开双线布局。</p><p></p><p>一方面，得益于推理、多模态与知识蒸馏等能力的提升，开源模型已经足够强大，成为初创企业和科研机构进行灵活定制与创新的基础。英伟达作为开源社区的重要贡献者，已有23个模型登上各类性能榜单，并承诺将持续投入。</p><p></p><p>另一方面，英伟达正加速与云计算和行业伙伴的深度集成：其模型与库已嵌入AWS、Google Cloud、Microsoft Azure等主流云平台，以及ServiceNow、SAP等 SaaS系统，使用户能够在不同生态中无缝调用AI能力。</p><p></p><p>同时，英伟达还与 CrowdStrike（网络安全）、Palantir（数据处理）和 Synopsys（芯片设计）等行业巨头合作，以 AI 提升垂直领域生产力，推动从安全到设计的智能化变革。</p><p></p><h2>4、AI在聊天机器人之外的更多应用</h2><p>AI 被广泛用于基础科学研究，远不止聊天或生成内容。在医疗、基因组学、企业计算等领域都有应用。不同类型的模型（卷积神经网络 CNN、图神经网络 GNN、状态空间模型等）被用于不同任务。</p><p></p><p>AI 不仅是工具，也能成为“数字员工”。例如英伟达内部的 <strong>Cursor</strong> 系统可帮助工程师自动生成代码；AI 驾驶员（AI Chauffeur）则被用于自动驾驶出租车。</p><p></p><h2>老黄还亲自详解：AI到底是什么？</h2><p>在GTC大会上，除了抛出一个又一个爆炸性的技术新动向，黄仁勋还特意拿出20分钟，讲了一堂“AI是什么”的深度课。</p><p></p><p>在他看来，这件事十分必要——如果没弄清AI的定义，就无法判断下一场产业潮水将流向何处。</p><p></p><p>黄仁勋首先澄清一个误区：<strong>AI的世界</strong>远不止ChatGPT所代表的聊天机器人，那只是大众心中的AI形象。<strong>真正关键的，是以AGI为代表的深层计算机科学，以及支撑它的惊人算力</strong>，“AI不是某个应用，而是一种新的计算方式。”</p><p></p><p>过去的计算世界，是程序员写规则、计算机执行命令；现在，机器靠数据自己学习规律。</p><p></p><p>比如，以前要教电脑识别猫，你得写几十条规则；如今只需给它十万张猫的照片，它自己就能学会什么是“猫”。</p><p></p><p>这场转变包含三层逻辑：计算方式变了——从写代码变成喂数据；计算工具变了——从CPU到GPU；计算目标也变了——从执行任务到生成智能。AI由此彻底重构了计算栈。</p><p></p><p>换句话说，AI正在从“螺丝刀”变成“工人”。</p><p></p><p>它不再是被动的工具，而是主动的执行者——会使用浏览器、写代码、制定计划、理解需求。</p><p></p><p>当技术第一次具备了“做事”的能力，机器也第一次进入了生产力的核心。</p><p></p><p>有了学习能力，AI看世界的方式也彻底不同。</p><p>在它的眼中，万物都被拆解成可学习的片段：文字、图片、声音、分子、蛋白质……这些最小的信息单位叫作token。</p><p>AI通过这些“语言颗粒”去理解、模仿、重建世界。谁能以更低成本、更高速度生成和操控token，谁就能主导下一代计算。</p><p></p><p>这也催生了一种全新的基础设施——AI工厂。</p><p></p><p>如果传统数据中心像多功能仓库，负责存储文件、运行程序，那么AI工厂就像一条生产线，只做一件事：生产token。</p><p></p><p>能源流入，驱动GPU；GPU通过NVLink和Spectrum-X网络连接成超级系统；软件与模型协同工作，批量产出token。</p><p></p><p><strong>黄仁勋把这条链概括成一句话：</strong></p><p><strong>“能源 → GPU → 算法 → 模型 → Token → 智能。”</strong></p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/f4/f4c48f7bceaa07695874eaefc8b24e0a.png<img></p><p></p><p></p><p>AI的成长轨迹，也像一个“吃电的天才”。</p><p></p><p><strong>它先在预训练阶段“学语言、记知识”，像学前班；</strong></p><p><strong>后在后训练阶段“学技能、学推理”；</strong></p><p><strong>最后进入“思考阶段”，在与你互动时实时规划、推理——那是最消耗算力的时刻。</strong></p><p></p><p>AI越聪明，就越费电。这是第一条指数——AI使用指数：智能与算力的爆炸式增长。</p><p></p><p>但还有第二条曲线——AI采用指数：AI越好用，越多人使用；使用越多，算力需求又被推高。</p><p></p><p>两条指数叠加，形成了一个强劲的正反馈——AI越聪明，体验越好；越好用，用户越多；越多的使用带来更多利润，又反过来推动AI变得更聪明。</p><p></p><p>面对算力需求如此巨大，要让这台“永动机”持续转动，黄仁勋的答案是“极致协同设计（Extreme Co-Design）”。</p><p></p><p>摩尔定律的线性提升早已赶不上AI的指数爆炸，必须在芯片、封装、互联、系统、编译器、模型、算法、应用等每一层同时创新。</p><p></p><p>他说：“要让AI继续前进，不能只造更快的芯片，而要重新设计整座工厂。”</p><p></p><p>这座“工厂”，就是未来的AI工厂。</p><p></p><p>在黄仁勋眼中，它不再是通用数据中心，而是一条智能的生产线——能源是燃料，GPU是引擎，模型是模具，Token是产品。</p><p></p><p>算力不再是辅助资源，而是新的生产资料。只有不断压低成本、扩大产能，AI的良性循环才能继续。</p><p></p><p>这就是未来计算的形态。</p><p></p><p>AI工厂将成为现代经济的新基础设施，从科学、医疗、制造到娱乐，所有行业都将围绕AI工厂重构。</p><p></p><p>从Arvin在Perplexity的工作、软件开发中的Cursor，到机器人出租车中的AI司机。人工智能正快速渗透到过去难以触及的经济领域，占据越来越广阔的版图。</p><p></p><p>那20分钟里，他讲的其实不只是AI，而是人类第一次拥有一种能把能量直接转化为智能的机器。</p><p></p><p>未来的计算，不再是让电脑执行命令，而是让世界自己学会思考。</p><p></p><p>老黄演讲视频回看地址：</p><p>https://www.youtube.com/watch?v=lQHK61IDFH4&list=TLGG23pf8VjteXoyODEwMjAyNQ</p><p>参考链接：</p><p>https://nvidianews.nvidia.com/news/nvidia-nokia-ai-telecommunications</p><p>https://www.reuters.com/world/asia-pacific/nvidias-huang-speak-washington-investors-look-hints-china-2025-10-28/</p></div>",
            "link": "https://www.infoq.cn/article/FDc37HIdrfyWGv3UqXVA",
            "pub_date": "2025-10-29 00:57:13",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "UF3guSh5REtRDy36ddwL",
            "title": "自带密钥（BYOK）：亚马逊云科技采用CMK以满足企业合规性的需要",
            "image": "https://static001.infoq.cn/resource/image/73/52/737c7e440457648e49b4ba2aa39e4552.jpeg",
            "description": "<div><p><color>亚马逊云科技最近宣布，其IAM身份中心（IAM Identity Center）服务支持</color><color>以用于静态加密。组织可以使用自己的密钥来加密身份中心的身份数据。</color></p><p><color> </color></p><p><color>IAM身份中心是一项云服务，它集中管理对多个AWS账户和云应用程序的单点登录（SSO）访问。虽然身份中心的数据一直使用AWS持有的KMS密钥进行静态加密，但新的CMK支持允许组织使用自己的密钥来加密他们的员工身份数据，例如用户和组属性。</color></p><p><color> </color></p><p><color>与</color><color>的集成至关重要，因为它将加密密钥的生命周期（创建、轮换和删除）的控制权直接转移到客户手中。</color></p><p><color> </color></p><p></p><p><img>https://static001.geekbang.org/infoq/c9/c90db2616af255f4c3d84b4ae27f41b6.png<img></p><p></p><p><italic>(图片来源：</italic><italic>)</italic></p><p><color> </color></p><p><color>亚马逊云科技的IAM身份中心的高级产品经理Alex Milanovic在LinkedIn的</color><color>中总结了它的核心优势：</color></p><ul><li><p><span style=\"color:#494949\">完全控制他们自己的加密密钥。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">通过KMS和IAM策略对身份数据进行细粒度访问管理，确保只有授权主体才能访问他们的加密数据。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">通过详细的AWS CloudTrail密钥使用日志增强审计能力。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">对于需要数据主权的受监管行业，这能够加强合规性。</span></p></li></ul><p><color> </color></p><p><color>亚马逊云科技的开发布道师Sébastien Stormacq进一步详细说明了它所支持的控制级别：</color></p><p><color> </color></p><p><color>基于审计和监管的需求，整个过程会通过</color><color>进行记录，提供密钥使用的详细记录。这种对加密密钥的细粒度控制通常是在高度受监管行业运营的企业的一个先决条件。</color></p><p><color> </color></p><p><color>由于合规性或安全策略，使用CMK对静态数据进行加密是企业的标准要求，如</color><color>。其他超大规模的提供商和产品也通过它们各自的密钥管理服务广泛支持它。</color></p><p><color> </color></p><p><color>微软Azure通过</color><color>实现了这一点，使客户能够在各种服务中加密敏感数据，并通过</color><color>验证访问。同样，谷歌云通过</color><color>提供CMK，为Cloud Storage和BigQuery等服务提供加密边界和完整的密钥生命周期控制。</color></p><p><color> </color></p><p><color>身份中心服务支持单区域和多区域密钥，以满足用户的部署需求。但是，目前，身份中心实例只能在单个区域部署。尽管如此，公司建议使用多区域AWS KMS密钥，除非公司政策要求用户使用单区域密钥。它指出，多区域密钥会在区域间提供一致的密钥材料，同时在每个区域保持独立的密钥基础设施。</color></p><p><color> </color></p><p><color>最后，该功能目前在所有AWS商业区域、AWS GovCloud（美国）和AWS中国区域均可用。此外，在定价方面，用户需要为身份IAM中心付费，对于标准AWS KMS，密钥存储和API使用会产生费用。</color></p><p><color> </color></p><p><color>查看英文原文：</color></p></div>",
            "link": "https://www.infoq.cn/article/UF3guSh5REtRDy36ddwL",
            "pub_date": "2025-10-27 05:00:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "OGyUaIfDwSHzByidNlM0",
            "title": "当 Kafka 架构显露“疲态”：共享存储领域正迎来创新变革",
            "image": "https://static001.infoq.cn/resource/image/0b/11/0b8fd03b29352dea29305a6ayy4d4311.jpg",
            "description": "<div><p></p><p></p><h2>Kafka：数据运营与数据分析之间的桥梁</h2><p></p><p>我已经使用 Apache Kafka 多年，并且非常喜欢这个工具。作为一名数据工程师，我主要将它用作连接数据运营端与数据分析端的桥梁。凭借优雅的设计和强大的功能，Kafka 长期以来一直是流处理领域的标杆。</p><p></p><p><img>https://static001.geekbang.org/wechat/images/b0/b0441c7183e351481db9fb74463b6898<img></p><p></p><p>Kafka 扮演着连接数据运营端与数据分析端的桥梁角色。</p><p></p><p>自问世以来，Kafka 就凭借独特的分布式日志抽象，塑造了现代流处理架构。它不仅为实时数据流处理提供了无可比拟的能力，还围绕自身构建了完整的生态系统。</p><p></p><p>Kafka 的成功源于其核心优势：能够大规模地实现高吞吐量与低延迟处理。这一特性使其成为各类规模企业的可靠选择，并最终确立了其在流处理领域的行业标准地位。</p><p></p><p>但 Kafka 的发展之路并非一帆风顺。它的成本可能急剧攀升，而在流量高峰时段进行分区重分配等运维难题，更是令人头疼不已。</p><p></p><p>我至今还记得在沃尔玛工作时的经历：曾花费数小时排查一次恰逢流量高峰发生的分区重分配问题，那次经历几乎让我心力交瘁。</p><p></p><p>尽管成本居高不下，Kafka 在流处理领域的主导地位依然稳固。在如今云优先的大环境下，一个多年前基于本地磁盘存储设计的系统，至今仍是众多企业的核心支撑，这着实令人意外。</p><p></p><p>深入研究后我发现，背后的原因并非 Kafka “完美无缺”，而是长期以来缺乏合适的替代方案。其最大的卖点 —— 速度、持久性与可靠性，至今仍具有重要价值。</p><p></p><p>但只要使用过 Kafka，你就会知道：它将所有数据都存储在本地磁盘上。这一设计暗藏着一系列成本与挑战，包括磁盘故障、扩展难题、突发流量应对，以及受限于本地或私有部署存储容量等问题。</p><p></p><p>几个月前，我偶然发现了一个名为 AutoMQ 的开源项目。起初只是随意研究，后来却深入探索，彻底改变了我对流处理架构的认知。</p><p></p><p>因此，在本文中，我们希望分享两方面内容：一是 Kafka 传统存储模型面临的挑战，二是以 AutoMQ 为代表的现代解决方案如何通过云对象存储（而非本地磁盘）另辟蹊径解决这些问题。这一转变在保留 Kafka 熟悉的 API 与生态系统的同时，让 Kafka 具备更强的扩展性、更高的成本效益与更优的云适配性。</p><p></p><h2>不容忽视的问题：Kafka 为何停滞不前</h2><p></p><p>坦白说，Kafka 十分出色，它彻底改变了我们对数据流的认知。但每当我配置昂贵的 EBS 卷、看着分区重分配进程缓慢推进数小时，或是凌晨 3 点因某个 Broker 磁盘空间耗尽而被惊醒时，我总会忍不住思考：一定有更好的解决方案。</p><p></p><p>这些问题的根源何在？答案是 Kafka 的 shared-nothing 架构。每个 Broker 都像一个 “隐士”：独自拥有数据，将其小心翼翼地存储在本地磁盘上，拒绝与其他 Broker 共享。这种设计在 2011 年合情合理，当时我们使用私有部署服务器，本地磁盘是唯一的存储选择。但在如今的云时代，这就好比在所有人都使用谷歌云盘（Google Drive）的情况下，仍坚持使用文件柜存储数据。</p><p></p><p>这种架构实际带来了以下成本负担：</p><p></p><ul><li><p>9 倍的数据冗余（没错，你没看错 ——Kafka 3 倍副本 × EBS 3 倍副本）。</p></li></ul><ul><li><p>分区重分配进程极其缓慢，如同看着油漆变干。</p></li></ul><ul><li><p>完全缺乏弹性 —— 尝试对 Kafka 进行自动扩展，你会发现整个周末都要耗费在这上面。</p></li></ul><ul><li><p>跨可用区（AZ）流量费用高到让首席财务官（CFO）头疼。</p></li></ul><p></p><p><img>https://static001.geekbang.org/wechat/images/39/39a35a1a9805884f3b4857bcbfc2615d<img></p><p></p><h2>Kafka 的运维成本：Shared-Nothing 架构的代价</h2><p></p><p>我想通过一个故事，直观展现 Kafka 的成本问题。</p><p></p><p>假设你运营着一个小型电商网站，每小时仅摄入 1GB 数据，包括用户点击、订单信息、库存更新等，数据量并不算大。在过去，你只需将这些数据存储在一台服务器上即可。但如今是 2025 年，为确保高可用性，你选择部署 Kafka。</p><p></p><p>而 Shared-Nothing 架构在此刻开始让你付出高昂代价。</p><p></p><p><strong>Shared-Nothing 的真正含义</strong></p><p></p><p>在 Kafka 的体系中，“Shared-Nothing” 意味着每个 Broker 都像一个 “多疑的隐士”，彼此之间不共享任何资源 —— 无论是存储、数据，还是其他任何东西。每个 Broker 都拥有独立的本地磁盘，自行管理数据，本质上把其他 Broker 当作 “恰好共事的陌生人”。</p><p></p><p>这就好比三个室友拒绝共享 Netflix 账号，反而各自付费订阅，将相同的节目下载到自己的设备上，并小心翼翼地守护着自己的密码。听起来成本很高？事实确实如此。</p><p></p><p><strong>三重（甚至更严重的）打击</strong></p><p></p><p>接下来，让我们看看成本问题有多棘手。</p><p></p><p><img>https://static001.geekbang.org/wechat/images/cc/ccf09819d26f4490ebfe903e79217460<img></p><p></p><p>请仔细观察上图。</p><p></p><p>现在，让我们跟踪 1GB / 小时的数据在 Kafka 副本机制中的流转过程：</p><p></p><ul><li><p>第 1 小时：应用产生 1GB 数据。</p></li></ul><ul><li><p>Kafka 副本（副本因子 RF=3）：1GB 数据在 Broker 间复制为 3GB。</p></li></ul><ul><li><p>EBS 副本：这 3GB 数据的每个副本又被 AWS 复制 3 份，最终变为 9GB。</p></li></ul><ul><li><p>预留空间：为避免午夜告警，需额外预留 30%-40% 的缓冲空间，最终需配置约 12GB 存储。</p></li></ul><p></p><p>也就是说，每摄入 1GB 数据，你需要为约 12GB 的存储付费</p><p></p><p><strong>一周的数据流转（与费用消耗）</strong></p><p></p><p>若设置 7 天的数据保留期（常见配置）：</p><p></p><ul><li><p>第 1 天：实际数据 24GB，需配置 288GB 存储。</p></li></ul><ul><li><p>第 3 天：实际数据 72GB，需配置 864GB 存储。</p></li></ul><ul><li><p>第 7 天：实际数据 168GB，需配置约 2016GB 存储。</p></li></ul><p></p><p>更关键的是：即便你只需要消费最近 1 小时的数据，仍需为整整 7 天的数据存储与复制付费。</p><p></p><p>以上仅是粗略计算，旨在说明 Apache Kafka 的高成本问题。</p><p></p><p><strong>雪上加霜的跨可用区成本</strong></p><p></p><p>跨可用区复制让成本问题进一步恶化：</p><p></p><p>当数据摄入速率为 1GB / 小时（RF=3）时：</p><p></p><ul><li><p>每小时有 2GB 数据跨可用区传输。</p></li></ul><ul><li><p>每月约产生 1460GB 跨区流量，按每 GB 约 0.02 美元计算（双向传输各按每 GB 约 0.01 美元计费），每月费用约 29 美元。</p></li></ul><p></p><p>当数据摄入速率为 100MB / 秒（RF=3）时：</p><p></p><ul><li><p>副本机制新增 200MB / 秒的跨可用区流量。</p></li></ul><ul><li><p>生产者向其他可用区的 Leader 节点写入数据，又新增约 67MB / 秒的跨区流量。</p></li></ul><ul><li><p>总跨区流量约为 267MB / 秒，每月流量达 700800GB。</p></li></ul><ul><li><p>仅跨可用区副本流量与生产者流量的月度费用就约为 1.4 万美元。</p></li></ul><ul><li><p>若消费者也跨可用区拉取数据，月度费用将攀升至约 1.75 万美元。</p></li></ul><p></p><p><img>https://static001.geekbang.org/wechat/images/f7/f736c35f0c16704e33922aa556ae0921<img></p><p></p><p><strong>核心结论</strong></p><p></p><p>在 2011 年，Shared-Nothing 架构合情合理。当时我们使用物理服务器与本地磁盘，存储区域网络（SAN）的性能无法与本地磁盘相比。</p><p></p><p>但在云时代，你需要为相同的数据支付 12 倍的存储费用，再加上网络费用与管理大量磁盘的运维成本。这就好比在 Netflix 时代仍购买 DVD，不仅如此，还为每张 DVD 购买 3 份副本，存放在 3 个不同的地方，并雇人确保这些副本同步更新。</p><p></p><p>如今情况已然不同。S3 已成为云存储的事实标准，具备低成本、高持久性与全局可用性的特点。正因如此，包括数据库、数据仓库乃至如今的流处理平台在内的各类系统，都在围绕共享存储架构进行重新设计。</p><p></p><p>AutoMQ、Aiven、Redpanda 等项目顺应这一趋势，将存储与计算解耦。它们不再在 Broker 间无休止地复制数据，而是利用 S3 保障数据持久性与可用性，既减少了基础设施重复建设，又降低了跨可用区网络成本。</p><p></p><p>这些项目均致力于减少资源重复、降低跨可用区成本，并采用云原生设计。目前，大多数试图降低 Apache Kafka 成本的新兴项目，实际上都采用了以下两种方案之一：</p><p></p><ul><li><p>部分项目推动 Kafka 向全共享存储模型演进 ——Broker 变为无状态，存储完全依托 S3。</p></li></ul><ul><li><p>另一些项目则采用分层存储方案 —— 将旧数据段迁移至 S3/GCS 等远程存储，减少本地磁盘占用，但仍保留热数据层。</p></li></ul><p></p><p>当然，在 S3 上运行 Kafka 也面临自身挑战，例如延迟、一致性与元数据管理等问题。我们将在后续内容中深入探讨这些挑战，并重点分析 AutoMQ 等开源新项目如何高效解决这些问题。</p><p></p><p>一定有更好的方案，对吧？</p><p></p><p>（剧透：答案是肯定的 —— 这正是我们深入探索的起点……）</p><p></p><h2>Kafka 分层存储（Tiered Storage）方案的提出</h2><p></p><p>Kafka 社区一直在积极讨论并开发分层存储功能（参见 KIP-405）。</p><p></p><p>在阐述我认为该设计可能存在缺陷的原因之前，先让我们用通俗的语言解释一下什么是分层存储。</p><p></p><p>传统上，Kafka Broker 将所有数据存储在本地磁盘中。这种方式速度快，但成本高且扩展性差 —— 一旦磁盘空间耗尽，你要么增加更多 Broker，要么更换更大容量的磁盘，这导致存储扩展与计算扩展深度绑定。</p><p></p><p>分层存储打破了这一模式，将数据分为两层：</p><p></p><p><img>https://static001.geekbang.org/wechat/images/16/164994b496290cd066334a2b06baf81c<img></p><p></p><p><strong>Kafka 分层存储的核心特点</strong></p><p></p><p>热数据 / 本地层</p><p></p><ul><li><p>该层位于 Kafka Broker 的本地磁盘中，存储最新数据，针对高吞吐量写入与低延迟读取进行优化。</p></li></ul><p></p><p>冷数据 / 远程层</p><p></p><ul><li><p>该层采用独立的、通常成本更低且扩展性更强的存储系统。旧数据段会被异步上传至这一远程层，从而释放 Broker 的本地磁盘空间。</p></li></ul><p></p><p>数据流转</p><p></p><ul><li><p>仅当日志段关闭后，才会将其上传至远程层。消费者可从任意一层读取数据；若 Broker 本地无目标数据，则 Kafka 会从远程层拉取数据。</p></li></ul><p></p><p><strong>分层存储宣称的优势</strong></p><p></p><ul><li><p>成本更低：旧数据存储在 S3/GCS 等远程存储中，而非昂贵的 Broker 本地磁盘。</p></li></ul><ul><li><p>弹性更强：存储与计算可实现更高程度的独立扩展。</p></li></ul><ul><li><p>运维更优：本地数据量减少，Broker 重启与恢复速度更快。</p></li></ul><p></p><p>从理论上看，这是一个巧妙的折中方案：将热数据就近存储以保证性能，将冷数据迁移至远程存储以降低成本。</p><p></p><h2>为何分层存储仍未真正解决问题</h2><p></p><p>接下来，我将分享我的观点：我认为分层存储只是对深层问题的 “治标不治本”。</p><p></p><p>还记得我们提到的 1GB 电商数据最终膨胀至约 12GB 的案例吗？分层存储无法解决这一根本性问题。这就好比在房屋地基开裂时，却只对厨房进行翻新。</p><p></p><p>让我们逐一分析其中原因。</p><p></p><p><strong>问题 1：难以摆脱的 “热数据长尾”</strong></p><p></p><p>Kafka 必须将活跃数据段存储在本地磁盘中，这一规则始终不变。只有当数据段 “关闭” 后，才可能被迁移至远程层。</p><p></p><p>一个活跃数据段的大小可能是 1GB，在黑色星期五等流量高峰时段甚至可能达到 50GB。若乘以 3 倍副本因子（RF=3），仅单个分区就需要在昂贵的本地磁盘中存储 150GB 数据。</p><p></p><p>因此，尽管旧数据被迁移至远程存储，但热数据长尾依然存在，且数据量可能非常庞大。</p><p></p><p><strong>问题 2：分区重分配仍令人头疼</strong></p><p></p><p>新增 Broker？重新平衡分区？分层存储仅能起到微小的缓解作用。</p><p></p><p>举例来说：</p><p></p><ul><li><p>无分层存储时：可能需要迁移 500GB 数据，耗时长达 12 小时，过程痛苦。</p></li></ul><ul><li><p>有分层存储时：可能仅需迁移 100GB 热数据，耗时缩短至 2-3 小时。</p></li></ul><p></p><p>不可否认，分层存储确实有所改善。但如果你的网站在结账高峰期出现故障，等待数小时迁移数据仍然无法接受。扩展瓶颈依然存在。</p><p></p><p><strong>问题 3：隐性的复杂性代价</strong></p><p></p><p>我的工程师思维这样总结道：</p><p></p><p>“现在我需要管理两个存储系统，而不是一个。我既要排查本地磁盘问题，又要处理 S3 相关问题。监控指标翻倍，告警数量翻倍。有时数据甚至会卡在两层之间无法流转。”</p><p></p><p>分层存储并未简化运维，反而增加了更多移动部件。这就好比为了整理凌乱的书桌，却买了一张新的书桌 —— 问题并未得到根本解决。</p><p></p><p><strong>我的结论</strong></p><p></p><p>分层存储设计巧妙，也确实能降低存储成本，但它无法解决 Kafka Shared-Nothing 架构中计算与存储深度耦合的根本问题。你仍需为热数据层成本、扩展摩擦与运维复杂性付出代价。</p><p></p><p>真正值得思考的问题并非 “如何降低 Broker 磁盘成本”，而是 “Broker 是否真的需要拥有磁盘”。</p><p></p><p>这正是 AutoMQ 等项目进一步探索的方向 —— 让 Broker 实现无状态，由共享云存储保障数据持久性。</p><p></p><h2>但是……Broker 仍是有状态的，不具备云原生特性</h2><p></p><p>随着我对 Kafka 的使用不断深入，我开始质疑其核心设计假设。</p><p></p><p>回顾我们此前讨论的 Kafka 各类缺陷，它们都指向一个缺失的关键特性：真正的云原生能力。</p><p></p><p>即便引入了分层存储，Kafka Broker 依然是有状态的，存储与计算仍紧密耦合。扩展或恢复 Broker 时，仍需进行数据迁移。</p><p></p><p>为了让 Kafka 真正实现云原生，社区开始探索 Diskless Kafka（参见 KIP-1150），实现计算与存储的完全解耦。</p><p></p><p>这就好比谷歌文档（Google Docs）：不再将文件保存到本地硬盘，而是将所有数据存储在共享云空间中。Broker 不再 “拥有” 数据，仅负责连接共享存储。</p><p></p><p>试想这样的场景：</p><p></p><ul><li><p>无需管理本地磁盘。</p></li></ul><ul><li><p>Broker 崩溃时无需恐慌 —— 不会有任何数据丢失。</p></li></ul><ul><li><p>无需再经历痛苦的分区重分配。</p></li></ul><ul><li><p>新增 Broker？只需接入集群即可。</p></li></ul><ul><li><p>移除 Broker？毫无问题 —— 数据安全地存储在其他位置。</p></li></ul><p></p><p>这不就能解决我们此前讨论的半数难题吗？以上仅为我的个人思考，你或许能提出更优的方案。欢迎在评论区分享你的想法，或通过私信与我交流。</p><p></p><h2>Diskless Kafka 才是破局之道</h2><p></p><p>尽管 Apache Kafka 尚未推出 Diskless 版本，但 AutoMQ 等开源项目已实现了这一功能 —— 而我个人最欣赏的一点是，AutoMQ 与 Kafka API 实现了 100% 兼容。</p><p></p><p>早在 2023 年，AutoMQ 团队就着手打造真正云原生的 Kafka。他们很早就意识到，Amazon S3（及兼容 S3 的对象存储）已成为耐用云存储的事实标准。</p><p></p><p>AutoMQ 与 Kafka 实现 100% 兼容，但对存储层进行了彻底重构：</p><p></p><ul><li><p>所有日志段均存储在云对象存储（如 S3）中。</p></li></ul><ul><li><p>Broker 变得轻量且无状态，仅作为协议路由器。</p></li></ul><ul><li><p>数据的可信来源不再是 Broker 磁盘，而是共享存储。</p></li></ul><p></p><p>既然云服务商已提供近乎无限的容量、跨可用区副本与 “11 个 9” 的持久性，为何还要重新构建复杂的存储系统？AutoMQ 充分利用 S3（或兼容存储）保障数据持久性，Broker 仅负责数据的传入与传出。</p><p></p><p>这一设计带来了显著优势：</p><p></p><ul><li><p>轻松扩展：计算与存储可独立扩展。新增 Broker 以提升吞吐量，存储则在云中自动扩展。</p></li></ul><ul><li><p>快速重平衡：无需进行数据迁移。新增或移除 Broker 时，仅需重新分配 Leader 即可。</p></li></ul><ul><li><p>更高持久性：云对象存储无需在 Broker 上维护 3 倍副本，即可提供数据冗余。</p></li></ul><ul><li><p>运维简化：Broker 可随时替换。若某个 Broker 故障，只需启动新的 Broker，无需进行副本同步。</p></li></ul><p></p><p>换言之，Broker 变得像 “牛群” 一样可替代，而非需要精心呵护的 “宠物”。</p><p></p><p>我最喜欢用这样的比喻来形容：这就好比谷歌文档，不再将文件保存到本地 “C 盘”，而是将所有数据存储在共享云盘中。Broker 仅提供访问能力 —— 数据本身始终安全地存储在云中。</p><p></p><p>AutoMQ 摒弃了每个 Broker 在本地磁盘囤积数据的模式，提出了共享存储理念：所有 Kafka 数据存储在一个公共云仓库中，任何 Broker 均可访问。这并非空想 ——AutoMQ 已通过与 Kafka 完全兼容的分支实现了这一设计，有效解耦了 Kafka 架构中的计算与存储。</p><p></p><p>本质上，他们选择站在 “巨人”（云服务商）的肩膀上，而非重复 “造轮子”。既然 S3 等服务已开箱即用地提供近乎无限的容量、跨可用区副本与极高的耐用性，为何还要从零构建复杂的存储系统？</p><p></p><p><img>https://static001.geekbang.org/wechat/images/24/24ed5e1e2b3098b23af126dcfa29873a<img></p><p></p><p>要理解 AutoMQ 的创新，不妨想象 Kafka 以谷歌文档的模式运行：Broker 不再将数据保存到本地 “C 盘”，而是写入一个所有人共享的云盘。具体而言，AutoMQ 的 Broker 是无状态的，仅作为轻量级 “交通警察”，解析 Kafka 协议并实现数据与存储之间的路由。Kafka 日志段不再存储在 Broker 磁盘中，而是以云对象存储（S3）作为可信来源。这一设计带来了诸多显著优势。</p><p></p><p>首先，数据持久性大幅提升 —— 你可利用 S3 内置的副本机制与可靠性，无需在不同 Broker 上维护 3 份数据副本。其次，成本显著降低 —— 大规模使用对象存储的成本远低于部署大量本地 SSD（尤其是考虑到这些 SSD 还需维护 3 倍副本）。此外，扩展变得几乎 “即插即用”。</p><p></p><p>需要更高吞吐量？只需新增更多 Broker 实例（计算资源），并将其指向同一存储即可；无需通过大规模数据迁移来重新平衡分区。Broker 变得像 “牛群” 一样可替代，而非 “宠物”—— 若某个 Broker 故障，新的 Broker 可立即启动并提供数据服务，因为数据安全地存储在其他位置。这正是 Kafka 此前一直难以实现的云弹性。正如一位 Kafka 云架构师所言：“存储在云中自动扩展，Broker 只需提供数据传入与传出的处理能力。”</p><p></p><p>最后，让我们总结 AutoMQ Diskless 架构带来的优势。</p><p></p><p><strong>Diskless 架构优势</strong></p><p></p><ul><li><p>轻松扩展：计算（Broker）与存储独立扩展。新增 Broker 以提升吞吐量，存储则在云中自动扩展。无需再过度配置磁盘空间，按实际使用付费即可。</p></li></ul><ul><li><p>快速重平衡：无需迁移分区数据。新增或移除 Broker 时，仅需重新分配 Leader，过程几乎即时完成。</p></li></ul><ul><li><p>更高持久性：对象存储提供 “11 个 9” 的耐用性，远优于 Broker 副本机制。</p></li></ul><ul><li><p>运维简化：Broker 故障无关紧要，只需替换即可。无需数据恢复或副本同步。</p></li></ul><p></p><p><strong>延迟挑战</strong></p><p></p><p>理论上，Diskless Kafka 堪称完美，但它存在一个问题：对象存储会引入延迟。</p><p></p><p>低延迟是 Kafka 的核心优势，而直接向 S3 或 GCS 写入数据会导致延迟增加，并产生 API 开销。</p><p></p><p>AutoMQ 在此处做出了明智的设计：引入预写日志（Write-Ahead Log，WAL）抽象。消息首先追加到一个小型、耐用的 WAL（基于 EBS/NVMe 等块存储）中，而长期持久性则由 S3 保障。这一设计在保持 Broker Diskless 特性的同时，有效降低了延迟。</p><p></p><h2>能否进一步优化？</h2><p></p><p>在某些场景中，延迟至关重要，例如金融系统、高频交易、低延迟分析等。对于这些场景，即便是 AutoMQ 的 WAL 方案，也需要进一步创新。</p><p></p><p>AutoMQ 已表示将推出更深入的专有 / 商业解决方案：</p><p></p><ul><li><p>直接写入 WAL：每条消息均写入耐用的云原生 WAL。</p></li></ul><ul><li><p>Broker 随后从缓存或内存中提供读取服务。</p></li></ul><ul><li><p>WAL 卷容量较小（如 10 GB），若某个 Broker 故障，可快速将其挂载到新的 Broker 上。</p></li></ul><p></p><p>这与 Kafka 的分层存储有何不同？</p><p></p><ul><li><p>分层存储：数据首先写入 Broker 磁盘，在 Broker 间复制，之后才将旧数据段迁移至 S3。</p></li></ul><ul><li><p>AutoMQ 的 Diskless 方案：完全无需 Broker 磁盘。数据持久性由云存储层直接保障，无需进行副本迁移。</p></li></ul><p></p><p>若某个 Broker 故障，只需将其 WAL 卷挂载到新的 Broker 上，新 Broker 即可无缝接续旧 Broker 的工作。存储的生命周期超越计算。</p><p></p><p>这是一个重大的思维转变：计算资源可随时替换，存储则保持稳定。</p><p></p><p>在部分场景中，延迟的影响至关重要。因此，上述方案可能并非完美适配，仍需进一步优化。深入研究后我发现，AutoMQ 已针对这类场景提供了相应解决方案，但该方案似乎属于其专有 / 商业产品范畴。</p><p></p><p>这一解决方案可能看似复杂，但彰显了真正的工程智慧，是下一代基于 S3 的 Diskless Kafka 方案。</p><p></p><p>当然，与 SSD / 本地磁盘相比，S3 的速度确实较慢。此外，还需提升向云存储（S3）写入数据的效率，以减少 API 开销。</p><p></p><p><img>https://static001.geekbang.org/wechat/images/fe/fed25258e52d849bdc0a94d5690b2d51<img></p><p></p><p>这与 Kafka 的分层存储是否相同？</p><p></p><p>我的第一反应也是如此：“等等，这难道不与 Kafka 将数据迁移至 S3 的分层存储方案一样吗？”</p><p></p><p>事实并非如此。二者的区别如下：</p><p></p><ul><li><p>在启用分层存储的 Kafka 中，数据仍需先写入 Broker 本地磁盘，Broker 间的副本复制（ISR）仍是必需步骤，之后才会将旧数据段迁移至 S3。</p></li></ul><ul><li><p>在 AutoMQ 中，完全无需本地磁盘。数据直接写入云原生存储中的 WAL，无需副本复制，因为云卷本身已具备耐用性与冗余能力。</p></li></ul><p></p><p>因此，这并非简单的优化，而是一种完全不同的设计。</p><p></p><p>若 Broker 故障怎么办？</p><p></p><p>这是一个很好的问题，也是我们接下来的 “顿悟” 时刻。</p><p></p><p>在 Kafka 中，若某个 Broker 故障，需重新分配分区并同步副本，过程十分痛苦。</p><p></p><p>而 AutoMQ 的处理方式完全不同：</p><p></p><ul><li><p>每个 Broker 本质上是一个挂载了耐用云卷（EBS 或 NVMe）的计算实例。</p></li></ul><ul><li><p>假设 Broker A 正在向其 WAL（EBS）卷写入数据，突然发生故障。</p></li></ul><ul><li><p>无需担心，数据仍安全地存储在 WAL 卷中。</p></li></ul><ul><li><p>集群会迅速将该 WAL 卷挂载到 Broker B 上，Broker B 可无缝接续 Broker A 的工作。</p></li></ul><ul><li><p>整个过程无数据丢失、无副本迁移、无需等待。</p></li></ul><p></p><p>本质上，在 AutoMQ 中，存储的生命周期超越 Broker。计算资源可随时替换，存储则保持稳定。</p><p></p><p>这与 Kafka 的设计理念存在巨大差异。AutoMQ 将计算与存储彻底解耦，这正是其设计的精妙之处。若你想深入了解，可查阅其官方文档。</p><p></p><h2>最后的思考</h2><p></p><p>若你能读到此处，感谢你的耐心阅读！</p><p></p><p>我们一直在探讨的理念简单却极具影响力：若用云存储取代本地磁盘，作为类 Kafka 系统的基础，会带来怎样的改变？</p><p></p><p>这一转变将大幅减少运维难题：</p><p></p><ul><li><p>无需再进行 Broker 重分配。</p></li></ul><ul><li><p>无需再为磁盘告警惊慌失措。</p></li></ul><ul><li><p>扩展变得 “即插即用”。</p></li></ul><p></p><p>令人振奋的是，AutoMQ 等项目正朝着这一方向探索，同时保持与 Kafka API 及工具的兼容性。</p><p></p><p><italic>今日好文推荐</italic></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p></div>",
            "link": "https://www.infoq.cn/article/OGyUaIfDwSHzByidNlM0",
            "pub_date": "2025-10-27 02:53:05",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "LcB8AsrqyOWMEGDl1BbB",
            "title": "谷歌云发布指南，阐述保护远程MCP服务器的关键策略",
            "image": "https://static001.infoq.cn/resource/image/6b/18/6b915854a68166008591cb560a325618.jpg",
            "description": "<div><p>谷歌云发布了一份，详细阐述了保护远程模型上下文协议（MCP）服务器的策略，尤其针对那些依赖外部工具、数据库和API的人工智能系统。该指南指出，尽管MCP显著提升了智能体的能力，但同时也引入了新的安全漏洞，包括工具投毒、提示词注入、动态工具操作、会话劫持、未经授权的访问以及数据泄露等风险。</p><p></p><p>在指南中，谷歌提出了一个以集中式MCP代理为中心的防御架构，这是一个介于客户端和远程MCP服务器之间的安全层。这个代理可在Cloud Run、Apigee或GKE等平台上部署，能够强制执行一致的访问控制、执行审计日志记录、应用机密和资源使用策略，并实时检测威胁，所有这些操作都不会改变各个MCP服务器的实现。</p><p></p><p>为了说明最佳实践，指南列举了组织应优先考虑的五种部署风险：因清单配置失误致使未经授权的工具意外暴露、会话劫持、伪装成合法端点的“幽灵”工具、令牌失窃或敏感数据外泄以及弱身份验证被轻易绕过。借助这种代理架构，这些潜在漏洞得以在大规模部署场景中得到有效缓解。</p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/76/767818a4302bab8e04f51015f018b61f.png<img></p><p></p><p></p><p>谷歌强调，由于MCP服务器通常会暴露给远程或外部访问，保障身份安全、传输安全以及架构稳固性绝非可选操作，而是必要的基础防线。集中式代理模型的引入，为安全防护、可观测性以及治理效能提供了一个集中的强制执行枢纽，使其能够在不引发多个服务器实例漏洞扩散的前提下实现高效且安全的横向拓展。</p><p></p><p>谷歌云建议采用集中式代理来统一管理客户端与远程MCP服务器之间的通信。该代理能够强制执行访问控制、审计日志记录、机密策略以及安全传输，通过设立一个集中的强制执行点，而非依赖众多分散的服务器，有效减少了潜在的攻击面。此外，谷歌着重强调了识别特定风险向量的重要性，例如未经授权的工具暴露、会话劫持以及弱身份验证等问题，并明确指出，将身份认证、传输安全以及策略强制执行视为基础性要求，而非可选操作。</p><p></p><p>相比之下，尽管AWS尚未发布专门针对MCP的指南，但它提供了，用于保障远程服务器编排以及智能体工具的安全性。例如，AWS会话管理器允许用户在无需开启SSH/RDP端口的情况下对EC2实例进行远程访问管理。它使用IAM策略进行访问控制，并通过与CloudTrail以及其他监控工具的深度集成完成日志记录与审计工作。</p><p></p><p>AWS还通过安全组、VPC端点和最低权限IAM角色限制网络访问，为智能体和管理员提供更精细的访问控制。</p><p></p><p>Azure通过提供智能体管理能力。使用Azure Arc时，无论服务器位于Azure、本地还是其他云中，都会在其上部署“连接的机器代理”。通过Azure基于角色的访问控制（RBAC）和基于身份的认证机制（例如Entra ID）来强制执行访问控制策略。Azure还支持默认禁用远程访问，只有在经过明确配置后才会启用，并且可以审计智能体行为和凭据。</p><p></p><p>谷歌云、AWS和Azure这三大云服务供应商在远程智能体/远程服务器方面有重叠的安全理念：强制实施强有力的身份认证与访问控制机制，避免服务器直接暴露于公共互联网，采用集中式代理或智能体框架来增强安全性，确保完备的日志记录与审计追踪，遵循最小权限原则，并严格限制可访问或执行的代理及工具范围。谷歌云针对MCP发布的安全指南与这些通用的安全实践高度契合，还增加了更多关于协议特定威胁（工具投毒、提示词注入等）的警告，这些内容在AWS和Azure的智能体安全文档中往往没有如此明确的说明。</p><p></p><p>【声明：本文由InfoQ翻译，未经许可禁止转载。】</p><p></p><p><strong>查看英文原文</strong>：</p></div>",
            "link": "https://www.infoq.cn/article/LcB8AsrqyOWMEGDl1BbB",
            "pub_date": "2025-10-24 04:00:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "oJdiIjMNPMRsJdqyZb0K",
            "title": "IBM Cloud Code Engine Serverless Fleets配备GPU以实现高性能人工智能和并行计算",
            "image": "https://static001.infoq.cn/resource/image/8e/54/8ef882yy4507edfbf3578036ee314e54.jpg",
            "description": "<div><p><color>IBM战略性全托管无服务器平台</color><color>引入了支持集成GPU的Serverless Fleets。凭借这项新能力，该公司直接解决了在简化版按需付费无服务器模型上运行大规模计算密集型工作负载的挑战，如企业级人工智能、生成式人工智能、机器学习和复杂仿真。</color></p><p><color> </color></p><p><color>正如学术论文中所指出的那样（包括康奈尔大学最近发表的一篇</color><color>），历史上，无服务器技术在高效支持这些要求苛刻的并行工作负载方面存在困难，这类负载通常需要使用专用的硬件同时执行数千或数百万个任务。通过Serverless Fleets，IBM旨在通过提供高性能计算资源来弥合这一差距，而且又不需要用户应对管理专用基础设施的操作复杂性。</color></p><p><color> </color></p><p><color>Michael Behrendt是无服务器平台首席技术官兼IBM杰出工程师，他在LinkedIn上的一篇</color><color>中评论道：</color></p><p><color> </color></p><p><color> </color></p><p><color>Serverless Fleets提供了单个端点用于提交大量的批处理作业，简化了数据科学家和开发人员执行计算密集型任务的方式。在这篇</color><color>中，IBM提到，Code Engine随后会自动处理基础设施编排：</color></p><p><color> </color></p><ul><li><p><span style=\"color:#494949\">该服务会自动配置所需的计算资源，包括虚拟机（VM）和无服务器图形处理单元（GPU），如NVIDIA L40，以便可以同时运行多个任务。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">此外，Serverless Fleets旨在运行可弹性扩展的运行至完成任务。该系统会确定所需的工作实例的最优数量并将其部署，以便高效处理并行执行。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">最后，当工作负载完成后，资源将自动移除，确保用户仅需为执行过程中实际消耗的技术资源付费。</span></p></li></ul><p><color> </color></p><p><color>随着IBM Cloud Code Engine Serverless Fleets的推出，公司带来了一项极具竞争力的服务。在其他超大型提供商中，亚马逊云科技提供了</color><color>这样的解决方案，用于在无服务器计算上运行容器（通常搭配EKS或ECS进行编排），而Azure在</color><color>中提供了</color><color>。然而，IBM致力于提供一个统一的环境，通过单个简单的平台为Web应用、函数以及现有的大量GPU加速的批处理作业提供服务。</color></p><p><color> </color></p><p><color>竞争对手可能需要开发人员将多个服务（如无服务器运行时、容器服务和批处理编排器）拼接在一起，Serverless Fleets则旨在简化这一过程，通过单个端点就可以完全管理基于GPU的虚拟机的配置和弹性扩展，减少了通常与在云中运行GPU密集型弹性工作负载相关的复杂性和运营开销。在Medium上的一篇</color><color>中，Luke Roy总结道：</color></p><p><color> </color></p><p><color> </color></p><p><color>在一篇</color><color>中，该公司表示，在当今竞争激烈的格局中，各行各业的企业都需要能够快速、便捷地交付服务，同时优先考虑安全性、弹性和成本节省。</color></p><p><color> </color></p><p><color> </color></p><p><color>声明：本文为InfoQ翻译，未经许可禁止转载。</color></p><p><color> </color></p><p><color>原文链接：</color></p></div>",
            "link": "https://www.infoq.cn/article/oJdiIjMNPMRsJdqyZb0K",
            "pub_date": "2025-10-23 03:31:24",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "8Ttbl2JhW4hslXgZ0Zdm",
            "title": "亚马逊云科技公司推出EC2实例认证",
            "image": "https://static001.infoq.cn/resource/image/0d/91/0dc9054c5cyy042f1235cf422baab991.jpg",
            "description": "<div><p><color>亚马逊云科技公司</color><color>，这是一种新的安全功能，使客户能够以加密安全的方式验证他们的虚拟机是否运行着经过批准的软件配置。这项功能由Nitro可信平台模块（</color><color>）和可认证的AMI提供支持。</color></p><p><color> </color></p><p><color>通过</color><color>，客户可以加密验证EC2实例是否运行着可信的配置和软件，解决了具有严格安全性和合规性要求的组织的关键问题。以前，可以从EC2的管理员和用户中移除操作员访问权限，但没有办法验证是否已经这样做了。AWS的首席安全架构师</color><color>解释道：</color></p><p><color> </color></p><p><color> </color></p><p><color>与以前仅在</color><color>中可用的功能类似，新功能将这些保护扩展到了标准EC2实例。Bean补充说：</color></p><p><color> </color></p><p><color> </color></p><p><color>一个可认证的</color><color>是一个具有相应的表示其所有内容的加密哈希的AMI。根据文档，这个哈希是在AMI创建过程中生成的，并且是基于AMI的完整内容计算的，包括应用程序、代码和启动过程。</color></p><p><color> </color></p><p><color>有了这个新功能，密钥和其他机密只能由运行经过批准的AMI的EC2实例使用AWS密钥管理服务（KMS）解密。此外，组织可以构建一个证书颁发机构（CA），仅向经过验证运行经过批准的AMI的实例颁发证书。在一个流行的</color><color>帖子上，一些用户质疑这个功能是否会被广泛采用。用户jiggawatts评论说：</color></p><p><color> </color></p><p><color> </color></p><p><color>The Duckbill Group的首席云经济学家Corey Quinn</color><color>：</color></p><p><color> </color></p><p><color>:</color></p><p><color>AWS英雄和无服务器专家Yan Cui有</color><color>：</color></p><p><color> </color></p><p><color>新的EC2实例认证对于企业SaaS和合规等用例来说是一个游戏规则改变者（…）许多企业更愿意在内部运行SaaS软件，以确保他们的敏感数据不会离开他们的网络。但SaaS提供商就没有办法保护他们的软件知识产权。有了可认证的AMI，SaaS提供商可以发布一个包含他们软件（比如说，一个AI模型）的AMI。客户可以通过镜像启动EC2实例并运行软件，但不能访问其内容。</color></p><p><color> </color></p><p><color>亚马逊云科技公司并不是唯一一个使客户能够验证虚拟机是否运行可信配置和软件的云提供商：</color><color>和</color><color>都提供了类似的认证能力。</color></p><p><color> </color></p><p><color>EC2实例认证在所有AWS区域均可用，该功能本身不收取额外费用。标准存储费用适用于AMI，如果使用该服务，AWS KMS定价适用于密钥操作。</color></p><p><color> </color></p><p><color>有一个</color><color>可用于使用</color><color>构建Amazon Linux 2023 Attested AMI，这是一个用于创建预配置的基于Linux的镜像的开源工具。</color></p><p><color> </color></p><p><color>原文链接：</color></p></div>",
            "link": "https://www.infoq.cn/article/8Ttbl2JhW4hslXgZ0Zdm",
            "pub_date": "2025-10-22 05:00:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "yK0XQ02CR1ZUKgM5HUFL",
            "title": "从芯粒到服务器：一文读懂大模型浪潮下的开放互连",
            "image": "https://static001.infoq.cn/resource/image/59/c1/59c1e8a68dbe5c9a95833216c96dyyc1.jpg",
            "description": "<div><p> </p><h2>前言</h2><p>过去的五到六年见证了AI大潮的汹涌澎湃，而在这巨浪之下，也涌动着开放互连标准的蓬勃发展：</p><ul><li><p>2019年3月，Compute Express Link（CXL）联盟成立，旨在解决当时异构XPU编程以及内存带宽和容量扩展所带来的挑战，以内存为切入点构建一个可编排的机柜级服务器架构。阿里作为以创始成员的身份成为CXL联盟董事会董事。 彼时，Transformer架构已发布近两年，其潜力已在BERT上牛刀小试。</p></li></ul><ul><li><p>2022年3月，Universal Chiplet Interconnect Express（UCIe）联盟成立。此时，基于芯粒（Chiplet）的芯片设计范式已成为后摩尔时代设计高算力芯片的业界共识，但整个行业还缺乏一个有号召力的，开放的Die-to-Die互连标准。UCIe旨在填补这一空白。同年8月，阿里正式成为UCIe联盟董事会，成为中国大陆唯一个UCIe董事会成员。 那时，距离ChatGPT的发布还有3个月。 </p></li></ul><ul><li><p>2023年7月，Ultra Ethernet Consortium（UEC）联盟成立。此时，整个行业正处在被ChatGPT点燃的百模大战中，大模型训练的需求爆炸式增长，而基于传统以太网的集群面对这波泼天富贵已显得力不从心。UEC的成立正当其时，它旨在为AI和HPC重建一个高效开放的以太网。 同年11月，阿里云正式加入UEC联盟，成为General会员（UEC两级会员体系中的高阶会员）。</p></li></ul><ul><li><p>2024年10月，Ultra Accelerator Link （UAL）联盟成立，旨在应对模型尺寸以及推理上下文快速增长带来的对Scale-up网络的需求。2025年1月，阿里以董事会董事的身份加入UAL，成为中国大陆唯一个UAL董事会成员。</p></li></ul><p></p><p>笔者有幸代表阿里云以UCIe和CXL董事会董事的身份，参与了UCIe 1.0～3.0标准, 以及CXL 3.x～4.0（即将发布）标准的制定和发布；同样来自服务器团队的廷钰代表阿里云以UAL董事会董事的身份，参与了UAL 1.0 标准的制定。正是因为这些在开放互连中的长远布局使阿里在AI大潮中占据了极为有利的硬件生态位，引领国内开放互连生态的发展。 本文旨在通过介绍UCIe/CXL/UAL/UEC的基本概念和特性，系统的梳理各个层次的开放互连在AI Infrastructure中的作用，帮助大家深入理解这些不同字母组合背后的含义和关系。</p><p> </p><h2>背景</h2><p>大模型的迭代在Scaling Law的信仰下，高潮迭起，你方唱罢我登场， 令人目不暇接。现在回望，所谓的Scaling Law大致有三个阶段：</p><ul><li><p>Pre-training Scaling：在模型预训练阶段，通过增加模型参数数量，训练用的数据量，以及用于训练的计算资源（即传统的算法，数据和算力三要素），来提升基础大模型的输出精度。 </p></li></ul><ul><li><p>Post-training Scaling: 在模型后训练阶段，通过基于专业数据集（Domain-Specific Dataset）的微调，Reinforcement Learning with Human Feedback（RLHF）以及 Teacher-Student调教和蒸馏（Distillation）等手段， 使大模型的输出更具有专业素养，符合人类沟通习惯和价值观。 </p></li></ul><ul><li><p>Test-time Scaling: 在模型Inference阶段，通过生成更多用于协助推理的token，从而形成更高质量的模型输出。常见的方式比如：Chain-of-Thought（CoT）Prompting： 通过将问题分解为多个步骤，展示推理过程；Sampling and Voting： 通过针对一个Prompt生成多种可能的响应结果，借助Majority Voting选择最佳结果。 </p></li></ul><p></p><p><img>https://static001.infoq.cn/resource/image/5e/0e/5e3591384cbfcf827689d14183952b0e.png<img></p><p>图1. 大模型Scaling Laws. Source: [3]</p><p> </p><p>如今，这三阶段Scaling Law依然在大模型的演进中起着各自的作用， 但是，Scaling的重心已经转移到了Test-time Scaling。这是从大模型竞赛到大模型落地应用自然过渡的结果；但另一方面，推理是对客服务的，它对延时指标（e.g. Time To First Token (TTFT)，Time Per Output Token (TPOT)) 和吞吐指标（Token Per Second（TPS)) 都有刚性需求，这对AI Infrastructure提出了与Pre-training Scaling截然不同的新挑战。 </p><p> </p><p>为此，当黄教主端出了热气腾腾的GB-200 NVL72全家桶时，广大CSP们无不为之垂涎：NVIDIA-HBI（High Bandwidth Interface）连接了两颗Blackwell Die，构成了一个完整Blackwell GPU；同时，Grace CPU通过NVLink C2C（利用其Chip-2-Chip互连功能）连接了两个Backwell GPU， 并以2CPU：4GPU的方式构建一个计算节点（A.K.A.，Compute Tray）；接着，NVLink连接起72个GPU（18个计算节点），构成一个Scale-up网络；Backend 网络( 或Scale-out网络 )？ CX-7 （IBTA RDMA or RoCE）顶上。这全家桶，从计算到互连，都是业界顶流，全面回应了CSP所面临Test-time Scaling的痛点，着实让人心动。但这里问题是其核心互连都是NVidia独家私有。 对于CSP来说，这意味着，在商业上，弱势的议价能力和Vendor Lock-in；在技术上，丧失了独立的优化空间；在产品上，不具备差异化特性和竞争力。 </p><p></p><p>因此，我们需要拥抱开放互连标准。 开放意味更多参与者，带来更多的选择性，更为健壮的供应链，也意味着更大的定制和优化空间。 而作为开放互连的代表，UCIe/CXL/UAL/UEC涵盖了从芯粒互连到Scale-out网络的全链条，是我们构建AI Infra核心竞争力的重要阵地和抓手。 </p><h2> </h2><h2>UCIe：芯粒？成本！</h2><h2>何以芯粒？</h2><p>基于芯粒（Chiplet）的设计方式已经成为当前高性能CPU/GPU的主流设计范式， 这是在后摩尔时代，应对单位晶体管成本下降缓慢的必然选择。为什么这么说呢？ 这是因为基于芯粒的设计在多个层面解决了成本的问题，如下图所示：</p><ul><li><p>提升良率：通过将单一大尺寸硅片切分成小的硅片，再合封在一起的方式，缩小了晶圆上单个缺陷所影响的范围， 从而提高了硅片产出的良率，降低了单位硅片的成本。 </p></li></ul><ul><li><p>制程节点优化：通过将芯片的电路进行合理的切分，提升了电路功能和对应制程的契合程度。 比如，将DDR控制器，PCI控制器等IO模块集中在IO Die中，而由于IO Die中模拟电路占比较高，对高端制程无刚性要求，因此可以采用较为成熟的制程，从而降低IO Die的制造成本。 </p></li></ul><ul><li><p>跨产品的芯粒复用：传统的单片SoC设计中， 如果要做多个不同的产品SKU，要么在一个硅片上做个多个产品SKU的超集，然后在硬件上关闭某些核心或者加速器，形成不同的SKU；要么针对每个SKU都设计一个硅片单独流片，这两者的整体成本都比较高。而基于芯粒的设计通过对Core芯粒和加速器芯粒的复用和组合，构成不同的产品SKU。这种复用和组合灵活性带来的成本分摊，充分体现了芯粒设计的价值。</p></li></ul><ul><li><p> 芯粒市场化：如果说跨产品的芯粒复用是在公司内部的芯粒复用，那么芯粒供应货架化则是跨公司的芯粒复用。它是当前板级设计中芯片货架化的商业模式在芯片内部的自然延伸，能使芯粒的设计和制造成本得到最大程度的分摊（Economy of Scale）。同时，通过市场竞争提升进一步提升可选择性和供应链的健壮性。 </p></li></ul><p></p><p><img>https://static001.infoq.cn/resource/image/07/0b/07f55740f2ec8e0c9e53d720a06ee40b.png<img></p><p>图2. 基于芯粒设计的价值支柱</p><p> </p><p>除了成本方面的优势，基于芯粒的设计模式还解决了芯片的极致性能问题，这在GPU侧体现的尤为突出。硅片的大小受到光罩尺寸（Reticle Size）的限制，对于EUV制程来说，它通常在850mm^2左右。 当前，GPU对更高单芯片算力的追求已使硅片面积经触及了光罩尺寸，而芯粒提供了一种突破光罩尺寸对算力限制的途径：将多个大尺寸硅片合封在一起，提供远超单一硅片能实现的单芯片算力。 这实际就是NVIDIA的Blackwell GPU（2 Die合封）和Rubin Ultra GPU（4 Die 合封）（参见下图 3.）背后的逻辑。</p><p></p><p><img>https://static001.infoq.cn/resource/image/50/3a/50d82381a2e61e003e9656732a07c03a.png<img></p><p>图3. Rubin Ultra 4个Reticle Size GPU 合封</p><p> </p><p>目前，前三个芯粒价值已经在芯片大厂（AMD，Intel，NVidia等）中得到充分渗透，但基本是通过私有Die2Die互连实现的（比如，AMD的Infinity Fabric，NVidia的NVLink C2C）。而芯粒的市场化依然比较遥远，它的实现有赖于一个开放和可互操作的Die2Die互连协议。 不仅如此，开放的Die2Die协议还能让广大的芯片创业公司都快速搭上芯粒设计的大船， 与芯片大厂同台竞技，而不必投入大量资源自研的相关协议。从这层意义上来说，开放协议涉及到平权和产业的繁荣，这也是我们为什么要坚定的支持开放的另一个原因。 但是光有开放还不够，一个协议要有生命力，必须要有足够好的KPI和产业链上主流厂商的背书；能够体察业界痛点，持续演进；被业界广泛采纳。 UCIe具备了这些维度上的特点：它的KPI在已有的芯粒互连协议中最具有竞争力；在产业链上得到了从Fab（TSMC，三星）到封装（日月光）到设计（Intel，AMD，ARM，NVIDIA，Qualcomm）到应用（Alibaba，Google，Meta，Microsfot）主流公司的支持；3年时间已经迭代到3.0版本，国内外主要EDA厂商已经开始销售UCIe IP。</p><p></p><h2>UCIe协议概览</h2><p>任何一个通信协议的设计都与它所要应对的信道状态和应用场景息息相关，UCIe也不列外。作为芯粒间的互连标准，UCIe要面对的是：</p><ul><li><p>极短的信道长度：对于2D/2.5D封装，信道长度在毫米级，而对于3D封装，若通过混合键合（Hybrid Bonding）来实现，其信道长度在亚微米级。 这意味信道衰减较低，无需对传输数据做前向纠错（FEC）。</p></li></ul><ul><li><p>多元的信道介质：不同的封装技术会让信道处在不同的介质中，标准封装工艺通常会让信道穿越有机基板或玻璃基板，而先进封装通常会采用Silicon Interposer（e.g., TSMC的Chip-on-Wafer-on-Substrate (CoWoS)) 或者硅桥（e.g. Intel的Embedded Multi-die Interconnect Bridge (EMIB) 等。不同的介质支持不同的信道密度，需要区别对待。</p></li></ul><ul><li><p>有限的IO Pad空间：硅片上Micro-bump资源相较于芯片的引脚资源来说更为丰富，但依然非常有限，毕竟当前的硅片设计大部分情况下都属于IO Pad Limited，即IO Pad的面积决定了硅片的面积。这意味着Die2Die协议需要有很高的带宽密度（GB/s/mm or GB/s/mm^2）来减少对硅片IO资源的占用。这进而意味着更小的Micro-bump Pitch，更高效的Micro-bump排布。 </p></li></ul><ul><li><p>极致的延时要求：芯粒合封通常有两个来源：一种是原本独立的板级加速设备被合封进芯片内部，此时跨Die传输延时相较于板级传输延时有天然的数量级优势，因此这种情况下，对跨Die延时的要求不高；另一种，也是更为重要的一种是通过切分将SoC分到多个Die上进行合封，此时跨Die延时越小越好。 UCIe将目标设为<2ns (TX+RX)，这是一个极具竞争力又极具挑战性的目标。为达成这一目标，UCIe采用了并行接口设计，以避免数据串行化带来的额外延时。这是综合考虑了信道长度和介质对信号完整性的影响，以及延时需求和IO Pad成本之间的权衡。 当前做得好的IP可以把延时控制在10ns以内。 </p></li></ul><ul><li><p>高标准的链接可靠性：Die与Die的物理链接要么经过Micro-bump（2D/2.5D），要么经过键合，两者对工艺的要求极高。如果在micro-bump焊点处或者键合处有杂质或缺陷，使用过程中容易出现电迁移造成链接失效。如果在服务器主板上出现这样的问题， 可以替换部件重新焊接；但在芯片内部，这种问题会导致整个芯片失效。 因此，跨Die链接需要引入冗余机制， 构建对此类问题的修复能力，提升可靠性。</p></li></ul><ul><li><p>支持多种协议的灵活性：如上所述， 芯粒合封通常有两种场景：一是将加速芯粒以CXL或PCIe设备的形式合封，此时跨Die通信的协议是CXL或PCIe；另一种是各类多核计算芯粒的合封，此时跨Die通信涉及到连接片上网络（NoC）的各种一致性协议（e.g. CHI）甚至是自定义协议。 因此，UCIe要做的是提供足够的灵活性，支持好各类已有的上层协议，而不是另起炉灶再定义一套上层协议。 </p></li></ul><p></p><p><img>https://static001.infoq.cn/resource/image/33/8c/33d741841db5b4a70cd8630b74900c8c.png<img></p><p></p><p><img>https://static001.infoq.cn/resource/image/a1/2f/a1eb7c47fbb285c3bee9986d0129c72f.png<img></p><p>图4. UCIe协议框架</p><p> </p><p>综合上述需求， UCIe构建了三层协议栈：物理层，Die-to-Die适配层和协议层。协议层和D2D适配层通过Flit-Aware Die-to-Die Interface（FDI）衔接，而Die-to-Die适配层与物理层之间通过Raw Die-to-Die Interface（RDI）衔接。 那么，为什么在UCIe协议中需要有这样的接口，而通常其他的协议栈中并没有呢？这和UCIe协议所要提供的上层协议灵活性有关。在UCIe中，协议层实质是个筐，各种协议都可以往里装：PCIe，CXL和以Streaming形式出现的其他协议。FDI负责将这些协议物理层以上的数据通过Arbitration/Mux的方式导入到Die-to-Die适配层。另外，Streaming在Raw Mode下可以绕过D2D适配层直接进入到物理层。 因此，我们需要RDI将来自于D2D适配层的数据和Raw streaming数据进行Mux。这也就是我们需要有FDI/RDI的原因：多个协议数据流之间的Arb/Mux。</p><p></p><p>D2D适配层主要通过CRC和Retry保证数据在跨Die传输的正确性。 物理层是UCIe协议中的大头，它负责链接初始化，Lane映射和重映射（出现通道失效，冗余备份通道被激活时需要重映射）等。 一个物理层对应一个模组信号，包括x16或x64的TX和RX数据信号，一个前向时钟，一个Valid和x2的侧信道。 一个D2D适配层可以支持1，2，4个这样的模组信号。 为适配不同的信道介质，UCIe区分了标准封装（UCIe-S）和先进封装（UCIe-A）两个Profile，其中UCIe-S只支持x16数据信号，而UCIe-A支持x64数据信号，但两者都支持32GT/s的传输速率。先进封装单模组的带宽可达256GB/s，更高的速率也在定义中。 </p><p> </p><p><img>https://static001.geekbang.org/infoq/d4/d44f68b3f4b01914f0ade0dca109734c.png<img></p><p>图5. UCIe-3D 层次结构</p><p> </p><p>UCIe-3D是UCIe 2.0中引入的新的Die2Die互连Profile，建立在混合键合3D封装技术之上。 由于其信道长度几乎可以忽略，因此，UCIe-3D的物理层可以也必须极致简化：一个信号驱动即可。 同时，由于3D封装将原先2D/2.5D封装中的Shoreline拓展到了整个Die平面， 如何组织这些信号端口就变的尤为突出。 如上图所示，为了便于管理， UCIe-3D把这些信号以0.02mm^2为单位组织成一个bundle，每个bundle中的有x80 TX/RX信号，并以9um bump pitch进行排布。相比之下， 2D/2.5D的先进封装中的bump-pitch在25～55um之间。 由于混合键合易受到杂质的干扰，因此，UCIe-3D在每一行的bundle中定义了两个冗余的bundle，用于应对杂质/缺陷的影响。为什么要有两个冗余的bundle？因为如果一个缺陷正好处在四个bundle的交接处，它就会让四个bundle变得不可用，正好每行两个。 这里Bundle的remapping并没有在UCIe-3D标准中定义，它需要上层的逻辑电路将该因素考虑进去，并提供相应的支持。 </p><h2>UCIe协议的应用</h2><p>UCIe芯粒合封的一个典型应用场景如下图（a）所示：两个计算Die通过UCIe相连，通过CHI协议实现计算Die之间的Cache一致性；同时一个计算Die再通过UCIe与加速Die互连，通过CXL协议，将加速器内存和计算Die内存构成统一空间，实现互访。 </p><p></p><p>UCIe也推动了光电共封（Co-Packaged Optics，CPO）的发展。当前的产业链下，Optics Engine（OE）和计算芯粒通常分属不同公司。要实现共封，大概有几种方式：1. 计算芯片大厂收购光模块公司后，通过私有Die2Die协议实现光电共封；2. 通过私有Die2Die协议IP对光模块公司的定向授权，实现CPO；3. 通过计算Die和OE共同遵循开放Die2Die协议UCIe，来实现光电共封。我们认为第三种方式成本最低，灵活度最高，可以灵活实现标准封装或先进封装下的CPO；但在实操中，实现来自两家公司UCIe的互操作，还存在前期系统验证和EDA工具不成熟等生态方面的挑战。而随着CPO在AI集群互连中变得日益重要， 它反过来对UCIe生态也是一种良性的促进。 </p><p></p><p><img>https://static001.infoq.cn/resource/image/33/58/334c39b388158a68eec44c83e7967e58.png<img></p><p></p><p>(a) Core Die以及加速器Die的片上互连</p><p></p><p><img>https://static001.infoq.cn/resource/image/57/50/5743c15b41198ae5d40c6f1fbbe39d50.png<img></p><p></p><p>(b). UCIe-based CPO</p><p>图6. UCIe互连应用场景</p><h2>CXL：重塑服务器组织形态</h2><p>CXL联盟成立于大模型爆发之前，那时深度学习和各类卸载所带来的XPU盛行，整个行业所面临的问题是： 如何在服务器层面对这些异构设备进行灵活的编排和高效的编程；同时，在CPU核心数快速增长的背景下，如何应对内存带宽需求增长和芯片有限引脚资源的矛盾。 因此CXL以内存为切入点，构建一个基于内存语义的一致性互连， 并通过解耦计算和内存，实现可编排的机柜级服务器硬件架构。但它仍然是CPU-centric的，又受到到PCIe带宽的限制，因此在大模型爆发的时代，无法直击核心痛点，未能成为AI基础设施海量投资的焦点。即便如此，这种对服务器组织形态的重构依然意义深远，而且随着PCIe速率的提升，CXL对AI基础设施的效能提升还在不断增强。 </p><h2>CXL主要特性：</h2><p>关于CXL协议的核心概念，可参考“”文章，这里就不再赘述。 上表总结了CXL1.0到CXL3.2标准所支持的特性清单。在这里，我们简单梳理下在笔者看来对AI有较大助力的CXL特性。</p><p> </p><p>表1. CXL1.0～3.2支持的主要特性</p><p><img>https://static001.infoq.cn/resource/image/d1/f1/d1b9e42d681eb8b1ee3c747d72b17df1.png<img></p><p></p><ul><li><p>内存扩展（Memory Expansion）和内存池化（Memory Pooling）: 内存扩展应该是目前为止CXL最为成熟的应用， 它能够通过CXL串行端口和 CXL Memory Module（Type 3 CXL Device，说人话就是CXL协议打包的DDR内存） 拓展Host内存容量。 同时，随着XConn的CXL 2.0交换机逐步推向市场， 通过交换机的内存池化又为内存容量的扩展提供了更加广阔的扩展空间。CXL池化有两种形态：一种是以设备为粒度的池化，每个设备为 Single-Logic-Device（SLD），它只能以一个整体分配给某个Host；另一种是将设备的物理资源分成多个逻辑设备(Up to 16) 并分配给不同Host，这种设备被称为Multi-Logic-Device (MLD)。MLD常见于CXL存储设备，比如像AliSCM的CXL Storage Class Memory。每个MLD中的逻辑设备（LD）都有一个ID来区分（LD-ID），但是这个LD-ID并不是Host可见的。CXL Switch根据Transaction中的Host Port来找到其对应的LD-ID，并通过Fabric Manager来构建和维护这样的映射表。内存扩展和池化为LLM推理服务系统提供了一个分层的缓存系统。 </p></li></ul><ul><li><p>统一的一致性内存空间: CXL通过将Host内存和设备内存统一编址，构建了统一的内存空间，并借助CXL.cache实现一致性。由于可以通过Host CPU扩展大容量的DDR内存，这种特性在理论上能够很好的弥补当前GPU显存不足且昂贵的缺陷。 这个其实和GB200通过NVLink C2C实现Grace和Blackwell统一内存是一样的。但是在CXL上遇到了一些比较现实的挑战：首先，由于CXL基于PCIe PHY，而当前相对成熟的PCIe Gen5/6的速率远低于NVLink，这导致Host与Device之间数据搬移的延时较大，严重影响了统一内存的实际效果 。其次，设备对Host内存的访问需要通过CXL.cache协议，但该协议的实现复杂性较高，到目前为止，尚未见到成熟的支持CXL.cache的设备；而PCIe Gen5/6较低的速率进一步削弱了在GPU上集成CXL的动力。这种情况估计要等到PCIe Gen7（128GT/s的传输速率）成为主流后才能有所改变。</p></li></ul><ul><li><p>通过GIM（Global Integrated Memory）的Host-to-Host内存共享 : GIM是CXL 3.1提出的概念，用来实现Host之间的一致性内存访问。具体来说，GIM指的是映射到本地主机物理地址空间中的远程主机的内存， 它可以让在同一CXL Fabric中的Hosts实现内存互访。这在一定程度上弥补了CXL1.1/2.0种非对称内存访问的短板（即只能实现Host与Device之间互访内存），结合设备之间的P2P内存访问，CXL 3.0+在形式上实现了对称的内存共享和互访。尽管在实现机理上有所不同：Host与Device之间通过CXL.mem以及CXL.cache, 而Host与Host，Device与Device之间通过UIO（Unordered IO），但这种全域的内存共享能力，为当前内存受限的AI基础设施提供了很大的想象空间。</p></li></ul><p></p><p><img>https://static001.infoq.cn/resource/image/23/ca/23e9961999b705419f55edd3a33752ca.png<img></p><p>图7. CXL Global Integrated Memory</p><h2>CXL在AI场景中的应用：</h2><ul><li><p>分层KV Cache缓存： 在用于大模型推理服务的集群中，由于需要同时服务多个用户，KVcache的内存占用量非常庞大。如果单纯依赖GPU上的HBM或者GDDR来保存这些数据，整个AI推理系统会变得极其昂贵且效率低下，经济上不可持续。 利用CXL的内存池化，我们可以构建一个分层的KV Cache缓存体系： Host经由CXLSwitch到CXL内存（e.g., Alimemory），CXL Storage Class Memory （e.g., AliSCM），乃至CXL SSD（e.g., AliFlash）。在这基础上，利用KV Cache的访问热度，将KV值分配到不同的缓存层级中，可极大的缓解KV Cache在显存中的存储压力， 降低成本的同时又可提高GPU的运行效率。 </p></li></ul><ul><li><p>Retrieval-Augumented Generation （RAG）向量数据库：为解决大模型推理受训练数据的时效限制的影响，最新主流的ChatBot服务以及主流的LLM推理框架都支持RAG：依据输入的Embedding，查询向量数据库，从而获取与输入相关度较高的最新上下文信息。由于RAG处在整个推理的关键路径上，因此提高向量数据库的查询性能将直接影响到推理系统的性能。这里，至少有两个途径CXL可以发挥作用：一种途径是，通过内存扩展或者池化，创建一个巨大的Host侧内存空间，这样整个向量数据库都可以常驻内存，变成In-memory database，使得延时和吞吐上都优于基于SSD存储的向量数据库；另一个种途径是，利用存内计算（In-Memory Computing）或近存计算（Near-Storage Computing）将RAG所需要的Approximate Nearest Neighbor Search (ANNS)卸载到CXL设备上，这样可以减少主机CPU和设备之间数据传输，进一步提升向量数据库查询效率。 </p></li></ul><h2>UAL：内存语义下的精简Scale-Up互连</h2><p>严格意义上来说，Scale-up是一个节点内扩展的概念：我们通常所说的单机4卡，单机8卡即属于该范畴。但是随着模型尺寸的不断增加，上下文的增长，即便是单机16卡也无法放下一个模型。那就只能将一个模型放在多个机子的多张卡上，这在HPC语境下是再自然不过事：将一个负载任务拆分成多个Partition分布到多个节点上，再通过Message Passing等方式进行节点间的同步和通信。 问题在于大模型和典型的HPC负载有着明显的不同： 典型的HPC负载通常存在通信本地性（Locality），比如，计算流体动力学（CFD）中，基本的通信模式是3D Nearest Neighbor（A.K.A., 3D Stencil），即一个分区仅和它直接相邻的分区进行通信，这就从根本上避免了昂贵的All-to-All通信；而大模型负载又“大”又“紧”，“大”是指计算量大，对内存容量要求大，故需要多张GPU卡，需要做切分；“紧”是指负载内部通信的紧耦合，即便是对切分比较亲和的专家并行（Expert Parallelism）也存在All-to-All等密集数据通信（单次通信GB级别，且无法通过overlapping被隐藏）。因此，我们就有了这样的需求： 既要分布在多个物理节点上的多张GPU卡，又要求卡间通信与单个物理节点内的通信在性能上无明显区别，这就是所谓的超节点Scale-up网络。 </p><h2>UAL协议概览</h2><p>要实现对Scale-Up网络的支持, UAL需要： </p><ul><li><p>支持内存语义，避免RDMA中Producer-consumer编程模式所带来的Doorbell/Interrupt延时开销。</p></li></ul><ul><li><p>精简协议栈， 减少在协议处理链路上的延时。 </p></li></ul><ul><li><p>采用高速率串口， 支持通道聚合，以满足scale-up带宽要求。</p></li></ul><ul><li><p>Time-to-Market，充分复用现有技术，降低研发复杂度，能够快速实现应用和部署，充分挖掘大模型推理爆发式增长带来的红利。 </p></li></ul><p><img>https://static001.geekbang.org/infoq/b9/b98e33973555d2dbc6598387d5d7a16e.png<img></p><p>图8. UAL协议框架</p><p> </p><p>基于上述需求，UALink定义了四层协议栈，包含了协议层（Protocol Layer），事务层（Transaction Layer），数据链路层（Data Link Layer）和物理层（Physical Layer）。</p><p></p><ul><li><p>物理层： UALink复用了IEEE802.3dj以太网的物理层，采用224G SerDes来实现200Gb/s的传输速率。在减少了协议IP设计成本的同时，又实现了远高于PCIe Gen5/6的传输速率。 这里考虑了40Byte 前向纠错码（FEC）和256B/257B的信道编码所带来的带宽损耗。这一通道速率已经与NVLink 5.0齐平，相较于CXL带宽问题有了极大的缓解。此外，4条这样的通道一起构成了一个UALink Station，它可以是一个x4的UALink端口，也可以bifurcate成2个x2，或者4个x1的端口。UALink通过这样4通道汇集以及对Bifucation的支持兼顾了高带宽与连接的灵活性。  </p></li></ul><ul><li><p>为了实现内存语义互访，UALink在协议层接口（UALink Protocol Layer Interface（UPLI））定义了四个逻辑信号通道，分别是：Request，Originator Data，Read Response/Data，Write Response。其中，请求通道（Request Channel）包含请求地址、请求类型（读取、写入、原子操作以及其他自定义命令）、请求长度、Source标识符、Destination标识符等；读取响应/数据通道（Read Response/Data Channel）最多可包含四个节拍，每拍的64字节，共256字节数据；写入响应通道（Write Response Channel）主要的任务是告知原写请求的完成状态， 因此它不包含数据。通过这些通道，UALink实现了对Load/Store内存语义操作的原生支持。 </p></li></ul><ul><li><p>事务层要解决的问题是如何高效的将来自于协议层的请求打包成TL Flit。在UALink中，每个TL Flit大小为64 Byte，其中0-31Byte构成Lower TL Half-Flit，32-63 Byte构成Upper TL Half-Flit。TL数据以Half-Flit为单位进行打包，并且第一个lower Half-Flit用来存放控制信息，这样的设计极大的降低了包解析时的复杂度，减少了延时开销。 但是32字节的空间用来放置请求控制信息显然是绰绰有余。 如下图（a）所示，此时控制信息占据了16Byte，剩余的16byte都为NOP信息；同时，由于TL Flit以64 Byte为一个整体，因此4号Flit在其Upper TL Half-flit用来填充载荷数据后，其Lower Half-flit绝大部份也被NOP占据， 仅有4个字节用于Flow Control（FC）。 可见这样的打包方式带宽利用率较低低，因此就有了下图（b）这样，将两个写请求合在一起打包，这样就可以充分利用原本被NOP占据的lower Half-flit。此外，我们还可以附带上两个与当前请求无关的WriteResponse请求，因为它不带有载荷数据。 但此时，算上FC信息后，还有共8个字节的空间被NOP占据，我们能否把这些闲置资源也利用起来呢？于是就有了下图（c）这种模式。这里我们打包了三个完整的写请求以及三个Compressed Write Response （CWR），这样没有空间被NOP闲置。 问题是，我们还能进一步提升打包的效率吗？ 这就涉及到了请求信息压缩。请求信息中相当一部分为地址信息（e.g.，64-bit地址信息），但是由于访问的时空本地化（Temporal-Spatial Locality），通常一段时间内只会访问一段邻近的地址空间。因此，如果在发送端和接收端做地址Cache，我们就可以仅传输一小部分的地址位（通常是低地址位）并通过地址Cache重新构建完整的地址。在此基础上，再通过对请求信息的其他部分重新编码，就可以实现用8字节代表原先完整的16字节请求字段。这样，我们就可以实现如下图（d）所示的，对5个写请求和5个CWR的打包。 除了高效打包，TL还负责基于Credit的流控管理，在TL Flit中的FC域就是用来传递credit信息的主要途径。具体来说，在链接初始化时，所有虚拟通道（Virtual Channel）上的发送方都会收到接收方的缓存credit， 发送方据此设定对应的Credit计数器；每次发送端发送一个TL flit, 发送端根据TL Flit的大小，降低credit计数器的值，而接收方在处理完相应Flit，并从缓存中移除该Flit，新释放的credit被接受方放入相应Response Flit的FC域中；发送方在收到Response后，根据FC中的信息增加credit计数。由于在UPLI的设计中，无论请求是读或写，都有相应的Response，因此发送方的credit计数器都能得到及时更新。</p></li></ul><p></p><p><img>https://static001.infoq.cn/resource/image/de/fd/debb5a1400511cc4efbd9c92613a5efd.png<img></p><p></p><p>图9. TL Flit的组织形式</p><p></p><ul><li><p>数据链路层在功能上，将事务层的TL Flits打包成640字节的DL Flit，或者将DL Flit解包多个TL Flits。这640 Byte的DL Flit还包含了 4Byte的CRC编码，3Byte的Flit Header以及5 Byte的Segment Header。由于要维持TL Flit的原子性（即，一个TL Flit不能被分割到不同的DL Flit上），一个DL Flit中包含9个TL Flit，考虑到用于CRC和Header的12 字节，还剩52字节空余。这部分剩余空间可以用来容纳DL-to-DL 消息或者只做Padding。数据链路层还需要支持 UALink构建Lossless 的Scale-up网络， 这也是为何需要有CRC的原因：用它来检测传输过程中是否出现了误码，并由此决定是否要启用链路层重传（Link-Level Retry）。支持重传也就意味着发送端需要保存发送出去的DL Flit，直到它收到来自接收端相应的ACK；但链路层支持重传所需的Flit缓存大小相较于端到端重传在源端的缓存小很多，实现复杂度也更低。这里有必要再提下物理层中加入的40Byte FEC，你可能会问，既然已经有了CRC，为什么还需要FEC？这是因为这里FEC通过纠正物理链路上传输出现的错误减少了循环冗余校验出错的概率，从而也减少了重传的概率，毕竟纠错的效率远高于重传；但是，FEC的纠错能力也是有限的，当误码率超过其纠错能力时，就由CRC兜底，通过重传来修复。另一方面，FEC涵盖的错误发生在物理传输链路上，而CRC还能涵盖部分缓存逻辑电路上的软错误。 因此，这两者是相辅相成的。 </p></li></ul><p></p><p><strong>地址转换</strong>：同NVLink一样，UALink通过PGAS（Partitioned Global Address Space）编程模型将不同节点加速器上的内存组织起来构成一个全局统一的内存空间，实现基于地址的跨节点互访。这里就涉及到三个方面的地址： Gest Virtual Address (GVA)，即源端加速器上所运行程序生成的地址； Network Physical Address（NPA），即网络物理地址，其中的部分信息用于指导UALink 交换机路由；System Physical Address（SPA），即远端加速器上的物理地址，它指向最终要访问的内存。 这三个地址之间存在GVA到NPA再到SPA的单向映射转换关系，其中GVA到NPA的转换由源端的MMU负责，而NPA到SPA的转换由Link MMU负责。当然，GVA不一定指向远端的地址，也有可能也是本地的地址，这也由源端MMU来负责区别和翻译。 UALink通过使用10-bit ReqDstPhysAccID来查找交换器内部的路由表来进行路由操作，因此UALink可以支持最多1024个节点；而这10bit的ReqDstPhysAccID通常是从NPA中提取或转换得到的。</p><p><img>https://static001.infoq.cn/resource/image/2d/cd/2d8yyedb66aff5ea1byycca380d369cd.png<img></p><p>图10. UAL地址转换</p><h2>UAL协议：探讨</h2><p>UAL尽管实现了内存语义，但是它不支持硬件层面的内存一致性。 这是因为一方面，选择硬件层面不支持内存一致性，可以显著降低协议复杂性和实现成本，也降低了协议的延时开销; 另一方面在Scale-Up域，面向PGAS的编程对跨GPU的内存一致性并没有刚性需求，因为计算时的数据是以“Share Nothing”的方式分布的，但是GPU内部也有cache，如何保证GPU间通信时能拿到最新数据，就依赖于软件同步，比如显式的Cache Flush，Invalidation和memory fence等。 因此与CXL不同， UAL选择了轻硬件，重软件的模式，这对于专注于Scale-up的新协议来说，是一个务实明智的选择。 就像NVLink 2.0才开始在硬件层面支持内存一致性一样， 不排除UAL在生态站稳后，开始支持硬件一致性。</p><p> </p><p>当前的UAL版本有点像CXL.mem，但是不同的是，UAL协议上是64-byte 寻址，而CXL是byte-addressable的。 基于64-byte的寻址是AI类负载在Scale-up域中访存行为特征的反应，能有效简化数据包的组织形式，提升带宽利用率；而CXL的细粒度访存能力为更加通用的场景提供了编程灵活性。 另一个不同点是： UAL支持原生的原子操作指令，但CXL.mem并没有原生原子操作命令，它需要CXL.cache的参与才能支持原子操作。 当然最终，原子操作都是有赖于目标端设备来实现。</p><p> </p><p>UAL采用Credit-based Flow Control和Link Level Retry来实现链路的无损传输。这样可以快速从丢包中恢复，避免端到端重传带来的长尾延时, 这对于AI集群系统的性能至关重要。</p><h2>UEC：再造以太</h2><p>传统的以太网，尽管在广域网互连中取得了巨大的成功，但是在数据中心场景，尤其是面向AI和HPC应用的场景中，出现了水土不服，这在RDMA上显的尤为突出。尽管借助ROCEv2，RDMA在数据中心得到了广泛的应用，但它依然面临着不少问题和挑战[4]，比如：</p><ul><li><p>RDMA网络规模扩展性问题。RDMA所依赖的Queue Pair配置是在RDMA环境初始化时设置完成的。尽管这种控制面和数据面分离的方式有它天然的好处，即在进行数据面操作时，无需考虑控制面，从而达到极高的传输效率，但这也意味着，初始化时需要将所有可能的QP都分配好。由于任意两个节点间的每个进程对都需要一个QP，对于一个N个节点，每个节点有P个进程参与通信的系统，这意味着整个系统需要分配P*P*（N-1）个QP。而对于一个scale-out训练用集群，这个N可能会达到几万到数十万。考虑到一个连接状态信息可能多大1KB， 显然，要维持这样规模数量的QP，对硬件来说极为昂贵。</p></li></ul><ul><li><p>在一个有损（Lossy）网络上构建RDMA的天然缺陷问题。传统以太网是一个有损网络，没有链路层重传(LLR)，所有的重传都要通过端到端的传输层协议来实现，这一设计理念在广域网中是一个极具智慧的选择。它一方面降低了网卡和交换机的硬件成本，提升了网络规模扩展的效率；另一方面， 它也是一种高效的错误隔离手段，避免了因为硬件缺陷结合链路层重传而导致的错误扩散。但是RDMA要构建在无损网络之上，为了支持数据中心RDMA，RoCEv2通过三层的ECN（Explicit Congestion Notification）和CNP（Congestion Notification Package）再结合二层的PFC（Priority Flow Control）构成的DCQCN来创建处一个近乎无损的网络。这是一个复杂的，混合了二层/三层的技术组合，带着浓浓的应对先天不足的无奈。 </p></li></ul><p></p><p>因此，在面向AI和HPC负载的场景中，我们需要一个从头重新思考和构建的新以太，而不是在原有协议上的修修补补。</p><h2>UEC协议概览：</h2><p>UEC依然采用传统以太网的四层协议架构，但是在数据链路层加入了Credit-based Flow Control以及链路层重传的支持，实现了原生的无损网络，并对传输层做了根本上的重构，通过定义Semantics Sublayer（SES），Packet Delivery Sublayer（PDS），Congestion Management Sublayer（CMS），以及Transport Security Sublayer（TSS）四个子层，实现了对上层语义，短时连接，多路径，拥塞控制，传输安全等方面的支持。 </p><p><img>https://static001.infoq.cn/resource/image/99/8e/99b4980038d3d7f3866byy15bff6188e.png<img></p><p>图11. UE 整体架构图（Source:[2]）</p><p></p><p><strong>短时连接（Ephemeral Connection）：</strong>为应对RDMA网络规模扩展性的问题，UEC重新构建了QP的配置方式。它基于这样一个观察：一个节点不可能同时与所有其他节点及进程通信， 即便像是All-to-all这样的集合通信，它也是分解成点对点通信以一定的顺序完成（e.g.，Recursive-Doubling算法）。因此，从一开始就为每个节点设置所有可能的QP其实是一种保守的资源over provisioning。 一种更为高效的做法是预留少量的QP资源池，在实际通信时动态的复用和释放这些QP资源，这就是Ephemeral Connection的核心想法。具体来说，UEC在PDS子层中定义了Packet Delivery Context（PDC），这是一个动态创建的为连接提供上下文的逻辑结构，与它相关联的信息包含：Job_ID, source Fabric Address, destination Fabric Address, Resource Index, Traffic Class等。 下图展示了短时链接构建的流程： 当源端发起连接的首个包时，它将源端的PDC ID和它期望目的端分配的Context信息放入包头，并将SYN设为1；当目的地端收到这些PDS请求包时， 由于SYN=1，它将检查相关连接是否已经建立；若没有，它分配一个PDCID并从相应的资源池中获取与之相关联的缓存资源，其中在PDC中Resource Index，可以被看作是指向缓存队列的指针；之后，目的地端将新分配的PCD ID通过ACK返回给源端；源端收到ACK后，后续的PDS请求中DPDCID被设为目的地新分配的PCDID，同时SYN被置为0；至此，连接构建完成。可以看到，短时连接的建立过程极为高效，它和数据传输是同步进行的。</p><p><img>https://static001.geekbang.org/infoq/3a/3a3772c02c6eb5ca50d40e8ad61f9d74.png<img></p><p>图12. 短时链接构建流程</p><p> </p><p><strong>多路径（Multipath）和包喷洒（Packet Spray）:</strong> 传统的ECMP（Equal-Cost Multi-Path）是面向基于五元组（源和目的地址、源和目的端口，以及协议类型）定义的流。不同的流会被哈希到不同的路径上， 但同一个流中的所有数据包都沿同一条确定性路径转发。显然，对于AI场景中常见的大象流，这种方式并不能充分利用多路径带来的带宽。因此，在UEC中， 多路径的对象成了数据包，使得同一个flow中的不同数据包也可以走不同的路径。为达到这一目的， UE在UET包头中引入了一些随机机制或者Entropy，并参与到选择路径的哈希中。具体来说，当UEC使用UDP封装时，随机生成的UDP源端口被用作Entropy。这是因为UDP中，一般发送方不期望得到接受方的回复，因此源端口通常是可选的，UEC利用了这一点，将随机性注入数据包头中。若UEC不使用UDP封装而是仅采用IP封装，那么，就必须使用UET Entropy Value（EV），作为PDS包头的一部分。网络节点可以通过选择不同的EV，让每个数据包经过不同路径发送；也可以对数据包使用相同EV，确保它们能够按序交付。</p><p></p><p><strong>乱序包交付，顺序消息交付（Out-of-Order Packet Delivery with In-order Message Delivery）</strong>：在使用多路径和包喷洒的情况下，必然出现数据包的乱序到达。好在包括AI/ML类负载在内的大部分应用并不关注数据包到达的顺序，只关注消息到达的顺序。但也有些应用需要In-order Packet Delivery，比如一些Legacy的HPC应用，网络入侵检测系统等。为此，UEC定义了可靠无序交付模式（Reliable Unordered Delivery，RUD），不可靠无序交付模式（Unreliable Unordered Delivery, UUD），可靠有序交付（Reliable Ordered Delivery，ROD)，以及幂等可靠无序交付 （Reliable Unordered Delivery for Idempotent Operations，RUDI) 四种模式，来应对各种场景需求。 其中RUD是UEC默认的传输模式，它充分利用了多路径带来的高带宽利用率，结合RDMA所带来的Direct Data Placement，让数据包直接放入应用的内存空间中，避免了NIC缓存中转。为确保消息的顺序交付， 每个消息都关连一个ID（Message ID，MID），每发送/接收一个消息都会在本地递增该消息ID。这样，若一个消息的数据包都到达，但是它的ID大于当前预期的ID，该消息就被Hold住，直到前面的所有消息完成交付。 在这种模式下，若出现丢包现象，则采用选择性重传（Selective Retransmission）以实现高效的丢包恢复。 而ROD模式禁用了包喷洒，确保数据包能够顺序到达， 并采用传统的Go-Back-N重传的方式应对可能出现的丢包情况。 </p><p> </p><p><strong>拥塞控制:</strong> UEC的拥塞控制主要承载在CMS上。它定义了两个互补的拥塞控制算法：一个是主要运行在发送端的基于网络信号拥塞控制（Network Signal-based Congestion Control，NSCC）；另一个是主要运行在接收端的基于接受端信用拥塞控制（ Receiver Credit-based Congestion Control，RCCC）。 它们可以单独使用，也可以一起使用。 </p><ul><li><p>NSCC综合考虑两个网络指标： ECN标识和Round-Trip Time（RTT）。与传统的DCQCN不同， ECN标识的拥塞信息不是通过CNP包反馈给源端，而是通过ACK包头上的Congestion Control（CC）State域来传递的，从这层意义上来说，它和DCTCP相似。不同的是，NSCC还综合考虑RTT，而RTT的测量是基于时间戳（Timestamp）来实现的：发送端会记录发送PDS请求的时间戳，同时也会记录对应ACK到达发送端的时间戳，两者的差再减去ACK包在接收端生成的时间，就是RTT较为精确的测量值。于是，我们就有了是否有ECN标识以及RTT高低所构成的四种不同状态组合，NSCC将根据这些状态组合来调整发送窗口的大小，从而实现拥塞控制。  </p></li></ul><ul><li><p>RCCC通过以Credit表示的接收端可用缓冲区大小来做拥塞控制。从大的方面来讲， RCCC的控制机制和传统的基于Credit拥塞控制方式差别不大：都是在发送端和接收端都维持Credit Counter，在发送端每发送一个数据包，相应的credit 计数器减一，当credit计数器到零时，发送端暂停发送数据包；当它收到来自于接收端的Credit更新（包含在ACK报文中）， 相应的Credit计数器将增加相应的量。 那么，发送端是如何在一开始知道接收端Credit数量的呢？这就是RCCC和传统的基于握手方式不同之处：RCCC一开始通过一个小的“Eager Window”发送数据包，这个窗足够小，确保接收端不会出现credit耗尽的情况，同时，利用这期间的ACK，发送端就可以知道接收端的Credit数量。</p></li></ul><p></p><p><strong>安全</strong>: UEC专门强调了数据传输安全并由TSS来承载, 解决了传统RDMA网络上没有原生加密支持，依赖于IPsec，性能开销大的问题。  TSS可以有选择性的对PDS包头（不包含Entropy域）和Payload数据进行加密。这里我们介绍两点UEC的安全特性：</p><ul><li><p>密钥分发：由于面向AI的负载，来自于一个租户的任务需要大量节点共同参与，如果采用传统的点对点密钥的方式，那么系统就要分发和管理Nx(N-1) 份密钥，这不仅带来了高昂的密钥分发成本，而且也因管理密钥造成了不小的性能开销。事实上，这种点对点的加密对于一个租户来说没有必要。因此TSS定义了一个Secure Domain（SD），所有在这个SD内的节点都共享相同的Secure Domain Key（SDK）。 这个SDK通过安全的带外通道分享给所有SD节点。 每个节点上，TSS采用Key Derivation Function (KDF)，结合每个工作任务的Session，从SDK中推演出相应的密钥。这样每个Session，所有节点都有相同的密钥，不同session的密钥不同，而且也避免了每次Session密钥分发的开销。</p></li></ul><ul><li><p>重放攻击（Replay Attack）的防护机制。所谓Replay攻击，是指攻击方通过劫持或拷贝原通信数据包，然后向目的地重新发送这些数据包，使系统在未经用户授权的情况下，让系统重复之前的操作。由于PDS连接本质是无状态的，因此在建立PDS的时候，如果可以接受任意起始的 Package Sequence Number（PSN），UEC系统就容易受到Replay 攻击。为此，TSS在一个SD中保留了start_psn和expected_psn。当接收端收到的连接请求中PSN小于 expected_psn，接收端会生成一个NACK，指示应使用的起始PSN；否则，该请求将被接受。当PDC关闭时，目标端的 expected_psn 会被更新为该PDC的当前PSN+1。这样当Replay攻击者发送之前的包文时， 由于报文的PSN小于expected_psn，因此该报文不会被接受，从而避免了重放攻击。新的 expected_psn 值会作为关闭PDC确认（ACK）的一部分返回给源端，源端随后将 start_psn 更新为此值，以确保后续连接可以以零往返时间建立。</p></li></ul><h2>UEC协议：探讨</h2><p>综上，可以看到UEC是对传统以太网在数据中心中应用的经验提炼，对RDMA网络所面临问题的全面思考，并在此基础之上提供了一个崭新的网络基座。 很多传统以太网在数据中心中遇到的问题将不复存在。 </p><p></p><p>和UAL一样，UEC也通过CBFC和LLR，实现了链路层的无损传输，这充分响应了大规模AI集群在长尾延时方面的诉求。同时，对可能出现的失效因LLR的扩散问题，在数据中心的高可观察/高可控环境下，有多种技术和手段予以应对。</p><p></p><p>UEC所面对的Scale-out网络规模大约在万卡到数十万卡量级。 这样的规模决定了其通信时延的量级，进而决定了UEC必然要采用面向连接的消息语义，而非面向内存的LD/ST语义。 面向连接的消息语义意味着有消息准备和消息到达通知等开销，通信粒度较大，时延较长，但是它可以在程序层面通过异步非阻塞（non-blocking）通信机制让计算隐藏一部分的通信延时；而面向内存的LD/ST语义则意味着更细的通信粒度（Cacheline级别），更短的延时（数百纳秒内），在GPU上，Load延时隐藏通常通过Warp的快速切换来实现。这两种截然不同的机制决定了Scale-out网络和Scale-up网络存在着清晰的边界。Scale-Up网络无法支撑起Scale-out网络的规模；反过来， Scale-out网络也无法实现Scale-up网络的性能。 </p><h2>竞合的互连江湖</h2><h2>Scale-up Ethernet (SUE)： 何方神圣？</h2><p>如果你了解UALink的成立过程， 你会发现博通一开始是在创始成员名单中，但后来并不在董事会成员中，而是自立门户，拉起了一杆SUE的大旗。 这里就有两个问题： 首先SUE是个啥？根据博通在25年7月发布的\"33页\"SUE规范，我们可以发现SUE和UAL有着诸多相似之处：都采用200G 以太网的PHY，都支持LLR和CBFC，都支持内存语义, 支持的超节点规模都是1024个。 但技术上，两者最根本的区别在对数据的封装上。UAL定义了全新的传输层和数据链路层的封装格式，其中传输层的包头包含了内存操作类型，支持原生的内存语义；而SUE要么采用传统的UDP+IP+以太网包头封装，要么采用AI Forwarding Header Gen 1/2格式的封装，如下图所示。 SUE上内存操作请求被放在SUE Payload中，发送到对端。可以看到，SUE的封装沿用了很多传统以太网的做法，相较于UAL复杂度更高，链路所需的延时也更高，大约比UAL多出数百纳秒。</p><p><img>https://static001.geekbang.org/infoq/ef/efd3811bc584d12eccb0134e1fff7794.png<img></p><p>图13. SUE的几种数据打包格式</p><p> </p><p>那么，博通为什么弃UAL而推SUE？ 这得从另一方面的信息来解读: 几乎在发布SUE规范的同时， 博通就发布了支持SUE的Tomhawk Ultra Switch芯片，并且和Tomhawk 5.0实现了Pin脚兼容。这说明，博通在SUE上已经在背后耕耘了很久。起初推动UAL成立的目的大概率是看看能否将SUE的这套规范推进到UAL，从而以近乎零成本占据Scale-up互连的最佳生态位。但在UAL坚持从头设计精简的封装模式后，如果还留在UAL，那反而成了后续SUE产品的反向宣传，不如退出。那么，博通为什么选择SUE这条路线呢？我们推断这是因为它在以太网交换机上有太多的利益。在以太网基础上进行轻量化改造，能够实现快速time-to-market， 先下手为强，抢到Scale-up互连的蛋糕， 不香么？ 相比之下，最早支持UAL的芯片或交换机可能要等到2026年年末。 </p><p></p><p>我们乐见博通能从NVLink虎口夺食， 但不认为SUE是Scale-Up协议发展的趋势。 如前所述，一个好的Scale-Up协议必须是延时导向的，内存语义原生的精简协议，才能解决Scale-Up网络的需求。而SUE复用了不少以太网的封装模式，看起来更多的是一个产品策略。虽然博通声称SUE是开放标准，谁都可以使用，但是相比UAL标准的253页和UEC的563页，SUE这33页规范，像是个产品说明书，多少让人感觉少了些诚意。</p><h2>Scale-up域，何处是边界？</h2><p>Scale-up域的大小根本上由两个因素决定：模型的大小和GPU算力及显存大小。模型的大小主要由两方面决定：静态的模型参数规模和动态的内存KV Cache。 下图（a）展示了QWen系列模型的大小。可以看到，突破模型大小边界的任务还是由语言类大模型来承担，其他各种专项或者多模态模型明显小于同代次LLM模型参数规模。但是即便是LLM，它模型参数的增长已经过了爆发式增长期，开始逐渐减缓。这背后一个更深层次的原因是，用来训练的数据已经接近人类产生的公开文本数据的上限，如下图（b）所示。 这在某种程度上也解释了为何当前Pre-training Scaling逐渐变缓。但也是因为随着scaling的重心转移到test-time scaling上， 长上下文所带来的KV cache的内存需求迅速增加，对内存容量的需求甚至超过了模型参数。 但为了应对KVCache对内存容量的挑战，业界也有大量的针对KVcache 的量化，压缩，和注意力稀疏化的工作，来减少KVcache的增加对内存容量和性能的影响。 所以，综合来看，我们的判断是大模型对内存容量的需求在未来还会持续增长，但是增长速度会明显放缓。 </p><p></p><p><img>https://static001.infoq.cn/resource/image/d6/8f/d68eb1605306689723f5d461fb5a548f.png<img></p><p></p><p>（a）</p><p><img>https://static001.infoq.cn/resource/image/e7/34/e725511b737a89cf084435c4f8a44f34.png<img></p><p>（b）</p><p>图14. (a)通义大模型参数规模；(b) LLM训练用数据vs人类产生的数据 [5]。 </p><p> </p><p>接着，我们再来看GPU算力/显存密度。下表展示了NVIDIA和华为的GPU/NPU产品线在算力和显存容量上的对比。 可以看到，NVIDIA的单位GPU的FP4算力几乎以每年两倍的速度在增长，而显存容量在Rubin Ultra这一代比之前增加到近4倍。相比之下华为的昇腾系列，虽然算力也以每年两倍左右的速度在增长，但由于起点较低，在算力密度上和NV产品线保持着大约25倍的差距。显存容量方面， 昇腾960和Rubin相仿，但是与Rubin Ultra相比也差了4倍。 虽然国内还有众多其他的GPU生产商， 但这些差距基本反映了当前国内在半导体制程，HBM，以及先进封装方面整体落后于美国的现状。</p><p>表2. 两家GPU产品线的算力和显存对比</p><p></p><p> </p><p>基于这样的现状和地缘政治，我们可以推断中美两国AI集群将呈现两种不同的形态：美式AI集群，由于单卡算力和显存密度高，其Scale-up域会尽量锚定在一个机柜内，毕竟这样Scale-up域无需经过多级交换机，整体通信延时是最优的。尽管NVIDIA将推出Vera Rubin CPX Dual Rack产品，但这是一种PD分离的组织方案, 两个机柜并不在一个Scale-up域：Rubin CPX机柜专用于Prefill，通过InfiniBand或Ethernet 与VR NVL144机柜相连。国内的AI集群，由于单卡算力密度和显存密度较低，其Scale-up域必然比美式集群的Scale-up域大。一个典型的例子就是华为的CloudMatrix384 [6]，它有16个机柜，其中12个用于计算，4个用于互连交换；384个NPU被分配到48个计算节点中，每个计算节点含8个NPU；所有NPU通过两级UB（Unified Bus）交换机连接，其中，一级交换机在计算节点内部，一级和二级交换机间通过LPO（Linear Pluggbale Optics）互连。两级交换可以支撑更大规模的Scale-Up域，384NPU处理器显然不是它能达到的上限。因此，我们预计Scale-up集群规模会经历扩大再收缩的过程： 规模扩大是因为当前国产单卡的算力和内存密度演进速度还未赶上大模型的发展速度，扩展超节点规模是最直接的应对方式；但是随着单卡算力和内存密度的快速提升，系统对互连延时的敏感性也迅速增加，过长的延时将使集群算力利用率迅速下降，因此像CloudMatrix384这样的两级交换超节点架构，必然要回归到单级交换，单机柜超节点的形态。 这必然对机柜的高密度架构设计，供电和散热都提出了巨大挑战。从这层意义上来说说， 单机柜的硬件工程能力（高密，供电，散热）决定了超节点规模的边界。 阿里云在这一领域也走在业界的最前沿。</p><h2>CXL能否王者归来？</h2><p>在2021-2022年，CXL几乎统一了包含GenZ，OpenCAPI，CCIX在内的主要服务器级内存一致性互连，风头盛极一时。但随着LLM大模型的兴起，大量的资本被吸引到AI基础设施的建设上，CXL领域的投入因为无法看到短期收益而大幅减少，其发展从聚光灯下，移到了幕后。但是CXL标准并没有停止演进，自从CXL 3.1推出GIM之后，CXL就开始摆脱非对称内存访问的标签。从功能上来讲， CXL具备了UAL的几个关键特性：内存语义，设备间内存互访；此外，它还具有UAL不具备的特性，比如，Host和设备的统一编址（UMA）以及通过CXL交换机的GPU资源池化，前者可以为推理系统提供更加庞大且相对廉价的内存，而后者则为节点的GPU Scale-Up提供了另外一种技术路径。 但是不幸的是，PCIe的速率问题严重影响了CXL潜力的发挥：当前主流的PCIe Gen5的速率才达到32GT/s，而PCIe Gen 8 虽然速率将达到 256GT/s，其x4的带宽和UAL 1.0的x4带宽基本持平，但是它要到2028年才可能有相应的IP，而要被集成到GPU中，则需要等待更长的时间。 这里的有两个问题需要回答： </p><ul><li><p>GPU集成CXL的动因是什么？由于GPU是以PCIe设备的形式呈现给操作系统的，因此GPU上必须要集成PCIe接口。即便是像GB200那样，CPU和GPU之间通过NVLink-C2C相连，它们之间也需有一个x1的PCIe通道来做GPU的枚举和设备配置等控制面的工作，而NVLink-C2C则负责数据面的数据搬移工作。但这是NVIDIA的专属方案，在开放世界中，最主要的方式依然是采用PCIe来同步承接控制面和数据面的工作，并且常以x16的端口形态出现，以满足CUDA Kernel Launch时数据搬移所需带宽。但除此以外，在AI集群中，GPU的PCIe端口带宽利用率很低，这其实是一个不小的浪费。而当PCIe演进到Gen7/8，在这之上的CXL一方面提供了与UAL相近的带宽，另一方面也实现了Host和GPU的UMA，两者结合可以极大的改善带宽利用率。相对于PCIe，它不增加额外引脚数，不会对芯片紧张的IO Pad面积增加额外的压力。这里面的巨大正收益将驱使非NVIDIA的GPU集成CXL，使它们与NVIDIA GPU在UMA上处在同一水位上。</p></li></ul><ul><li><p>对于集成CXL的GPU，其集群的组织形态有什么变化？像GB200那样，Host CPU和GPU之间的直连是利用CXL-enabled GPU的最直接的方式。此时由于CPU引脚资源受限，同时又要预留较多引脚给DDR内存，因此单个CPU直连的GPU数目较少，CXL在这里的作用就是构建Host与GPU之间的UMA。但CXL还提供了另外一种集群组织形态：GPU通过CXL 3.1 Switch连到CPU。此时，CPU与挂载在Switch上的各个GPU之间构成统一的内存空间；同时，GPU与GPU之间通过 CXL Switch实现点对点通信。 由于CXL的P2P通信地址分配和UAL的地址分配存在冲突，因此，在采用CXL Switch后，GPU就只能通过它来构成Scale-Up域。该Scale-Up域的GPU数量取决于CXL Switch的端口数量，64或32是一个可以实现的配置（当然，可以通过两级交换机的方式实现单个 Scale-Up域更多的GPU数量支持，但访存延时将增加）。而Scale-Out通信则通过挂载在同一个CXL Switch上的多张网卡来实现。与现有超节点集群不同，这个组织架构一定程度上回归了传统基于PCIe交换机的集群构建模式，但因为CXL，也具有了一些令人期待的特性：通过CXL Switch，它提供了更加灵活的CPU-GPU配比；通过CXL Memory，提供了更加灵活和低成本的内存扩展等。我们认为这是一个值得探索的方向，当然也有赖于整个CXL上下游生态的共同努力。诺能成, 则是CXL的高光时刻，我们拭目以待。 </p></li></ul><h2>结语</h2><p></p><p>过去几年，大模型的爆发式发展对AI基础设施提出了巨大的需求，我们见证了私有互连的春天：那些具有完整芯片产品线且积极布局互连技术的公司，比如NVIDIA（NVLink），AMD（Infinity-Fabric），以及华为（Unified Bus），是最先吃到这波AI基建浪潮红利的玩家。 而开放互连的春天要来的晚一些：UAL和UEC相应的产品落地可能需要到26年年底。好在整个行业已经通过UCIe/CXL/UAL/UEC构建起了一个从芯粒互连到Scale-Out互连的完整技术路图。 或许感受到了来自于开放互连的压力，NVIDIA也开始尝试NVLink IP授权的商业模式，结合近期Intel和NVIDIA的合作，不难想象在不久的将来，我们可能会看到在x86 CPU通过NVLink与NV GPU相连的服务器产品。这反过来把压力传递到了开放互连这边。 这是“独行快，众行远”的生动展现；我们也希望通过私有和开放的相互竞争，彼此成就，推动互连技术的发展，最终实现AI的普惠。</p><h2>参考文献</h2><p>[1]. Yiren Zhao, Ran Shu, and Yongqiang Xiong. 2025. SRC: A Scalable Reliable Connection for RDMA with Decoupled QPs and Connections. In Proceedings of the 9th Asia-Pacific Workshop on Networking (APNET '25). Association for Computing Machinery, New York, NY, USA, 44–50. https://doi.org/10.1145/3735358.3735366</p><p>[2]. Hoefler T, Schramm K, Spada E, Underwood K, Alexander C, Alverson B, Bottorff P, Caulfield A, Handley M, Huang C, Raiciu C. Ultra Ethernet's Design Principles and Architectural Innovations. arXiv preprint arXiv:2508.08906. 2025 Aug 12.</p><p>[3]. Wu, X., 2025. Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards. arXiv preprint arXiv:2505.02686.</p><p>[4]. Hoefler, Torsten, Duncan Roweth, Keith Underwood, Bob Alverson, Mark Griswold, Vahid Tabatabaee, Mohan Kalkunte et al. \"Datacenter ethernet and rdma: Issues at hyperscale.\" arXiv preprint arXiv:2302.03337 (2023).</p><p>[5]. </p><p>[6]. Zuo, P., Lin, H., Deng, J., Zou, N., Yang, X., Diao, Y., Gao, W., Xu, K., Chen, Z., Lu, S. and Qiu, Z., 2025. Serving Large Language Models on Huawei CloudMatrix384. arXiv preprint arXiv:2506.12708.</p><p>[7]. </p></div>",
            "link": "https://www.infoq.cn/article/yK0XQ02CR1ZUKgM5HUFL",
            "pub_date": "2025-10-21 03:52:15",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        }
    ]
}