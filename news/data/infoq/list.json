{
    "data": [
        {
            "id": "Ct0XiRzHirx4rz1ESjuH",
            "title": "2025年InfoQ趋势报告：云计算和DevOps篇",
            "image": "https://static001.infoq.cn/resource/image/13/59/138c45d7a73a19c942d63fe6ec3ace59.jpg",
            "description": "<div><p><color>InfoQ趋势报告为InfoQ读者提供了一个简洁且有见解性的概览，我们认为架构师和技术领导者应该优先考虑这些主题。除了这份报告和更新后的DevOps和云InfoQ趋势图之外，还有一个</color><color>，其中包括几位编辑和InfoQ的朋友们讨论这些趋势。</color></p><p><color> </color></p><p><color>年度趋势报告的一个重要部分是趋势图，它显示了哪些趋势和主题进入了创新者类别，哪些被提升到了早期采用者和早期主流类别。这些类别基于Geoffrey Moore的</color><color>一书。在InfoQ，我们主要关注尚未跨越鸿沟的类别。以下是今年的图表：</color></p><p></p><p><img>https://static001.geekbang.org/infoq/a4/a43db4d1ddcf609f188354d1e663d26f.jpeg<img></p><p></p><p><color>自InfoQ团队18个月前讨论趋势报告以来（见下面的2023年趋势图），已经发生了一些重大的创新和发展。</color></p><p></p><p><img>https://static001.geekbang.org/infoq/34/3425bec43aca64d19a2f99429af47af1.jpeg<img></p><p></p><p><color>本文主要展示了不同技术采用阶段的趋势图，并提供了去年发布趋势报告以来新增或更新的个别技术的详细信息。我们还讨论了哪些技术和趋势在采用图中得到了提升。</color></p><p><color> </color></p><p><color>以下是自去年报告发布以来一些值得关注的变化：</color></p><h2>创新者</h2><p><color>在我们的采用图中，第一个类别是创新者，在这里我们会看到一些变化，这些变化主要是由高级AI自动化主导的。下面详细讨论了这些变化。</color></p><p><color> </color></p><h2>云工程中的AI Agent</h2><p><color>今年看到显著发展的一个领域是AI工具从简单的聊天机器人演变为复杂的、面向行动的AI Agent。这些Agent旨在执行复杂的任务并与云资源进行交互，这超越了对话辅助，真正增强了工程团队。这一转变标志着迈向企业级就绪AI的重要进步，当然这仍然需要人类监督和强有力的治理。</color></p><p><color> </color></p><p><color>Steef-Jan Wiggers和Matt Saunders在</color><color>中讨论了AI Agent：</color></p><p><color>Steef-Jan Wiggers</color></p><p><color> </color></p><p><color>Matt Saunders</color></p><h2>可实用的、早期的治理自动化</h2><p><color>创新者还在可实用的层面上引入AI，以执行治理和简化内部流程，本质上这实现架构师和审查员角色的自动化，以减少摩擦和认知负荷。</color></p><p><color> </color></p><p><color>我们的小组成员在播客中讨论了在实用层面上引入AI以执行治理。</color></p><p></p><p><color>Shweta Vohra</color></p><h2>模型协议上下文（Model Context Protocol，MCP）</h2><p><color>Anthropic在2024年11月引入的</color><color>是一个开放性的标准，旨在帮助前沿的大型语言模型产生更好、相关度更高的回应。它提供了一个标准化的协议，用来在外部工具、系统和数据源与大型语言模型之间集成和共享数据，它有效地消除了分散和定制集成的需求。像OpenAI、微软和谷歌这样的主要参与者已经宣布计划将对MCP的支持集成到他们的产品中。</color></p><p><color> </color></p><p><color>小组成员讨论了MCP在云和DevOps中将扮演的角色。</color></p><p><color> </color></p><p><color>Matt Saunders</color></p><h2>早期采用者</h2><p><color>在早期采用者类别中，我们想强调的是平台工程团队、跨云/云原生混合方法和开发者体验框架。</color></p><p><color> </color></p><h2>平台工程成熟度</h2><p><color>随着平台工程概念的发展，组织正在超越简单的工具聚合，而是在构建内部的开发平台（IDP），将平台本身视为具有可衡量结果的产品。然而，真正具有连贯性的“平台即产品（platform as a product）”实现很少；它的成功取决于价值流思维、清晰的团队界限和减少工具碎片化。</color></p><p><color> </color></p><p><color>Shweta Vohra</color><color>:</color></p><h2>跨云/云原生混合方式</h2><p><color>早期采用者不仅仅使用多个云，他们正在拥抱一个由合规性和主权等外部约束驱动的策略性混合云和多云方案。他们的目标不仅仅是采用云，还要确保业务连续性，并遵守不同地理区域的法律。</color></p><p><color> </color></p><p><color>在数字主权下，云的生态正在扩大。除了“三大”厂商之外，区域提供商正在出现；Kubernetes成为事实上的基础，混合/多云是针对延迟、合规性和灾难恢复的默认设置。</color></p><p><color> </color></p><p><color>Steef-Jan Wiggers</color></p><p></p><p><color>Shweta Vohra</color></p><h2>开发者体验框架（DevEx）</h2><p><color>对衡量和提高开发人员效率的工具的大量资金投入表明，量化开发人员体验的框架（DevEx）属于早期采用者阶段。它们被视为将工程工作直接与业务价值联系起来的关键创新。</color></p><p><color> </color></p><p><color>Daniel Bryant和Matt Saunders的讨论提到了DX被Atlassian以十亿美元收购的重大新闻。</color></p><p></p><p><color>Matt Saunders</color></p><h2>早期主流</h2><p><color>我们将以下主题转移到了早期主流类别，因为这些技术已经变得更加成熟，并被各种组织的软件开发团队广泛采用。</color></p><ul><li><p><span style=\"color:#494949\">专注于开发者体验</span></p></li></ul><ul><li><p><span style=\"color:#494949\">服务网格和eBPF</span></p></li></ul><ul><li><p><span style=\"color:#494949\">持续测试</span></p></li></ul><h2>晚期主流</h2><p><color>在我们采用曲线的最后一个类别，即晚期主流类别，也出现了一些新成员，因为这些技术现在已经被团队完全采用，并成为他们核心架构模式的一部分。</color></p><ul><li><p><span style=\"color:#494949\">FinOps</span></p></li></ul><ul><li><p><span style=\"color:#494949\">企业级DevOps工具链</span></p></li></ul><ul><li><p><span style=\"color:#494949\">可观测性</span></p></li></ul><h2>结论</h2><p><color>FinOps、可观测性和企业DevOps工具链的成熟，并进入晚期主流阶段，标志着云和DevOps生态系统的一个重要拐点。基础现在是稳固的：团队已经在云原生基础设施上进行了标准化，将自动化作为默认选项，并规范化了持续交付的实践。</color></p><p><color> </color></p><p><color>然而，正是这种成熟暴露了一系列新的挑战。跨组织的工具和责任的碎片化导致了认知超载和收益递减。许多领导者正在发现，下一波进步将不是来自采用更多的工具，而是来自整合和管理已经就绪的工具。</color></p><p><color> </color></p><p><color>与此同时，人工智能Agent和模型上下文协议（MCP）等标准的出现标志着一种新的运维范式的开始，在这种范式中，自动化变得越来越自主，但永远不能完全独立于人类的判断。将人工智能驱动的执行与明确的护栏和平台治理相结合，这决定了这些技术是增强还是破坏生产力。</color></p><p><color> </color></p><p><color>与此同时，数字主权和可持续性的问题继续将基础设施决策拉回混合云和多云的现实。这些制约因素迫使人们重新重视弹性、成本优化和数据控制，从而强化了FinOps和平台工程作为战略学科而非战术学科的重要性。</color></p><p><color> </color></p><p><color>简而言之，DevOps运动正在进入下一个阶段，这不是颠覆性的方式，而是深思熟虑的整合。这个新阶段的赢家将是那些将平台工程、人工智能自动化和以人为中心的治理整合成一个有凝聚力的价值流，将效率转化为流程，将复杂性转化为清晰性的人。</color></p><p><color> </color></p><p><color>原文链接：</color></p></div>",
            "link": "https://www.infoq.cn/article/Ct0XiRzHirx4rz1ESjuH",
            "pub_date": "2025-10-31 08:28:21",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "DXJFlWBkQJ260Y7KIWML",
            "title": "共筑智能时代安全防线！AI 创新与系统安全分论坛议程出炉 | 2025 龙蜥大会",
            "image": "https://static001.infoq.cn/resource/image/80/39/80ff1bc4207c348038fc97f3bbf6ec39.jpg",
            "description": "<div><p><strong>2025 龙蜥操作系统大会将于 11 月 17 日在北京·星地艺术中心举办，由中国计算机学会开源发展技术委员会、泛在操作系统开放社区、中关村科技园区朝阳园管理委员会（北京市朝阳区科学技术和信息化局）、中国开源软件推进联盟指导，龙蜥社区主办，中关村互联网 3.0 产业园（星地中心）协办，阿里云、中兴通讯、海光信息、Intel、浪潮信息、Arm 等 24 家理事单位共同承办，主题为“生态共融·智驱未来”， 云集逾千位全球技术领袖、业界精英和行业开发者，深入探讨 AI 时代下操作系统产业发展的新趋势、新挑战与新路径。大会将聚焦技术融合、生态共建、人才培养等核心议题，致力于推动操作系统与AI、云计算、大数据等前沿技术的深度融合，打造政、产、学、研多方深度对话的年度盛会，构建更加开放、协同、繁荣的产业生态体系。</strong></p><p></p><p>在人工智能技术迅猛发展的今天，大模型、智能体与生成式 AI 正深度融入关键基础设施与企业核心业务，操作系统作为连接硬件、AI 框架与上层应用的“信任根”，其安全能力直接决定了智能时代的基础设施可信度。如何在保障高性能 AI 计算的同时，构建从芯片到应用的全栈安全防护？这已成为国产操作系统必须回答的时代命题。2025 龙蜥操作系统大会安全分论坛以<strong>“AI 创新与系统安全”</strong>为主题，聚焦系统内生安全架构、机密计算与可信 AI、智能化安全防御等几大核心方向，共同探讨如何以安全为底座、以 AI 为引擎，打造面向未来的操作系统安全生态。</p><p></p><p>本论坛由龙蜥社区主办，龙蜥社区安全联盟（OASA）、海光信息联合承办，汇聚来自企业、高校及科研机构的顶尖安全专家与系统架构师，共同呈现精彩技术演讲、架构剖析与实战分享，诚邀安全工程师、系统开发者及开源爱好者莅临交流，共筑智能时代的安全防线。</p><p></p><p><strong>时间：</strong>2025.11.17 13:30-16:30</p><p></p><p><strong>地点：</strong>北京·星地艺术中心 D 座 5F 会议室 3</p><p></p><p><strong>报名链接</strong>（复制链接至浏览器打开）：</p><p><italic>https://hd.aliyun.com/form/6795</italic></p><p></p><p>会议议程详见下方海报：</p><p></p><p><img>https://static001.geekbang.org/infoq/10/104c3fc9feb132108df34a2ca429064b.png<img></p><p></p><p>更多 2025 龙蜥大会详情，请点击大会官网链接查看（复制链接至浏览器打开或点击阅读原文）：</p><p><italic>https://openanolis.cn/openanolisconference2025</italic></p></div>",
            "link": "https://www.infoq.cn/article/DXJFlWBkQJ260Y7KIWML",
            "pub_date": "2025-10-31 04:01:52",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "DWyEL9LsfxEV8JW5eZWU",
            "title": "火热报名中！2025 龙蜥操作系统大会亮点速递",
            "image": "https://static001.infoq.cn/resource/image/61/89/614eb2533cf3f6c55d1438ed9cf7f389.jpg",
            "description": "<div><p>Hi，小伙伴们，好久不见，很高兴再次与大家相聚在北京！</p><p></p><p>2025 龙蜥操作系统大会（简称“2025龙蜥大会”）将于 11 月 17 日在北京·星地艺术中心盛大召开，主题为“生态共融·智驱未来”，汇聚全球操作系统领域的前沿探索与最佳实践，诚邀产业共建者一起，洞悉行业机遇，把握产业脉动，领略技术跃迁，聆听生态蝶变。</p><p></p><p>作为全程蹲守筹备一线的“情报官”小龙，今天就带着满满诚意，为大家抢先剧透一波：更硬核的技术论坛、更沉浸的动手体验、更闪耀的生态阵容……统统安排上！</p><p></p><p>那 2025 龙蜥大会到底有哪些新亮点？小龙沉浸式您你揭晓：</p><p></p><p><strong>● 大咖齐聚</strong>：20+ 院士学者、企业领袖、业界翘楚，超 300 家企业和逾千位行业开发者。</p><p></p><p>●<strong> 前沿探索</strong>：1 个产业主论坛、5 个技术分论坛，深入探讨 AI 时代下操作系统产业发展的新趋势、新挑战与新路径。</p><p></p><p>● <strong>生态共荣</strong>：10+ 个特色展区/开发者活动，推动技术融合，构建更加开放、协同、繁荣的产业生态</p><p></p><p>● <strong>干货满满</strong>：60+ 场前沿技术分享，聚焦技术融合、生态共建、人才培养等议题。</p><p></p><p><img>https://static001.geekbang.org/infoq/e9/e9050dc316ea470d706b4dcb898e9e8e.jpeg<img></p><p></p><p><img>https://static001.geekbang.org/infoq/23/23b0589b80b4f2db74e809e2f1ce7b74.jpeg<img></p><p></p><p><img>https://static001.geekbang.org/infoq/dc/dc0f175c747cbd9d78bd2e920b7d6cf2.jpeg<img></p><p></p><p><img>https://static001.geekbang.org/infoq/52/5208077ab26f832866bd56c66211f721.jpeg<img></p><p>前 100 名报名且现场签到的小伙伴可在现场“龙蜥礼物岛”领取限量社区周边一份，五周年定制冰箱贴、帆布袋等可选。欢迎大家速速报名，共赴操作系统产业的年度盛会！</p><p></p><p>报名链接（复制链接至浏览器打开）：</p><p>https://hd.aliyun.com/form/6795</p><p></p><p>2025 龙蜥操作系统大会官网链接（复制链接至浏览器打开或点击阅读原文直达）：</p><p>https://openanolis.cn/openanolisconference2025</p></div>",
            "link": "https://www.infoq.cn/article/DWyEL9LsfxEV8JW5eZWU",
            "pub_date": "2025-10-31 04:07:41",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "EMmU8fChp9qiNjRCLjQZ",
            "title": "深度探讨“云+智能计算”，智算新基础设施分论坛议程揭晓 | 2025 龙蜥大会",
            "image": "https://static001.infoq.cn/resource/image/73/01/737fc57061fb38c67ac345b1c2beb501.jpg",
            "description": "<div><p><strong>2025 龙蜥操作系统大会将于 11 月 17 日在北京·星地艺术中心举办，由中国计算机学会开源发展技术委员会、泛在操作系统开放社区、中关村科技园区朝阳园管理委员会（北京市朝阳区科学技术和信息化局）、中国开源软件推进联盟指导，龙蜥社区主办，中关村互联网 3.0 产业园（星地中心）协办，阿里云、中兴通讯、海光信息、Intel、浪潮信息、Arm 等 24 家理事单位共同承办，主题为“生态共融·智驱未来”， 云集逾千位全球技术领袖、业界精英和行业开发者，深入探讨 AI 时代下操作系统产业发展的新趋势、新挑战与新路径。大会将聚焦技术融合、生态共建、人才培养等核心议题，致力于推动操作系统与AI、云计算、大数据等前沿技术的深度融合，打造政、产、学、研多方深度对话的年度盛会，构建更加开放、协同、繁荣的产业生态体系。</strong></p><p></p><p>当前，人工智能正加速演进，智算基础设施作为支撑人工智能技术发展和产业落地的底座已成为驱动技术创新与产业变革的核心引擎。本论坛以<strong>“共建智算新基础设施”</strong>为主题，汇聚企业、科研机构及产业生态各方力量，共同探讨云+智能计算的产业进展、前沿技术创新与生态建设，旨在凝聚共识、激发创新。</p><p></p><p>本论坛由龙蜥社区主办，龙蜥社区智算基础设施联盟、Intel 、中国智算产业联盟、浪潮信息、大普微企业等联合承办，欢迎来自产业、高校的智算系统领域工程师、科研人员、专家和架构师分享精彩的技术演讲与生态实践经验，共同探索和构建高效、普惠的智能算力新基础设施。</p><p></p><p><strong>时间：</strong>2025.11.17 13:30-16:35</p><p></p><p><strong>地点：</strong>北京·星地艺术中心 D 座 5F 会议室 2</p><p></p><p><strong>报名链接</strong>（复制链接至浏览器打开）：</p><p><italic>https://hd.aliyun.com/form/6795</italic></p><p></p><p>会议议程详见下方海报：</p><p></p><p><img>https://static001.geekbang.org/infoq/7d/7d9d50ea577717b348a49542b813902d.webp<img></p><p></p><p>相关分论坛议程链接：</p><p></p><p>更多 2025 龙蜥大会详情，请点击大会官网链接查看（复制链接至浏览器打开或点击阅读原文）：</p><p><italic>https://openanolis.cn/openanolisconference225</italic></p></div>",
            "link": "https://www.infoq.cn/news/EMmU8fChp9qiNjRCLjQZ",
            "pub_date": "2025-10-31 02:42:12",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "WzV23TlWBq6F7Qsi2zz4",
            "title": "AWS ALB 现已原生支持 URL 与主机头重写功能",
            "image": "https://static001.infoq.cn/resource/image/9a/1e/9a5eab18f719b8160fc42319ab45531e.jpg",
            "description": "<div><p><color>亚马逊云（AWS）近日宣布，</color><color> ALB 现已全面支持原生的 URL 与 Host Header（主机头）重写功能。有了这一新特性，用户无不再需要依赖自定义的应用逻辑，也不用部署和维护额外的第三方代理解决方案（例如 NGINX Ingress Controller）来管理 Layer 7 流量的修改。</color></p><p><color> </color></p><p><color>在这项功能推出之前，如果企业需要复杂的请求路由功能，比如在</color><color>，就必须在架构中引入额外的代理层。这些额外的组件不仅增加了维护负担，往往也会带来不必要的延迟。</color></p><p><color> </color></p><p><color>在 Reddit 上的</color><color>中，这一变化得到了开发者们的肯定。有用户评论指出，最大的亮点是：</color></p><p><color> </color></p><p><color>修改 URL 路径和主机头的功能通过正则表达式（regex）来匹配条件，在 ALB 内部有原生的支持。用户可以在</color><color>或</color><color> ALB 上进行配置，无论是通过 </color><color>、</color><color> </color><color>还是</color><color> </color><color> 都可以实现，对流向后端服务（如 EC2 实例、容器或 Lambda 函数）的流量进行精细化控制。</color></p><p><color> </color></p><p></p><p><img>https://static001.geekbang.org/infoq/6c/6c46068f88a9f9c8d0ddfd8df44d0bcb.png<img></p><p></p><p><color>图源：AWS Networking & Content Delivery </color></p><p><color> </color></p><p><color>此外，ALB 规则中新增的 Transforms（转换）部分，允许在请求到达目标组之前，对进入的请求进行修改。一个常见的使用场景就是调整路径前缀，例如将旧的 API 版本路径 </color><color> 替换为新的 </color><color>。</color></p><p><color> </color></p><p></p><p><img>https://static001.geekbang.org/infoq/79/79ade952a074c05a2dcc3a750dc50c5c.png<img></p><p></p><p><color>图源：AWS Networking & Content Delivery </color></p><p></p><p><color>AWS 的 Serverless Hero Luc van Donkersgoed 在</color><color>上评论道：</color></p><p></p><p><color>随着这一功能的上线，AWS 也正式加入了其他已提供类似 </color><color> 原生能力的主要云服务商行列。例如，谷歌云（GCP）的</color><color> 提供强大的 URL 和头修改功能，支持在请求进入后端前重写主机和路径；而微软 Azure 则在区域范围内通过 Azure Application Gateway 提供</color><color>，在全球范围则通过 </color><color>（Rule Engine）支持更复杂、基于模式的流量控制。</color></p><p></p><p><color>用户“A Snark bot from lastweekinaws”在 Bluesky 上</color><color>道：</color></p><p><color> </color></p><p><color>最后，这项功能目前已在所有 AWS 商业区、AWS GovCloud（美国）以及 AWS 中国区域全面上线。关于 AWS ALB 的更多详细信息，可在</color><color>查看。</color></p><p></p><p>原文链接：</p></div>",
            "link": "https://www.infoq.cn/article/WzV23TlWBq6F7Qsi2zz4",
            "pub_date": "2025-10-31 01:45:28",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "e7QqfyYa9GebHrSC7IYt",
            "title": "重磅！华为开源业界首个Serverless分布式计算引擎openYuanrong，单机体验编程、极致分布式运行性能",
            "image": "https://static001.infoq.cn/resource/image/5d/66/5dd649e659e79158f6ca0ea5d6ae7366.jpg",
            "description": "<div><p>作者 | openYuanrong 团队</p><p>策划 | 华卫</p><p></p><p>近期，华为开源了自研的 Serverless 分布式计算引擎 openYuanrong。openYuanrong 已经在华为 MetaERP、小艺、华为云、终端云、ICT、海思等核心产品和平台广泛使用，是华为面向分布式计算领域长期积累下来的核心竞争力产品，也是业界首个统一支持通用计算和 AI 智能计算、灵衢超节点多种分布式应用场景的开源 Serverless 分布式计算引擎。同时，工商银行参与了openYuanrong建设，在交易对手风险计量场景应用。该场景用蒙特卡洛模拟，基于该引擎实现CPU+GPU异构算力融合计算，对比Ray性能有一定提升。下阶段华为与工行围绕分布式推理、强化学习场景深化联创。</p><p></p><p>开源地址：</p><p></p><p></p><p>Serverless 计算以其单机体验编程、自动弹性扩缩、免运维、和屏蔽复杂基础设施等优点广受业界关注，但传统的 Serverless 产品（以 FaaS 为代表）大多是面向特定垂直领域定制、缺乏通用性。因此，面对复杂的应用，往往需要和现有微服务（容器）集群混合部署。如此便会造成多个基于异构软件技术栈的异构集群之间的互操作效率低下、集成和管理复杂。不仅如此，随着 AI 技术的普及，以及 AI Agent 新应用的兴起，通算和智算集群之间也需要相互集成，当前的各个集群软件之间缺乏协同，也带来开发运维复杂、集群资源割裂、跨系统数据流转开销大等严重问题。而 openYuanrong 则致力于以统一的 Serverless 技术栈支持上述各类通算和智算负载在统一资源池上的细粒度融合部署，实现各负载间的高效协同和资源复用，同时提高系统性能，提升计算资源的使用效率。</p><p></p><h2>经过华为核心场景锻造的 Serverless 分布式计算引擎</h2><p></p><ul><li><p>通用计算场景。华为 MetaERP 以 openYuanrong 为分布式底座构建业界首个 Serverless 的 ERP 系统。MetaERP 为华为公司整体提供 ERP 服务，包含上千个微服务、日处理百 TB 级数据，是目前国内最大的 ERP 系统之一。MetaERP 使用 openYuanrong 提供零代码修改方案将大量使用 Spring 开发的标准微服务直接进行 Serverless 化，和 FaaS 原生的扩展服务实现共集群部署。openYuanrong 的快照极速冷启动等方案使得大型 Java 微服务启动时延从 90s 缩短至 1.4s，满足了 Serverless 自动水平弹性，以及无请求缩容到 0 的需求。同时，MetaERP 还采用了 openYuanrong 提供的 Serverless 流处理方案，实现性能优于 Flink 的大数据处理流水线。通过 openYuanrong 提供的 Serverless 技术，MetaERP 整体提升开发效率 60%，资源成本节省 30％。</p></li></ul><p></p><p><img>https://static001.geekbang.org/wechat/images/22/2222dfc7fe575481ab2cc70caea955e8.jpeg<img></p><p></p><ul><li><p>智能计算场景。华为小艺服务广泛采用 openYuanrong 作为其云端 AI 训 - 推 - 用的 Serverless AI 基础设施。传统方案下，SFT 训练、强化学习、大模型推理、AI Agent 分别采用不同的软件技术栈，独立集群部署（如在训推场景中使用 Ray，而 Agent 实例则运行在 K8S 管理的容器中），带来了大量的开发运维成本。小艺基于 openYuanrong 在单一资源池上构建了统一的 Serverless AI 平台，同时支持了 SFT 训练、强化学习、模型推理、AI Agent 四个领域负载的融合部署和资源灵活调度，在极大简化了分布式系统的开发运维的同时提升了资源利用率。利用 openYuanrong 的分布式内核技术，小艺的强化学习训推任务调度端到端时延减半；大模型推理实例启动速度从分钟级下降到秒级（如 Llama2-70B 水平弹性时延从 571 秒缩短至 4.55 秒）。基于 openYuanrong 的异构分布式内存数据系统，分布式 KV Cache 整体性能提升 1 倍以上，同时强化学习过程中，训推转换时实例间参数同步时延也从分钟级缩短至秒级。相较于业界类似产品（如 Ray），openYuanrong 提供了更高的性能和弹性调度能力，更成熟稳定，同时也支持通算和智算负载的融合部署，构建全流程的 Serverless AI 平台。</p></li></ul><p></p><p><img>https://static001.geekbang.org/wechat/images/fa/fa82b171fc3c2cf7d40938b811d46fe4.png<img></p><p></p><h2>openYuanrong 的整体架构</h2><p></p><p>openYuanrong 从架构上看主要由三部分组成：</p><p></p><ul><li><p>多语言函数运行时：一套以函数、状态、数据对象、数据流为核心抽象的分布式编程接口和高效实现，其中“函数”类似单机 OS“进程”，可以表达任意分布式应用的运行实例，并天然支持相互间直连互调或通过分布式内存共享传递数据，无需经过网关中转或外部存储。支持 Python、Java、C++ 等主流编程语言及相互间跨语言调用，使能用户以单机体验的编程方式高效开发各类分布式应用。</p></li></ul><ul><li><p>函数系统：Serverless 大规模分布式动态调度，支持函数动态生命周期管理（函数实例可在运行过程中动态创建 / 删除、长时间运行、休眠 / 唤醒），支持包括 Java 微服务、大模型推理在内的各类应用实例秒级冷启动和快速水平弹性，支持实例自动垂直弹性和跨节点迁移，实现节点内高密部署和集群资源高效利用。</p></li></ul><ul><li><p>数据系统：近计算的异构分布式内存数据缓存，提供 Object、Stream、KV 等语义，实现函数实例间高性能数据共享及传递。通过 HBM/DDR/SSD 多级缓存，支持节点内基于共享内存的免拷贝数据访问、节点间基于 D2D/H2H/D2H/H2D 高速传输的高效分布式数据访问。</p></li></ul><p></p><p><img>https://static001.geekbang.org/wechat/images/8c/8c030ac0c51d87d5ec74e664738a0710.png<img></p><p></p><p>本质上 openYuanrong 提供了一套统一 Serverless 架构支持各类分布式计算场景，通过提供多语言函数编程接口，以单机编程体验简化分布式应用开发；通过提供分布式动态调度和数据共享等能力，实现分布式应用的高性能运行和集群的高效资源利用。</p><p></p><h2>openYuanrong 与业界同类系统的异同点</h2><p></p><p>业界同类系统有 Ray、AWS Lambda 等。和 Ray 相比，Ray 和 openYuanrong 都支持面向 Python 的“单机体验编程、分布式运行”；但和 Ray 主要面向 Python 编程语言不同，openYuanrong 采用多语言运行时和分布式内核分离的架构，能够支持更丰富的编程语言，也支持更大规模集群调度，以及极致水平 / 垂直弹性和跨节点迁移。同时，openYuanrong 也构建了比 Ray Object Store 性能更高、功能更全的分布式内存数据系统，实现高性能数据共享 / 流转。此外，openYuanrong 的分布式管理架构也支持分布式高可靠运行，消除了单点故障。和 AWS Lambda 等 FaaS 系统相比，FaaS 只适用于特定的工作负载，而 openYuanrong 是业界首个将 Serverless 从专用场景扩展到通用分布式场景的系统。</p><p></p><p>openYuanrong 已在 openEuler 社区全面开源，采用 Apache 2.0 License，期待更多优秀的开发者参与，共同定义智能时代的分布式计算新范式。</p></div>",
            "link": "https://www.infoq.cn/article/e7QqfyYa9GebHrSC7IYt",
            "pub_date": "2025-10-29 09:30:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "ZCe0w3QBY1NZPbrAja8E",
            "title": "IDC 最新AI Infra报告解读｜AI 云爆发的背后：AI 应用全球化势不可挡",
            "image": "https://static001.infoq.cn/resource/image/89/43/89450fe1ca9656ee2b00404548888943.jpg",
            "description": "<div><p><img>https://static001.infoq.cn/resource/image/ef/a8/efbc7726f884035803f978c44b350aa8.png<img></p><p>近期 IDC 发布了一份《AI 原生云 / 新型云厂商重构 Agentic 基础设施》报告，报告中的调研数据显示：87% 的亚太企业在 2024 年至少部署了 10 个 GenAI 场景，预计到 2025 年这一数字将上升到近 94%。此外，IDC 调查发现，亚太地区日常使用 GenAI 的消费者从 2024 年的 19% 增加到 2025 年的 30%，企业 GenAI 的采用率也激增，65% 的亚太区企业将有超过 50 个 GenAI 场景投入生产，预计到 2025 年，26% 的企业将拥有超过 100 个应用。IDC 预测，到 2028 年中等以上的规模企业当中，至少会有上百个智能体在运转。</p><p></p><p>在此背景下，企业需要重新思考其 AI 基础设施，是否足以应对即将到来的智能体协同时代。这场变革不仅关乎技术升级，更是一场关于商业模式、市场格局和全球战略的深度博弈。</p><p></p><p>为了帮助更多 AI 从业者、企业决策者理解这份报告的核心内容，InfoQ 特别邀请了 IDC 中国研究总监卢言霞、GMI Cloud 创始人 & CEO Alex Yeh，从数据洞察与实战视角拆解报告，解读亚太地区 AI 基础设施的新趋势。</p><p></p><p><img>https://static001.geekbang.org/infoq/26/2613c5b8ac674be323f251cb41b816db.webp<img></p><p></p><p><italic>以下为经整理的直播内容精要。</italic></p><p></p><p><img>https://static001.infoq.cn/resource/image/dc/2b/dc0ac9e3ef27ba6384c00f6ac063802b.png<img></p><p></p><p>AI 应用构建浪潮扑面而来，但技术栈深海处的「链式反应」，可能比表面热浪更具颠覆性。</p><p>IDC 指出，自 ChatGPT 2022 年发布以来，大型互联网公司以及部分初创企业聚焦在大模型训练，以期在基础模型领域占据市场领先地位。因此在 2022-2024 年间，AI 基础设施的投资更多聚焦在模型训练侧。进入 2025 年，大模型的预训练开始收敛，市场的焦点更多在于模型推理侧，AI 推理专用基础设施需求激增。</p><p></p><p>IDC 报告指出，在亚太市场， 2023 年只有 40% 的组织使用人工智能推理基础设施，但在 2025 年，这一数字增长到 84%。这一数据标志着 AI 产业正在从模型开发阶段进入大规模应用落地阶段。</p><p></p><p><strong>这种变化一方面正在模糊传统技术栈的界限，另一方面也催生出专为 AI 工作负载优化的新型云服务——AI Native Cloud.</strong></p><p></p><p><strong>什么是 AI Native Cloud（AI 原生云）？</strong>IDC 报告里定义：“需要同时满足 GPU 高密度算力、超低延迟网络、以及面向 GenAI 的编排与冷却等需求”。</p><p></p><p>卢言霞分析道：“未来企业可能有多个智能体，成千上万个智能体之间并行大规模交互，对分布式算力和模型间传输的要求已经与传统 AI 时代有很大不同。”更关键的是，从通用模型到行业定制化的转型中，模型调优、RAG 推理环节的增加催生了训推一体需求。</p><p></p><p>Alex 从技术角度分析了 AI Native Cloud 的核心技术壁垒：“<strong>首先是 GPU 集群的高效调度能力。</strong> 比如在泰国、越南、马来西亚等亚太区域间的算力调度，关键不在于硬件扩容，而是通过 K8s 等动态资源调配技术，将算力利用率稳定维持在 98% 以上。我们自研的 Cluster Engine 技术能在亚太四个节点间实时调度，甚至能利用时区差异 —— 当亚洲进入夜间时，美国客户可调用亚洲节点算力，让整体利用率持续攀升。这种调度稳定性还能避免训练任务中断，原本 10 天的训练周期可提前完成，这是高效调度能力的核心价值。</p><p></p><p><strong>其次是算力的适配能力。</strong> 不同 AI 场景的算力需求差异极大：量化训练的逻辑如同骨架般具有严格时序节点，需高算力密度支持，而视频扩散模型或图片处理可能用中低端显卡就能完成。因此，能否打造统一框架适配多元场景至关重要。我们的第二个产品 Inference Engine 正是为此设计 —— 它打通硬件适配层，让客户无需关注底层硬件，直接通过 API token 按调用量付费。不是按卡计费，而是根据文本、图像、语音等不同模态动态调配算力，用灵活的算力资源支撑‘按 token 计价’模式，这是算力适配的核心逻辑。</p><p></p><p><strong>再次是全链路的优化能力。</strong> 这正是 GMI Cloud 与传统云的差异所在：传统云仅提供‘多少张卡 + 多少存储’的资源组合，而我们会做分散式推理架构设计，效率远高于传统方案。基于 Inference Engine 的模型调度能力，我们的模型吞吐率、TTFT（首 token 生成时间）等指标均优于传统云厂商 —— 这源于从模型侧到硬件侧的深度调优，不再是单纯提供资源，而是将算力转化为直接可用的模型服务接口。”</p><p></p><p>他进一步补充了三点关键技术洞察，阐释了 AI Cloud 与传统云的根本区别：<strong>第一，架构范式转变：从虚拟化到裸金属。</strong> Alex 指出，传统云厂商受制于过去二十年的虚拟化架构，通常以虚拟机形式提供算力。然而，AI 计算，尤其是训练和低延迟推理，需要直接掌控底层硬件资源以避免虚拟化带来的性能损耗。</p><p>Alex 提到与其他云厂商合作时的体验，“但现在 AI Native 的新创公司，常常会需要 BareMetal（裸金属），因为需要控制到整个架构。” 这种对底层硬件的直接访问和控制，对于实现极致的性能优化和稳定性至关重要。</p><p></p><p><strong>第二，服务模式变革：从远程支持到陪伴式服务。</strong> AI 时代云服务的深度正在发生本质变化。“我们服务了很多训练类的客户，基本需要陪伴式服务，因为训练集群随时可能出现各种问题”，Alex 描述道。这种 “长期陪伴的服务能力” 要求云厂商的工程师团队几乎驻扎在客户现场，与客户共同调试和优化，这与传统云时代 “开个网站、基本不会坏” 的远程、标准化服务模式截然不同。GMI Cloud 为此建立专属 SLA 团队，承诺 10 分钟响应、1 小时问题诊断、2 小时系统恢复。</p><p></p><p><strong>第三，核心竞争壁垒</strong>：<strong>全球化合规与运营。</strong> 这一点在当前的国际环境下显得尤为关键。亚太地区数据法规碎片化，GPU 资源也相对抢手，这要求云厂商不仅要在技术上过硬，还必须具备在全球复杂的地缘政治和监管环境中安全、合规运营的能力。GMI Cloud 已在亚太建立多个合规节点，通过本地化集群 + 动态调度，满足不同区域的合规与延迟需求。</p><p></p><p>这三项要求共同构成了传统云厂商转型的壁垒。“很多传统云厂商或者 GPU 集群供应商很容易被过去的架构给限制住，而不能提供给客户更敏捷的产品，”Alex 总结道。而这恰恰为没有历史包袱、从一开始就围绕 AI 工作负载构建技术栈的新兴云厂商创造了巨大的市场机会，进而推动了 AI 云厂商的快速崛起。目前 GMI Cloud 正持续推进 “AI Factory” 计划，即将落地全亚洲最大的万卡液冷 GB300 集群，未来还将在东南亚、日本、中东、美国等区域布局，以支撑超大规模算力需求等。</p><p></p><p><img>https://static001.infoq.cn/resource/image/a8/ae/a8fecdc311439d80e61442fa7bd867ae.png<img></p><p></p><p>任何技术的革新，最终落地到企业的视角，除了提效，能否节省成本则是技术选型的另一考核要素。</p><p>亚太地区 AI 企业普遍采用多云策略，以规避供应商锁定、追求最佳性价比或满足数据本地化要求。然而，<strong>“算力资源分散在不同云平台、管理规则与接口五花八门” 的局面，构成了一个巨大的 “隐性成本黑洞”。</strong></p><p></p><p>卢言霞详细剖析了其中的挑战：“企业的管理成本变得非常高，这涉及到完全不同厂商的技术栈，它们的定价模式、服务水准协议（SLA）、技术支持方式都存在巨大差异。要实现这些异构技术栈的融合、保证不同平台间的兼容性，其整体的运营复杂度和成本是相当可观的。”</p><p></p><p>她进一步指出了更棘手的数据问题 —— 生成式 AI 应用往往需要从多个异构数据源读取数据。当企业设想一个核心智能体与内部成千上万的其他智能体进行并行交互时，这些数据和系统可能分布在不同的公有云、甚至私有的本地化基础设施中，其间的数据同步与协同成为了巨大的工程挑战。更关键的是，不同系统接口标准化程度低，多数定制开发系统接口不统一，进一步抬高了技术门槛。</p><p></p><p>面对这一行业痛点，Alex 阐述了 GMI Cloud 提供的 “统一算力纳管” 解决方案：</p><p></p><ul><li><p><strong>底层 GPU 硬件架构：提供高端 GPU 云与裸金属服务</strong>。通过顶级生态协作获取英伟达高端硬件资源，并为高性能和高控制权限要求的客户提供直接开放硬件层访问的裸金属方案，消除虚拟化损耗，适配泛互联网、自动驾驶等对性能与控制权要求严苛的场景。</p></li></ul><p></p><ul><li><p><strong>IaaS 层：Cluster Engine 平台。</strong> 基于 K8s 架构实现全球算力弹性调度，支持跨区域负载均衡与错峰复用，资源利用率达 98%+，并通过可视化工具实现实时监控与智能管理。</p></li></ul><p></p><ul><li><p><strong>MaaS 层：Inference Engine 推理引擎平台。</strong> 底层搭载 H200 芯片，集成 DeepSeek、Qwen 等近百个大模型，平台通过自研推理优化技术提升模型调用效率，提供统一 API 接口，支持文本 / 图像 / 视频多模态模型调用，实现 “按 token 用量付费” 的弹性服务。</p></li></ul><p></p><p>三层架构的协同形成了完整的算力价值闭环：底层硬件提供性能基础，Cluster Engine 实现资源高效流转，Inference Engine 交付即用模型能力，最终帮助企业破解算力分散、管理复杂、成本高企等核心痛点。</p><p></p><p><strong>除了多云管理带来的复杂度和成本挑战，企业在算力投入上还面临一个两难困境：“前期投入巨大，但后期利用率难以保证，导致闲置率高企”。</strong> 卢言霞观察到，这一问题在 2025 年上半年的中国市场尤为典型。“尤其是一些大型企业，之前投入了大量的一体机方案。在大模型浪潮爆发之前，中国市场对 AI 和 IT 的投入就比较重视算力基础设施的采购，但往往未能与最终的应用场景和效率紧密挂钩。”</p><p></p><p>传统 AI 时代，企业 IT 投入中硬件常作为固定资产，但技术迭代快（如几年前的芯片型号如今可能过时），加上 AI 应用未大规模落地，导致前期投入易形成浪费；而互联网企业因业务波峰波谷明显，新兴 AIGC APP 试点新功能时，也不适合过早投入硬件，否则可能因功能未留存造成资源闲置。</p><p></p><p>针对这一核心痛点，Alex 分享了 GMI Cloud 给客户的方案。“GPU 的迭代速度正在变得越来越快，从过去的 5-6 年缩短到现在的 3 年甚至更短。技术迭代的加速意味着硬件贬值的风险急剧增加。因此，我们提供了 ‘Rent versus Buy’（租用而非购买） 的服务方式。客户可以与我们签订三年的合同，以租用的方式获得顶尖的算力，并在合同结束后，可以根据需要轻松升级到最新的硬件，从而彻底避免了技术迭代带来的资产贬值风险。” 这种模式对于现金流敏感的新创 AI 应用公司尤其具有吸引力，因为它将沉重的固定资产投入转化为了灵活的运营成本。</p><p></p><p>Alex 强调:“这种深度合作模式也使得 GMI Cloud 与客户的关系从传统的 ‘供应商 — 采购方’转变为了 ‘战略伙伴、共同成长’。”实际上，当 AI 算力需求从 “标准化采购” 转向 “场景化定制”，传统云厂商 “卖算力资源” 的供应商模式也将发生改变。随着 AI 基础设施进入 “效果为王” 的深水区，云厂商的竞争力不再取决于 “有多少算力”，而在于 “能为客户的每一分算力投入创造多少商业价值”。</p><p></p><p><img>https://static001.infoq.cn/resource/image/6f/a2/6f883ce41dfdfe1b720d2745a21d39a2.png<img></p><p></p><p>GenAI 场景应用的加速，除了带来技术栈、需求、模式等变革外，也在深层次影响产业发展的风向和竞争格局。</p><p></p><p>报告数据显示，95% 的亚太企业正在同时部署训练和推理基础设施。<strong>从行业分布来看，泛互联网、制造业和具身智能成为推理设施投入增速最快的三大领域。</strong> 其中，泛互联网既包括传统大型互联网企业，也包含当下火热的 AIGC 应用，特别是中国企业出海的重点方向 ——AI 社交、内容生成等 to C 应用；制造业则涵盖高端器械、医疗器械、重工业设备等领域的出海企业，这些企业在海外建设智能制造工厂，带动了大模型和 AIGC 应用需求；具身智能领域的机器人企业，无论是新秀还是老牌厂商，在 AI 推理算力基础设施上的投入也呈指数级增长。</p><p></p><p>Alex 通过实战观察验证了这一趋势：“我们看到的最大需求来自泛互联网，接下来是制造业。这些需求可以细分为三种模态：语音、视频加图像、文本。” 他进一步解释道，语音包括语音转换、呼叫中心、陪伴应用；视频主要是电商领域，需大量图像与视频制作广告素材；文本则是 Copilot、会议摘要等工具。</p><p></p><p><strong>技术应用层面，多模态融合正成为场景爆发的核心方向。</strong>Alex 预判视频领域将迎来 “DeepSeek 时刻”，B200 相比 H100 速度提升两倍，原本生成 5 秒视频需要耗时 30 秒，未来可能缩短至 400 毫秒，实现即时生成，这将彻底改变内容生产方式。另外，电商、影片生成、短视频、动画、广告都是亚洲市场的热门领域，庞大的用户基数与场景红利，为 AI 技术提供了天然的试验场与商业化土壤。而开源与闭源格局的变化更是降低了入场门槛 ，中尾部企业无需自建大模型，通过 Finetuning 即可快速落地场景。</p><p></p><p><strong>推理需求的快速增长，也带动了 AI 基础设施市场的竞争格局重塑</strong>。传统公有云厂商与 AI Cloud/GPU Cloud 新型云厂商之间的市场份额变化呈现出明显趋势。卢言霞透露：“2024 年到 2025 年间，GPU Cloud 和新兴云厂商在整个生成式 AI 基础设施市场上可能占到 15% 左右的市场份额。不要小瞧这 15%，对基础设施这么庞大的市场来说已经是非常大的进展。”</p><p></p><p><strong>一个反常识的转变也在发生：亚太市场的算力玩家们正在从 “零和博弈” 走向 “竞合共生”。</strong>Alex 提到，不同于传统 IT 行业的 “要么我卖进、要么你卖进”，AI 赛道因算力普遍短缺，“合作潜力非常多，大家都不够用，就互相借卡、租卡”，泛互联网超大型企业、公有云甚至会与新兴 AI 云厂商合作，“他们不想持续砸钱买卡，直接向我们租，我们能在小地方快速建立集群，速度比他们更快”。</p><p></p><p>这种资源互补的模式，打破了传统市场的竞争壁垒，让算力资源流动更高效，为中国企业提供了更多合作机遇，也为中企 AI 应用出海提供了更多的支撑。</p><p></p><p>对于计划出海的中国 AI 企业，在直播最后，卢言霞给出了三点核心战略建议：</p><p></p><ul><li><p><strong>第一，建立负责任的 AI 体系</strong>，“现阶段对整个行业参与者非常重要”。随着生成式 AI 能力增强，伦理风险、内容合规等问题已引发全球监管关注，头部企业需优先构建全流程的 AI 治理框架，这不仅是准入门槛，更是长期信任的基础；</p></li></ul><p></p><ul><li><p><strong>第二，紧盯大模型能力进化</strong>，“大模型迭代快，要判断哪些能力可能由大模型直接提供，无需开发工具重复投入”。避免在通用能力上浪费资源，聚焦行业定制化的差异化价值；</p></li></ul><p></p><ul><li><p><strong>第三，重视 AI 专用基础设施建设</strong>，“传统 AI 时代企业对基础设施重视不足，如今生成式 AI 广泛部署，必须关注面向 AI 工作负载优化的基础设施”，尤其是训推一体、低延迟网络等核心能力，这是业务落地的技术基石。</p></li></ul><p></p><p>对中国企业而言，只有抓住推理市场新机遇，在性能、合规、成本间找到平衡点，才有机会在算力变革的浪潮中抢占先机，从 “AI 应用追随者” 稳步进阶为 “区域规则的共建者”，进而在全球市场竞争中筑牢优势。</p><p></p><p><img>https://static001.infoq.cn/resource/image/5c/95/5c1ea66f943e5888a5dbb8dba7085595.png<img></p><p></p><p>卢言霞与 Alex 指出，亚太 AI 基础设施的变革本质是技术与产业话语权的双重重构。随着推理基础设施渗透率稳步提升，传统云“卖资源”的模式正在失效，取而代之的是“技术栈 + 服务模式 + 全球布局”的综合实力较量。</p><p></p><p>从技术底层看，AI 原生云正通过裸金属架构、K8s 弹性调度等关键技术，将 GPU 算力利用率提升至行业领先水平，并实现训推一体的闭环优化。这一能力直接推动了 AI 应用（如智能体）从单一任务执行向复杂多场景协同的演进。以 GMI Cloud 的分布式推理架构为例，其通过 PD 分离、跨区域动态扩缩容等技术，稳定支撑了高并发实时推理，满足了多区域用户访问与智能体并行决策的需求。</p><p></p><p>对行业而言，AI 原生云的核心价值在于构建了一个高效、智能的“能力底座”。它通过将算力精准转化为业务迭代的直接生产力，助力企业驾驭高并发推理与智能体协同等复杂场景。亚太市场正在经历的，正是一场从“资源上云”到“智能用云”的深刻效能革命。</p><p></p><p><italic>点击</italic><italic>获取完整报告。</italic></p><p></p></div>",
            "link": "https://www.infoq.cn/article/ZCe0w3QBY1NZPbrAja8E",
            "pub_date": "2025-10-25 08:36:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "WhRl2cJktHCN5wrxAcvg",
            "title": "亚马逊云科技推出EC2容量管理器，实现集中式跨账户容量优化",
            "image": "https://static001.infoq.cn/resource/image/78/7a/78237cd7b0d794dddf959163da05dc7a.jpg",
            "description": "<div><p><color>最近，亚马逊云科技新推出了一个集中式解决方案——</color><color>，旨在通过单个界面监控、分析和管理客户所有账户和AWS区域内的EC2容量使用情况。</color></p><p><color> </color></p><p><color>以前，在跨多个可用区协调按需实例、竞价实例和容量预留（CR）等类型的数百个实例时，大规模运营</color><color>（Amazon EC2）的组织会面临高度复杂的管理开销。底层容量数据分散在AWS Management Console、Cost and Usage Reports(CUR)、Amazon CloudWatch和各种EC2 API中，造成了明显的运营障碍。</color></p><p><color> </color></p><p><color>为了应对这一挑战，EC2容量管理器将所有容量数据聚合到了一个统一的、跨账户、跨区域的仪表板中。此外，该服务每小时刷新一次容量信息，并且初始设置保留14天的历史数据用于即时分析。</color></p><p></p><p><img>https://static001.geekbang.org/infoq/00/003b246124752afb25da42b37588a9df.png<img></p><p></p><p><color>（图片来源：</color><color>）</color></p><p><color> </color></p><p><color>亚马逊云科技安全英雄Sena Yakut</color><color>了这种整合的迫切需求：</color></p><p><color> </color></p><p><color> </color></p><p><color>主仪表板提供了一个综合视图，上面显示了所有实例类型的利用率，提供了基于vCPU、实例计数或估计成本（使用按需费率计算）的指标。其核心功能包括：</color></p><p><color> </color></p><ul><li><p><span style=\"color:#494949\">预留指标：可视化跟踪已使用与未使用预留容量的比例，可直接反映预留效率。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">竞价分析：竞价标签聚焦使用模式，显示关键指标，如竞价实例在被中断前的平均运行时长。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">直接管理：对于操作人员来说，一个特性是能够直接从容量管理器界面修改按需容量预留（ODCR），当预留资源位于同一个账户下时，这可以减少上下文切换并简化响应性更改。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">数据导出和集成：容量管理器支持将数据导出到Amazon S3，这使得组织可以保留超过标准保留期限（90天）的容量数据，用于长期趋势分析以及与外部商业智能（BI）工具集成。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">AWS Organizations集成：设置部分原生支持集中化的企业范围的容量可见性，以及跨多个账户的委托访问控制，从而简化治理流程。</span></p></li></ul><p></p><p><img>https://static001.geekbang.org/infoq/86/86ea5eacafd71e6dd59b3a7c3675f7f8.png<img></p><p></p><p><color>（图片来源：</color><color>）</color></p><p><color> </color></p><p><color>该公告在开发者和财务运营社区中引发了褒贬不一的反应。许多人对此表示欢迎，认为这减轻了运营压力，尤其是对成本管理专业人员而言。在</color><color>中，一位参与者评论道：</color></p><p><color> </color></p><p><color> </color></p><p><color>在LinkedIn上，首席云架构师Ivo Pinto</color><color>了安全和访问控制方面的好处：</color></p><p><color> </color></p><p><color> </color></p><p><color>不过，也有人对这一功能的长期云价值主张表示了怀疑。在同一个Reddit讨论帖中，另一位评论者</color><color>，集中式工具并不能解决云弹性成本的核心问题：</color></p><p><color> </color></p><p><color> </color></p><p><color>在推特上，首席云工程师Jack Hendy</color><color>：</color></p><p><color> </color></p><p><color> </color></p><p><color>最后，Amazon EC2容量管理器在所有商业AWS区域中默认启用，并且不额外收取任何费用。</color></p><p><color> </color></p><p><color> </color></p><p><color>声明：本文为InfoQ翻译，未经许可禁止转载。</color></p><p><color> </color></p><p><color>原文链接：</color></p></div>",
            "link": "https://www.infoq.cn/article/WhRl2cJktHCN5wrxAcvg",
            "pub_date": "2025-10-29 04:00:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "FDc37HIdrfyWGv3UqXVA",
            "title": "黄仁勋凌晨炸场：6G、量子计算、物理AI、机器人、自动驾驶全来了！AI芯片订单已达3.5万亿｜2025GTC最全指南",
            "image": "https://static001.infoq.cn/resource/image/96/e6/96e4789d23f9486d3b5de16a4e4101e6.jpg",
            "description": "<div><p>本周可谓<strong>美国科技行业的“超级周”</strong>，而号称“AI风向标”的<strong>英伟达GTC</strong>（GPU Technology Conference，GPU技术大会）也于美东时间10月27日<strong>开幕</strong>。</p><p></p><p>10月28日，黄仁勋依旧身着皮衣登场。但与以往有明确重点的发布会不同，老黄此次演讲<strong>几乎把全球科技圈的热词悉数点了一遍：6G、量子计算、物理AI、机器人、自动驾驶、核聚变...</strong>一个没落下。</p><p></p><p>他还官宣了一个<strong>跨次元般的</strong>重磅消息：</p><p></p><p>英伟达和<strong>诺基亚</strong>合作了，用AI提高无线通信的速度，共同建造面向AI原生的移动网络，搭建6G AI平台；为此，英伟达推出新品<strong>NVIDIA Arc（Aerial Radio Network Computer）</strong>，还将对诺基亚投资10亿美元（约合人民71亿元）。</p><p></p><p><img>https://static001.geekbang.org/infoq/96/962bde9f6657f45727124ebfc87d63c3.png<img></p><p></p><p>也就是说，AI不再只是网络的使用者，还成为了网络本身的“智能中枢”。</p><p></p><p>黄仁勋直言：“我们将采用这项新技术，升级全球数百万个基站。”</p><p></p><p>值得一提的是，老黄这次（可能下次血本了）不仅演讲话题涉猎甚广，还组建了一支<strong>超级“嘉宾足球队”</strong>，在一众大佬中，具身智能当红新星<strong>Figure AI创始人兼CEO Brett Adcock</strong>（第二行左四）也赫然在列。</p><p></p><p></p><p>话说回来，<strong>英伟达AI芯片</strong>的成绩也着实亮眼：Blackwell和Rubin芯片订单总额，已达<strong>5000亿美元</strong>（约合人民币35,000亿元）。</p><p></p><p>在今年3月的春季GTC大会上，老黄曾秀出<strong>最强AI芯片GB300 NVL72</strong>，其中<strong>“G”</strong>是该芯片中的CPU架构<strong>Grace</strong>，<strong>“B”</strong>是GPU架构<strong>Blackwell</strong>，也是英伟达现在主推的数据中心GPU架构。</p><p></p><p>截至当日收盘，<strong>英伟达</strong>股价涨约5%，<strong>总市值逼近5万亿美元（约合人民币35万亿元）</strong>，创下新高。</p><p><img>https://static001.geekbang.org/infoq/b3/b3c03b0c2ab8ac8383c78cbafb996e04.png<img></p><p></p><p></p><h2>次元壁破了：和老牌手机霸主一起搞通信</h2><p>在这次GTC上，老黄官宣的第一个合作协议，就是和诺基亚携手共建6G AI平台。</p><p></p><p>二者此次<strong>合作重点</strong>不在“造芯片”，而在<strong>“让网络学会思考”</strong>：英伟达把其加速计算平台<strong>Aerial RAN Computer Pro（ARC-Pro）</strong>，带进诺基亚的无线通信系统<strong>AirScale</strong>中，推动运营商向AI原生的5G与6G网络过渡。</p><p></p><p>话说，<strong>诺基亚</strong>这个零几年的全球手机霸主、听起来好像是个“上古战神”，现在<strong>为什么会出现在英伟达的合作名单上，而且还被放在超级显眼位置？</strong></p><p></p><p><strong>首先，其实诺基亚“没死”</strong>，它只是卖掉了手机业务，<strong>退回了“根部”的通信设备业务</strong>。</p><p></p><p>2013年，在卖掉手机部门给微软之后，诺基亚彻底转向<strong>电信基础设施</strong>：基站、天线、光纤网络、核心网软件——全球运营商的底层网络都离不开这些。</p><p></p><p>2016年，诺基亚收购了阿尔卡特朗讯——这是由传奇的<strong>“贝尔实验室”</strong>（曾获9项诺贝尔奖、4项图灵奖）改组重建的公司，然后就自然而然地继承了贝尔实验室一堆领先的技术和专利；收购后，贝尔实验室也更名为Nokia Bell Labs。</p><p></p><p>如今的诺基亚和华为、爱立信并列，是全球三大通信设备厂商之一。</p><p></p><p><strong>第二，英伟达做芯片的终极目标</strong>，其实不是“更强的GPU”，而是<strong>让所有计算都发生在“靠近数据产生的地方”——也就是网络边缘，</strong>而这正是诺基亚的主场。</p><p></p><p>英伟达提供 ARC-Pro平台：让通信基站不仅传信号，还能执行 AI 推理；诺基亚则提供AirScale 无线系统和RAN软件栈：让这些“AI 基站”融入现有的5G网络，并能平滑升级到 6G。另外，T-Mobile是他们的首个运营商合作方，将在 2026年开始实地测试。</p><p></p><p>简单来说，就是英伟达把GPU算力塞进基站，诺基亚负责让它能在真实的网络中跑起来。</p><p></p><p>正如黄仁勋在发布会上所说的：“基于NVIDIA CUDA和AI的AI-RAN，将彻底改变电信行业，这是一次跨时代的平台变革。”</p><p></p><p>英伟达的官方新闻稿也指出，此次合作标志着行业的转折点，通过在全球范围内推动AI-RAN的创新和商业化，为AI 原生6G铺平了道路。</p><p></p><p>在诺基亚之外，黄仁勋还一口气官宣了几家合作伙伴：从自动驾驶巨头Uber、政府AI供应商Palantir到与美国能源部、甲骨文达成战略合作——几乎覆盖了AI产业的每一个关键环节，暗含其野心。</p><p></p><p>先是Uber。黄仁勋认为“机器人出租车的拐点即将到来”，双方计划在全球铺开10万辆自动驾驶汽车。这不仅是自动驾驶的突破，更是AI硬件与智能算法的深度融合——英伟达正试图让GPU成为Robotaxi时代的“车载大脑”，推动出行生态进入商业化阶段。</p><p></p><p>接着是Palantir。这家擅长处理政府和军情数据的公司，将把英伟达的CUDA-X和Nemotron模型嵌入自家系统，让AI学会“看懂”世界。Lowe’s已经在用这套组合调度它的供应链——AI代理成了企业的隐形决策官。</p><p></p><p>不过更大胆的布局在科研领域，与美国能源部、甲骨文联手打造七台AI超级计算机。其中阿贡实验室的Solstice与Equinox系统，将以2,200 exaFLOP的惊人算力，成为“美国的探索引擎”。</p><p></p><p>从Robotaxi到政府级AI决策，再到超级计算，英伟达不再“造芯片”，而是在“造智能的地基”。</p><p></p><h2>前沿科技话题“大点兵”</h2><p></p><p>前文提到，老黄这次的演讲涉及多个话题，除了6G，还有<strong>量子计算、物理AI、机器人、自动驾驶等</strong>；下面来具体看几个。</p><h2>1、量子计算</h2><p>自理查德·费曼提出量子计算概念起，40年后，业界终于在去年实现了关键突破——创造出可相干、稳定、且具纠错能力的<strong>逻辑量子比特（logical qubit）</strong>。</p><p></p><p>英伟达此前推出了开放式量子GPU计算平台<strong>CUDA-Q</strong>，这次又开发了一个基于CUDA-Q核心构建的<strong>NVQLink，</strong>是一种能把传统GPU和量子处理器连接起来的互联架构。</p><p></p><p>当下的量子计算仍处在“易碎”阶段——对环境噪声异常敏感，且算力利用率有限。为了让量子比特保持稳定运行，往往要借助 GPU 超算系统承担控制与纠错计算，这让量子计算暂时还离不开经典计算的“辅助臂”。</p><p>黄仁勋分享称，英伟达将与美国能源部合作建设<strong>7台AI超级计算机</strong>，这些超算将使用Blackwell和下一代Vera Rubin架构芯片，利用 AI、量子计算等最前技术投入研究。</p><p></p><p>老黄还化身“AI赛道的美国队长”，举起了他的“芯片盾牌”：由NVLink连接的72块GPU构成。</p><p></p><p><img>https://static001.geekbang.org/infoq/ba/bab53b874868d7722e6f77cea2a837ca.png<img></p><p></p><h2>2、具身智能和物理AI（Physical AI）</h2><p>对于具身智能与机器人计算，英伟达的理念是，如果要让AI真正进入物理世界、具备感知和行动能力，必须依托一个“三计算机”体系：</p><p></p><p>一是用于模型训练的<strong>Grace Blackwell AI 计算机</strong>，负责生成大规模智能模型；二是用于仿真和虚拟验证的 <strong>Omniverse 数字孪生计算机</strong>，在虚拟环境中模拟机器人行为与物理交互；三是用于实际执行的 <strong>Jetson Thor 机器人计算机</strong>，让智能在真实世界中运行。</p><p></p><p>这三者都基于CUDA平台运行，形成从训练、仿真到执行的完整“物理智能”闭环，使AI能够真正连接虚拟与现实世界。</p><p></p><p>英伟达投资过多家具体智能公司，和美国具体智能新独角兽Figure AI也在开展合作，加速下一代机器人的研发。</p><p></p><p><img>https://static001.geekbang.org/infoq/3e/3ea8fd1b7156268885bc9b96f0803704.png<img></p><p></p><p></p><h2>3、开源模型与生态合作</h2><p>英伟达在开源模型和产业生态上正展开双线布局。</p><p></p><p>一方面，得益于推理、多模态与知识蒸馏等能力的提升，开源模型已经足够强大，成为初创企业和科研机构进行灵活定制与创新的基础。英伟达作为开源社区的重要贡献者，已有23个模型登上各类性能榜单，并承诺将持续投入。</p><p></p><p>另一方面，英伟达正加速与云计算和行业伙伴的深度集成：其模型与库已嵌入AWS、Google Cloud、Microsoft Azure等主流云平台，以及ServiceNow、SAP等 SaaS系统，使用户能够在不同生态中无缝调用AI能力。</p><p></p><p>同时，英伟达还与 CrowdStrike（网络安全）、Palantir（数据处理）和 Synopsys（芯片设计）等行业巨头合作，以 AI 提升垂直领域生产力，推动从安全到设计的智能化变革。</p><p></p><h2>4、AI在聊天机器人之外的更多应用</h2><p>AI 被广泛用于基础科学研究，远不止聊天或生成内容。在医疗、基因组学、企业计算等领域都有应用。不同类型的模型（卷积神经网络 CNN、图神经网络 GNN、状态空间模型等）被用于不同任务。</p><p></p><p>AI 不仅是工具，也能成为“数字员工”。例如英伟达内部的 <strong>Cursor</strong> 系统可帮助工程师自动生成代码；AI 驾驶员（AI Chauffeur）则被用于自动驾驶出租车。</p><p></p><h2>老黄还亲自详解：AI到底是什么？</h2><p>在GTC大会上，除了抛出一个又一个爆炸性的技术新动向，黄仁勋还特意拿出20分钟，讲了一堂“AI是什么”的深度课。</p><p></p><p>在他看来，这件事十分必要——如果没弄清AI的定义，就无法判断下一场产业潮水将流向何处。</p><p></p><p>黄仁勋首先澄清一个误区：<strong>AI的世界</strong>远不止ChatGPT所代表的聊天机器人，那只是大众心中的AI形象。<strong>真正关键的，是以AGI为代表的深层计算机科学，以及支撑它的惊人算力</strong>，“AI不是某个应用，而是一种新的计算方式。”</p><p></p><p>过去的计算世界，是程序员写规则、计算机执行命令；现在，机器靠数据自己学习规律。</p><p></p><p>比如，以前要教电脑识别猫，你得写几十条规则；如今只需给它十万张猫的照片，它自己就能学会什么是“猫”。</p><p></p><p>这场转变包含三层逻辑：计算方式变了——从写代码变成喂数据；计算工具变了——从CPU到GPU；计算目标也变了——从执行任务到生成智能。AI由此彻底重构了计算栈。</p><p></p><p>换句话说，AI正在从“螺丝刀”变成“工人”。</p><p></p><p>它不再是被动的工具，而是主动的执行者——会使用浏览器、写代码、制定计划、理解需求。</p><p></p><p>当技术第一次具备了“做事”的能力，机器也第一次进入了生产力的核心。</p><p></p><p>有了学习能力，AI看世界的方式也彻底不同。</p><p>在它的眼中，万物都被拆解成可学习的片段：文字、图片、声音、分子、蛋白质……这些最小的信息单位叫作token。</p><p>AI通过这些“语言颗粒”去理解、模仿、重建世界。谁能以更低成本、更高速度生成和操控token，谁就能主导下一代计算。</p><p></p><p>这也催生了一种全新的基础设施——AI工厂。</p><p></p><p>如果传统数据中心像多功能仓库，负责存储文件、运行程序，那么AI工厂就像一条生产线，只做一件事：生产token。</p><p></p><p>能源流入，驱动GPU；GPU通过NVLink和Spectrum-X网络连接成超级系统；软件与模型协同工作，批量产出token。</p><p></p><p><strong>黄仁勋把这条链概括成一句话：</strong></p><p><strong>“能源 → GPU → 算法 → 模型 → Token → 智能。”</strong></p><p></p><p></p><p><img>https://static001.geekbang.org/infoq/f4/f4c48f7bceaa07695874eaefc8b24e0a.png<img></p><p></p><p></p><p>AI的成长轨迹，也像一个“吃电的天才”。</p><p></p><p><strong>它先在预训练阶段“学语言、记知识”，像学前班；</strong></p><p><strong>后在后训练阶段“学技能、学推理”；</strong></p><p><strong>最后进入“思考阶段”，在与你互动时实时规划、推理——那是最消耗算力的时刻。</strong></p><p></p><p>AI越聪明，就越费电。这是第一条指数——AI使用指数：智能与算力的爆炸式增长。</p><p></p><p>但还有第二条曲线——AI采用指数：AI越好用，越多人使用；使用越多，算力需求又被推高。</p><p></p><p>两条指数叠加，形成了一个强劲的正反馈——AI越聪明，体验越好；越好用，用户越多；越多的使用带来更多利润，又反过来推动AI变得更聪明。</p><p></p><p>面对算力需求如此巨大，要让这台“永动机”持续转动，黄仁勋的答案是“极致协同设计（Extreme Co-Design）”。</p><p></p><p>摩尔定律的线性提升早已赶不上AI的指数爆炸，必须在芯片、封装、互联、系统、编译器、模型、算法、应用等每一层同时创新。</p><p></p><p>他说：“要让AI继续前进，不能只造更快的芯片，而要重新设计整座工厂。”</p><p></p><p>这座“工厂”，就是未来的AI工厂。</p><p></p><p>在黄仁勋眼中，它不再是通用数据中心，而是一条智能的生产线——能源是燃料，GPU是引擎，模型是模具，Token是产品。</p><p></p><p>算力不再是辅助资源，而是新的生产资料。只有不断压低成本、扩大产能，AI的良性循环才能继续。</p><p></p><p>这就是未来计算的形态。</p><p></p><p>AI工厂将成为现代经济的新基础设施，从科学、医疗、制造到娱乐，所有行业都将围绕AI工厂重构。</p><p></p><p>从Arvin在Perplexity的工作、软件开发中的Cursor，到机器人出租车中的AI司机。人工智能正快速渗透到过去难以触及的经济领域，占据越来越广阔的版图。</p><p></p><p>那20分钟里，他讲的其实不只是AI，而是人类第一次拥有一种能把能量直接转化为智能的机器。</p><p></p><p>未来的计算，不再是让电脑执行命令，而是让世界自己学会思考。</p><p></p><p>老黄演讲视频回看地址：</p><p>https://www.youtube.com/watch?v=lQHK61IDFH4&list=TLGG23pf8VjteXoyODEwMjAyNQ</p><p>参考链接：</p><p>https://nvidianews.nvidia.com/news/nvidia-nokia-ai-telecommunications</p><p>https://www.reuters.com/world/asia-pacific/nvidias-huang-speak-washington-investors-look-hints-china-2025-10-28/</p></div>",
            "link": "https://www.infoq.cn/article/FDc37HIdrfyWGv3UqXVA",
            "pub_date": "2025-10-29 00:57:13",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        },
        {
            "id": "UF3guSh5REtRDy36ddwL",
            "title": "自带密钥（BYOK）：亚马逊云科技采用CMK以满足企业合规性的需要",
            "image": "https://static001.infoq.cn/resource/image/73/52/737c7e440457648e49b4ba2aa39e4552.jpeg",
            "description": "<div><p><color>亚马逊云科技最近宣布，其IAM身份中心（IAM Identity Center）服务支持</color><color>以用于静态加密。组织可以使用自己的密钥来加密身份中心的身份数据。</color></p><p><color> </color></p><p><color>IAM身份中心是一项云服务，它集中管理对多个AWS账户和云应用程序的单点登录（SSO）访问。虽然身份中心的数据一直使用AWS持有的KMS密钥进行静态加密，但新的CMK支持允许组织使用自己的密钥来加密他们的员工身份数据，例如用户和组属性。</color></p><p><color> </color></p><p><color>与</color><color>的集成至关重要，因为它将加密密钥的生命周期（创建、轮换和删除）的控制权直接转移到客户手中。</color></p><p><color> </color></p><p></p><p><img>https://static001.geekbang.org/infoq/c9/c90db2616af255f4c3d84b4ae27f41b6.png<img></p><p></p><p><italic>(图片来源：</italic><italic>)</italic></p><p><color> </color></p><p><color>亚马逊云科技的IAM身份中心的高级产品经理Alex Milanovic在LinkedIn的</color><color>中总结了它的核心优势：</color></p><ul><li><p><span style=\"color:#494949\">完全控制他们自己的加密密钥。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">通过KMS和IAM策略对身份数据进行细粒度访问管理，确保只有授权主体才能访问他们的加密数据。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">通过详细的AWS CloudTrail密钥使用日志增强审计能力。</span></p></li></ul><ul><li><p><span style=\"color:#494949\">对于需要数据主权的受监管行业，这能够加强合规性。</span></p></li></ul><p><color> </color></p><p><color>亚马逊云科技的开发布道师Sébastien Stormacq进一步详细说明了它所支持的控制级别：</color></p><p><color> </color></p><p><color>基于审计和监管的需求，整个过程会通过</color><color>进行记录，提供密钥使用的详细记录。这种对加密密钥的细粒度控制通常是在高度受监管行业运营的企业的一个先决条件。</color></p><p><color> </color></p><p><color>由于合规性或安全策略，使用CMK对静态数据进行加密是企业的标准要求，如</color><color>。其他超大规模的提供商和产品也通过它们各自的密钥管理服务广泛支持它。</color></p><p><color> </color></p><p><color>微软Azure通过</color><color>实现了这一点，使客户能够在各种服务中加密敏感数据，并通过</color><color>验证访问。同样，谷歌云通过</color><color>提供CMK，为Cloud Storage和BigQuery等服务提供加密边界和完整的密钥生命周期控制。</color></p><p><color> </color></p><p><color>身份中心服务支持单区域和多区域密钥，以满足用户的部署需求。但是，目前，身份中心实例只能在单个区域部署。尽管如此，公司建议使用多区域AWS KMS密钥，除非公司政策要求用户使用单区域密钥。它指出，多区域密钥会在区域间提供一致的密钥材料，同时在每个区域保持独立的密钥基础设施。</color></p><p><color> </color></p><p><color>最后，该功能目前在所有AWS商业区域、AWS GovCloud（美国）和AWS中国区域均可用。此外，在定价方面，用户需要为身份IAM中心付费，对于标准AWS KMS，密钥存储和API使用会产生费用。</color></p><p><color> </color></p><p><color>查看英文原文：</color></p></div>",
            "link": "https://www.infoq.cn/article/UF3guSh5REtRDy36ddwL",
            "pub_date": "2025-10-27 05:00:00",
            "source": "infoq",
            "kind": 1,
            "language": "zh-CN"
        }
    ]
}