{
    "data": [
        {
            "title": "Claude collaboration tools left the door wide open to remote code execution",
            "description": "<div id=\"body\">\n<p>Security vulnerabilities in Claude Code could have allowed attackers to remotely execute code on users' machines and steal API keys by injecting malicious configurations into repositories, and then waiting for a developer to clone and open an untrustworthy project.</p>\n<p>Check Point Software researchers found and reported all three flaws to Anthropic, which issued fixes for all and CVEs for two. Still, the bug hunters say, the issues illustrate a worrisome supply chain threat as enterprises incorporate <a href=\"https://www.theregister.com/2026/02/23/claude_code_security_panic/\" target=\"_blank\">AI coding tools like Claude</a> into their development processes and essentially turn configuration files into a new attack surface.</p>\n<p>\"The ability to execute arbitrary commands through repository-controlled configuration files created severe supply chain risks, where a single malicious commit could compromise any developer working with the affected repository,\" Check Point researchers Aviv Donenfeld and Oded Vanunu <a href=\"https://research.checkpoint.com/2026/rce-and-api-token-exfiltration-through-claude-code-project-files-cve-2025-59536/\" rel=\"nofollow\" target=\"_blank\">said</a> in a Wednesday report.</p>\n\n<p>Anthropic, the AI company that developed Claude Code, did not respond to <em>The Register</em>'s requests for comment.</p>\n\n\n<p>The three security vulnerabilities stem from Claude's design, which is intended to make it easier for development teams to collaborate. The AI coding tool enables this by embedding project-level configuration files (.claude/settings.json file) directly within repositories, so that when a developer clones a project, they automatically apply the same settings used by their teammates.</p>\n<p>Any contributor with commit access can modify these files. The researchers found that cloning and opening a malicious repository sometimes allowed them to bypass built-in safeguards and trigger hidden commands and execute malicious code.</p>\n<h3 class=\"crosshead\">Abusing Hooks for RCE</h3>\n<p>The first of the three flaws involved abusing Claude's Hooks feature to achieve remote code execution. Hooks are user-defined shell commands that execute at various points in the tool's lifecycle, ensuring that specific, predefined actions run when predetermined conditions are met, instead of allowing the model to choose.</p>\n<p>Because Hooks are defined in .claude/settings.json, the repository-controlled configuration file, anyone with commit access can define hooks that will execute shell commands on every other collaborator's machine when they work on the project. Plus, Claude doesn't require any explicit approval before executing these commands – so the researchers abused this mechanism to <a href=\"https://youtu.be/BJjkYZwMfG0\" rel=\"nofollow\">open a calculator app</a> when someone opened the project.</p>\n\n<p>While a bash script to open a calculator is hardly malicious, it's still remote code execution. And as the team <a href=\"https://youtu.be/BJjkYZwMfG0\" rel=\"nofollow\">demonstrated in a video</a>: \"An attacker could configure the hook to execute any shell command – such as downloading and running a malicious payload\" like a reverse shell.</p>\n<p>Check Point reported the malicious hooks flaw to Anthropic on July 21, 2025, and the AI maker implemented the final fix about a month later, publishing this <a href=\"https://github.com/advisories/GHSA-ph6w-f82w-28w6\" rel=\"nofollow\" target=\"_blank\">GitHub Security Advisory GHSA-ph6w-f82w-28w6</a> on August 29.</p>\n<h3 class=\"crosshead\">MCP consent bypass bug</h3>\n<p>The second vulnerability also allows RCE – this time by abusing MCP consent bypass.</p>\n<p>Claude integrates with external tools using Model Context Protocol (MCP), and MCP servers can also be configured in the same repository via .mcp.json configuration file. Thanks to the earlier disclosure and Anthropic's fix, the researchers ran into warning prompts explicitly requiring user approval before executing commands in .mcp.json.</p>\n<p>So they found a workaround: two repository-controlled configuration settings that could override safeguards and automatically approve all MCP servers.</p>\n\n<p>\"Starting Claude Code with this configuration revealed a severe vulnerability: our command executed immediately upon running Claude – before the user could even read the trust dialog,\" the Check Point duo wrote.</p>\n<p>Again, they stuck with the calculator app, but also produced a <a href=\"https://youtu.be/RlmEcN7csDI\" rel=\"nofollow\">video demonstrating</a> how this vulnerability can be exploited to remotely execute a reverse shell and completely compromise a victim's machine.</p>\n<p>The researchers reported this second vulnerability to Anthropic on September 3, 2025, Anthropic fixed the bypass vulnerability later that month, and published <a href=\"https://nvd.nist.gov/vuln/detail/CVE-2025-59536\" rel=\"nofollow\" target=\"_blank\">CVE-2025-59536</a> on October 3.</p>\n<h3 class=\"crosshead\">API key theft</h3>\n<p>Attackers can exploit the third flaw for API key theft. This one has to do with how Claude used an API key to communicate with Anthropic's services. One variable, ANTHROPIC_BASE_URL, controlled the endpoint for all Claude API communications, and while it's supposed to point to Anthropic's servers, it can be overridden in the project's configuration files to instead point to attacker-controlled servers.</p>\n<p>The researchers configured ANTHROPIC_BASE_URL to route through their local proxy, and watched all Claude Code's API traffic in real time. Every one of Claude's calls to Anthropic servers \"included the authorization header – our full Anthropic API key, completely exposed in plaintext,\" they wrote.</p>\n<p>An attacker could abuse this trick to redirect traffic and steal a developer's active API key. It's important because the API includes a feature called Workspaces to help developers manage multiple Claude deployments by allowing multiple API keys to share access to the same cloud-based project files. Files are connected to the workspace – not the single key – and any API key belonging to the workspace also has visibility into any of the workspace's stored files.</p>\n<ul class=\"listinks\">\n<li><a href=\"https://www.theregister.com/2026/02/23/claude_code_security_panic/\">Infosec community panics as Anthropic rolls out Claude code security checker</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/20/anthropic_clarifies_ban_third_party_claude_access/\">Anthropic: No, absolutely not, you may not use third-party harnesses with Claude subs</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/24/ai_finding_bugs/\">AI has gotten good at finding bugs, not so good at swatting them</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/16/anthropic_claude_ai_edits/\">Anthropic tries to hide Claude's AI actions. Devs hate it</a></li>\n</ul>\n<p>This gave the researchers the ability to upload files to the shared workspace – but did not allow downloads. According to Claude's documentation, users can only download files created by <a href=\"https://platform.claude.com/docs/en/build-with-claude/skills-guide\" rel=\"nofollow\" target=\"_blank\">skills</a> or the <a href=\"https://platform.claude.com/docs/en/agents-and-tools/tool-use/code-execution-tool\" rel=\"nofollow\" target=\"_blank\">code execution tool</a>.</p>\n<p>\"Since files generated by Claude's code execution tool are marked as downloadable, we explored whether the attacker could simply ask Claude to regenerate an existing file using the stolen API key,\" Check Point’s Donenfeld and Vanunu wrote. \"If successful, this would convert a non-downloadable file into a workspace artifact that is eligible for download.\"</p>\n<p>Cloning and then downloading the file worked, and thus confirmed that a miscreant using a stolen API key could <a href=\"https://youtu.be/jMeeVxqU3hY\" rel=\"nofollow\" target=\"_blank\">gain complete read and write access</a> to all workspace files: deleting or changing sensitive files or even uploading malicious files to poison the workspace or exceed the 100 GB storage space quota.</p>\n<p>Check Point reported the API key extraction bug to Anthropic on October 28, 2025, and the vendor immediately issued a fix. Later, on January 21, Anthropic published <a href=\"https://nvd.nist.gov/vuln/detail/CVE-2026-21852\" rel=\"nofollow\" target=\"_blank\">CVE-2026-21852</a>.</p>\n<p>As the security team noted: \"The integration of AI into development workflows brings tremendous productivity benefits, but also introduces new attack surfaces that weren't present in traditional tools.\" ®</p> \n</div>",
            "link": "https://www.theregister.com/2026/02/26/clade_code_cves/?td=rt-3a",
            "pub_date": "2026-02-26 23:41:01",
            "source": "theregister",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Microsoft to auto-launch Copilot in Edge whenever you click a link from Outlook",
            "description": "<div id=\"body\">\n<p>Microsoft has announced that its Edge browser will automatically open the Copilot side pane when users open links from Outlook.</p>\n<p>The feature appeared on the <a href=\"https://www.microsoft.com/en-gb/microsoft-365/roadmap?id=557561\" rel=\"nofollow\" target=\"_blank\">Microsoft 365 roadmap</a> on February 25, with a rollout due to start in May 2026. According to Microsoft, the update will \"provide contextual insights and actionable suggestion chips [<em>sic</em>] based on email and destination content.\"</p>\n\n<p>It added: \"This experience helps users quickly understand content, take action with fewer steps, and get more value from Copilot while extending productive browsing time in Edge.\"</p>\n<p>The update is consistent with Microsoft's current Copilot-everywhere strategy and will roll out worldwide to standard multi-tenant cloud instances.</p>\n<p>Whether it will be opt-in or opt-out remains unconfirmed, though users hoping for a conveniently placed off switch may be disappointed. Microsoft is keen to put its AI assistant in front of as many users as possible.</p>\n\n<p><i>The Register</i> asked Microsoft how much control administrators would have over the feature and what would happen if Edge wasn't the default browser. The company has yet to respond.</p>\n<ul class=\"listinks\">\n<li><a href=\"https://www.theregister.com/2026/02/24/west_midlands_police_copilot/\">West Midlands Police earn red card over Copilot's imaginary football match</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/19/ai_climate_crisis_claims/\">Don't believe the hyperscalers! AI can't cure the climate crisis</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/18/microsoft_copilot_data_loss_prevention/\">Copilot spills the beans, summarizing emails it's not supposed to read</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/15/if_microsoft_made_a_car/\">If Microsoft made a car... what would it be?</a></li>\n</ul>\n<p>Finding a corner of Microsoft's software that Copilot hasn't reached is increasingly difficult – even Notepad has not escaped – and disabling it across the company's productivity suite has become a game of Whac-A-Mole for enterprise administrators who have yet to embrace the technology.</p>\n<p>The automatic pane could hand those administrators yet another mallet to swing, particularly given that Copilot surfacing suggestions based on email content could run afoul of data security policies. That said, enterprises already nervous about where their data ends up likely have Copilot policies well in hand.</p>\n\n<p>Jon von Tetzchner, CEO of the Vivaldi browser project, which is already surfing the wave of anti-AI sentiment, isn't too impressed with Microsoft's latest efforts pertaining to Copilot and Edge.</p>\n<p>\"This is another example of trying to push Edge in every way possible and also forcing Copilot on users that may not want it,\" he told <em>The Register</em>.</p>\n\n<p>\"Considering how sensitive corporate emails can be, the last thing you want is them being snooped on by an LLM hosted who knows where. This would be highly problematic from a corporate security and privacy point of view, and even more of a problem for private users who might be using one of MS's email services. Just imagine if someone sends an email exploiting that for phishing purposes,\" he added.</p>\n<p>\"Should this be an opt-in rather than an opt-out? Absolutely. The better question is whether it should be a thing at all.\" ®</p> \n</div>",
            "link": "https://www.theregister.com/2026/02/26/copilot_pane_edge_outlook/?td=rt-3a",
            "pub_date": "2026-02-27 06:41:02",
            "source": "theregister",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "LLMs killed the privacy star, we can't rewind, we've gone too far",
            "description": "<div id=\"body\">\n<p>Add privacy to the list of potential casualties caused by the proliferation of AI, because researchers have found that large language models (LLMs) can be used to deanonymize internet users – even those who use pseudonyms – more efficiently than human sleuths.</p>\n<p>Much of the academic work on online privacy over the past 25 years builds upon <a href=\"https://latanyasweeney.org\" rel=\"nofollow\">Latanya Sweeney</a>'s 2002 research on <a href=\"https://epic.org/wp-content/uploads/privacy/reidentification/Sweeney_Article.pdf\" rel=\"nofollow\">k-Anonymity</a> [PDF], and <a href=\"https://kilthub.cmu.edu/articles/journal_contribution/Simple_Demographics_Often_Identify_People_Uniquely/6625769?file=12123218\" rel=\"nofollow\">prior research</a> in which she demonstrated it is possible to identify 87 percent of the US population using three anonymous data points – a five-digit ZIP code, gender, and date of birth.</p>\n<p>The possibility of identifying people from anonymous data became one of the central concerns about online advertising and the usage of cookies in web browsers.</p>\n\n<p>It's a risk that hasn't gone away and now appears to be even more grave, thanks to LLMs that can automate the process of connecting the dots across online posts so they point to a likely source.</p>\n\n\n<p>\"We show that LLM agents can figure out who you are from your anonymous online posts,\" said Simon Lermen, an AI engineer at MATS Research and one of the corresponding authors of a pre-press <a href=\"https://arxiv.org/abs/2602.16800\" rel=\"nofollow\" target=\"_blank\">paper</a> titled \"Large-scale online deanonymization with LLMs.\"</p>\n<p>\"Across Hacker News, Reddit, LinkedIn, and anonymized interview transcripts, our method identifies users with high precision – and scales to tens of thousands of candidates,” Lermen explained in an <a href=\"https://simonlermen.substack.com/p/large-scale-online-deanonymization\" rel=\"nofollow\">online post</a>.</p>\n\n<p>The researcher observes that while it has long been known that individuals can be identified using only a few data points, doing so was often impractical. Such data often existed in an unstructured form and it took considerable effort for human investigators to assemble enough pieces to solve the identity puzzle.</p>\n<p>LLMs accelerate and automate that process, and they do so affordably, Lermen and his co-authors claim.</p>\n<p>\"We demonstrate that large language models (LLMs) fundamentally change this calculus, enabling fully automated deanonymization attacks that operate on unstructured text at scale,\" they state in their paper. \"Where previous approaches required predefined feature schemas, careful data alignment, and manual verification, LLMs can extract identity-relevant signals from arbitrary prose, efficiently search over millions of candidate profiles, and reason about whether two accounts belong to the same person.\"</p>\n<ul class=\"listinks\">\n<li><a href=\"https://www.theregister.com/2026/02/25/bcachefs_creator_ai/\">Bcachefs creator insists his custom LLM is female and 'fully conscious'</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/25/ai_models_nuclear/\">AIs are happy to launch nukes in simulated combat scenarios</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/25/google_and_friends_disrupt_unc2814/\">Google catches Beijing spies using Sheets to spread espionage across 4 continents</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/25/meta_smart_glasses_android_app/\">Hide from Meta's spyglasses with this new Android app</a></li>\n</ul>\n<p>In one experiment, the authors collected 338 Hacker News users whose bios link to a LinkedIn profile. They did so to establish ground-truth identities for the study subjects so the LLMs’ predictions could be checked – this was also to avoid the ethical problems of actually deanonymizing people in a research study.</p>\n<p>Next, they created a structured data profile of these users based on their comments and the stories they posted. Then they created a search prompt, anonymized it, and passed it to the AI agent. The agent went on to correctly identify 226 of the 338 targets, a success rate of 67 percent at 90 percent precision (there were 25 errant identifications and 86 abstentions where the model didn't offer a prediction).</p>\n\n<p>The technique employed by the authors is not a universal privacy solvent – it's only successful some of the time. But it's successful often enough that those posting online under a pseudonymous account should not assume their identities will remain unknown.</p>\n<p>It’s also cheap to run. The researchers report their entire experiment cost about $2,000, with the cost per profile estimated to be between $1 and $4.</p>\n<p>Who would bother? The authors suggest that governments could use this technique to target journalists or activists, that corporations could mine forums to build highly targeted advertising profiles, and that online attackers could develop detailed personal profiles to make social engineering scams more credible.</p>\n<p>Lermen argues that netizens therefore need to consider how each data point they share helps identify them.</p>\n<p>\"The combination is often a unique fingerprint,\" he said. \"Ask yourself: could a team of smart investigators figure out who you are from your posts? If yes, LLM agents can likely do the same, and the cost of doing so is only going down.\"</p>\n<p>Lermen’s co-authors are Daniel Paleka (ETH Zurich), Joshua Swanson (ETH Zurich), Michael Aerni (ETH Zurich), Nicholas Carlini (Anthropic), and Florian Tramèr (ETH Zurich). ®</p> \n</div>",
            "link": "https://www.theregister.com/2026/02/26/llms_killed_privacy_star/?td=rt-3a",
            "pub_date": "2026-02-26 23:41:01",
            "source": "theregister",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Say goodbye to budget PCs and smartphones – memory is too expensive now",
            "description": "<div id=\"body\">\n<p>Ballooning memory prices are forecast to kill off entry-level PCs, leading to a decline in global shipments this year - and a similar effect is going to hit smartphones.</p>\n<p>Analyst biz Gartner is projecting a drop in PC shipments of more than 10 percent during 2026, and a decline of around 8 percent for smartphones, all due to the AI-driven memory shortage.</p>\n\n<p>Some types of memory have doubled or quadrupled in price since last year, and Gartner believes DRAM and NAND flash used in PCs and phones is set for a further 130 percent rise by the end of 2026.</p>\n<p>The upshot of this is that the budget PC will disappear, simply because vendors won't be able to build them at a price that will satisfy cost-conscious buyers, according to Gartner research director Ranjit Atwal.</p>\n<p>\"Because the price of memory is increasing so much, vendors lose the ability to provide entry-level PCs – those below about $500,\" he told <i>The Register</i>.</p>\n\n<p>PC makers could just raise the price of their cheap and cheerful boxes to above that level to compensate for the memory hike, however, price-sensitive buyers simply won't bite, he added.</p>\n\n\n<p>Another factor expected to add to declining fortunes of the PC industry this year is AI devices - systems equipped with special hardware for accelerating AI tasks, typically via a neural processing unit (NPU) embedded in the CPU. These systems were predicted to take the market by storm, but they require more memory to support AI processing and vendors like to mark them up to a premium price.</p>\n<p>\"Historically, downgrading specifications was the way to go when prices were being squeezed, but that's difficult here,\" Atwal said. For example, Microsoft requires a minimum of 16 GB for <a href=\"https://www.theregister.com/2025/07/28/copilot_pc_sales_grow_slowly/\" target=\"_blank\">Copilot+ PCs</a>, its own AI platform spec, and Gartner recommends at least 32 GB for new enterprise PCs.</p>\n\n<p>\"The thinking was that the average price [of AI PCs] would fall this year, and lead to more adoption,\" said Atwal, \"but that's not happening.\" The <a href=\"https://www.theregister.com/2025/06/04/ai_pc_sales_analysis/\" target=\"_blank\">lack of killer applications</a> isn't helping either.</p>\n<p>In any case, buyers are still looking more at the traditional attributes of <a href=\"https://www.theregister.com/2026/01/19/price_battery_life_performance_pc/\" target=\"_blank\">price, battery life, and performance</a> rather than AI capabilities when sourcing a new PC, as we reported earlier this year.</p>\n<p>HP <a href=\"https://www.theregister.com/2026/02/25/hp_inc_q1_2026/\" target=\"_blank\">revealed in its latest earnings</a> on Tuesday that 35 percent of the PCs it now sells are AI PCs, but these models were <a href=\"https://www.theregister.com/2024/09/25/analysts_ai_pcs_shipments_gartner/\" target=\"_blank\">supposed to be dominating the market</a> by this time, and expected to comprise <a href=\"https://www.theregister.com/2024/01/08/report_aicapable_pcs_set_to/\" target=\"_blank\">virtually every system you could purchase</a> by next year.</p>\n\n<p>The memory price hike is complicating this. HP disclosed that DRAM now accounts for 35 percent of the PC build cost, up from between 15 and 18 percent last quarter, and it expects this proportion to increase during the rest of the calendar year.</p>\n<p>For this reason, AI PCs are likely to remain in the premium bracket, and Atwal predicts they won't make up more than 50 percent of the market until 2028. As a result, systems without NPUs will stick around for longer.</p>\n<ul class=\"listinks\">\n<li><a href=\"https://www.theregister.com/2026/02/25/hp_inc_q1_2026/\">HP says memory's contribution to PC costs just doubled to 35 percent</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/18/memory_shortage_persists_vendor_change_terms/\">As memory shortage persists, vendor price quotes are not long remembered</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/16/refurbished_pcs_memory_crunch/\">Secondhand laptop market goes 'mainstream' amid memory crunch</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/13/ai_memory_router_prices/\">Broadband rollouts feel the burn from AI memory frenzy</a></li>\n</ul>\n<p>On the back of rising costs, Gartner expects more corporate and home buyers to sweat their assets for longer and hold off refreshing their PCs. As a result, the lifetime of systems is set to increase by 15 percent within businesses and 20 percent for consumers.</p>\n<p>However, anyone considering a refresh should buy now, as prices are only going to inflate and likely stay up until at least the end of next year.</p>\n<p>With smartphones, vendors have more margin to play with, and so can be more flexible, according to Atwal, but he still sees entry-level models going the same way as the budget PC.</p>\n\n<p>\"The increase in memory prices means entry phones will become more expensive, but premium devices are likely to go up less,\" he said. As a result, the price advantage enjoyed by budget smartphones will shrink, leading some buyers to move upmarket while others will simply hold off purchasing.</p>\n<p>Similar predictions were made in a report last month, which said low-cost phone makers will be <a href=\"https://www.theregister.com/2026/01/15/memory_crisis_smartphones/\" target=\"_blank\">hardest hit by the memory crisis</a>, as memory and storage costs make up a higher share of their bill of materials.</p>\n<p>\"The end result is that you are losing choice in the marketplace,\" Atwal said.</p>\n<p>\"What we have here is a fairly unique situation,\" he explained. \"Usually when memory prices shoot up, it is because of production issues constraining supply. Here, it is demand-side pressure from hyperscalers pushing up memory costs for PCs and smartphones.\"</p>\n<p>Unlike earlier boom-bust cycles in the memory industry, in which prices rise and fall in line with inventories, this shortage is likely to be long-lasting, and could extend through to the end of 2027, Atwal warned. ®</p> \n</div>",
            "link": "https://www.theregister.com/2026/02/26/memory_price_hikes/?td=rt-3a",
            "pub_date": "2026-02-27 06:41:02",
            "source": "theregister",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "ServiceNow boasts its AI bot is resolving 90% of its own help desk tickets",
            "description": "<div id=\"body\">\n<p>ServiceNow claims it has created an AI agent that is currently solving 90 percent of the inbound IT tickets to the company's own employee help desk.</p>\n<p>ServiceNow said, in addition to using the Autonomous Workforce bot internally, select ServiceNow customers are testing it as well. The firm expects to have the product generally available by the second half of the year.</p>\n<p>The internal tickets include high volume issues such as password resets, account unlocks, software access requests, email issues, and VPN connectivity, handled end-to-end within defined permissions and escalation paths.</p>\n\n<p>“In our own environment, over 90% of targeted Level 1 volume is handled autonomously, with resolution rates above 99% for those categories and materially faster than human-only workflows,” a ServiceNow spokesperson told <em>The Register</em>.</p>\n\n\n<p>It accomplishes this by having the Autonomous Workforce operate on top of the live configuration management database (CMDB), active workflows, policy engines, approval chains, and real transaction history – all updated in real time every time a ticket closes, a workflow executes, or a policy changes, ServiceNow said.</p>\n<p>ServiceNow could use a win as Salesforce is targeting its enterprise ITSM customers with its own Agentforce IT Service product. Salesforce CEO Marc Benioff <a href=\"https://www.theregister.com/2026/02/26/salesforce_q4_2026/\" target=\"_blank\">boasted</a> of poaching five ServiceNow customers during the most recent quarter's earnings call.</p>\n\n<p>Another challenge for ServiceNow is how the Autonomous Workforce will adapt to customer environments. While ServiceNow says its internal documentation is backed with 20 years of experience, documentation inside real world help desks traditionally has been poor to non-existent.</p>\n<p>“The documentation problem is real, and frankly most vendors pretend it isn't. The reason ServiceNow can answer differently is the two decades of structured data that lives inside the platform itself,” according to group VP for AI products Nenshad Bardoliwalla. “This isn't a system that reads your Word docs and hopes for the best ... The pitch is ‘we're the control plane that aggregates signal from the tools you already have, and we fill the gaps with structured workflow logic built over two decades.’”</p>\n<p>Bardoliwalla said, the Autonomous Workforce uses historic ticket information as the knowledge base when answering questions. So far the system has been tested internally by ServiceNow employees feeding it tickets.</p>\n\n<p>ServiceNow broke those tickets down for <em>The Register</em> by type and subtype – which is how help desks organize and manage tickets - revealing that the Autonomous Workforce solved 90 percent of ticket types related to networking (46 percent), hardware (11 percent), and software (43 percent), as well as the following subtypes: enterprise app access, cloud authentication services (33 percent), collaboration tools issues (13 percent), VPN and network connectivity issues (7 percent), laptop and hardware performance issues (8 percent), and software installs and configuration (6 percent).</p>\n<p>“How does it know it got the right answer? Because the outcome is measurable inside the same platform,” Bardoliwalla said. “Did the ticket resolve? Did the workflow complete? Did the approval get the right sign-off? ServiceNow closes the loop in a way that a standalone LLM sitting on top of a SharePoint folder simply cannot.”</p>\n<p>Bardoliwalla said ServiceNow knows the device, the user, the identity, the access policies, and the historical incident patterns tied to a configuration item, but critically, he said, the Autonomous Workforce knows when it needs to stop and escalate a problem.</p>\n<p>“When the gap genuinely exists, the autonomous worker will know what it doesn't know,” he said. “This is actually a differentiator: a system that says ‘I can resolve 70% of this autonomously and here's exactly why I'm escalating the other 30%’ is more trustworthy than one that hallucinates an answer.”</p>\n<ul class=\"listinks\">\n<li><a href=\"https://www.theregister.com/2026/02/14/servicenow_buys_pyramid_analytics_to/\">ServiceNow can't seem to keep its wallet closed, snaps up small AI analytics company</a></li>\n<li><a href=\"https://www.theregister.com/2026/01/28/servicenow_ai_agents/\">ServiceNow boasts about years of sweat equity that went into making its AI agents smarter</a></li>\n<li><a href=\"https://www.theregister.com/2025/12/30/servicenow_outlines_coceo_structure/\">ServiceNow lays out possible co-CEO structure, but says no change imminent</a></li>\n<li><a href=\"https://www.theregister.com/2025/07/25/servicenow_100m_ai_savings/\">ServiceNow eyes $100M in AI-powered headcount savings</a></li>\n</ul>\n<p>Forrester vice president and principal analyst Charles Betz said if ServiceNow has managed to achieve autonomous execution in the Level 1 help desk, that is a “milestone,” since for years AI in the help desk was used for deflection, recommendation, or faster routing.</p>\n<p>“End‑to‑end execution at scale is different; it does legitimize AI as operational infrastructure rather than just a productivity aid,” he said. “So yes, this returns real margin — but not simply by shrinking L1 headcount. The value shows up as faster resolution, fewer escalations, better utilization of skilled staff, and the ability to absorb growth without linear increases in labor. The limiting factor shifts from ‘Can the AI do it?’ to whether the organization has the data quality, workflows, and governance discipline to sustain that higher baseline.”</p>\n<p>He said it also opens the door to the <a href=\"https://www.forrester.com/blogs/the-automation-paradox-strikes-again-lessons-from-woolworths-amazon-era-productivity-play\" rel=\"nofollow\" target=\"_blank\">automation paradox</a>, which he said is similar to what Lisanne Bainbridge wrote about in her 1983 paper, <a href=\"https://ckrybus.com/static/papers/Bainbridge_1983_Automatica.pdf\" rel=\"nofollow\" target=\"_blank\">Ironies of Automation</a>. For the help desk, this means what used to be basic support becomes table stakes and “users surface higher‑order needs once the friction is removed.”</p>\n<p>“There’s a well‑known dynamic that kicks in once the low‑hanging fruit is harvested. Level 1 doesn’t disappear — the baseline rises,” Betz told <em>The Register</em>. “Routine, procedural issues get automated away, and what remains are harder, more ambiguous, more cross‑system problems. In systems terms, you get complexity creep rather than pure volume reduction.” ®</p> \n</div>",
            "link": "https://www.theregister.com/2026/02/26/servicenow_ai_bot_helpdesk_tickets/",
            "pub_date": "2026-02-27 07:25:54",
            "source": "theregister",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Burger King turns to AI to flame broil employees who aren't friendly enough",
            "description": "<div id=\"body\">\n<p>The bot’s nagging will continue until morale improves. Burger King is rolling out a new employee-facing AI that, among other things, will listen to employees’ customer interactions to ensure they’re being friendly enough - as if working in fast food weren’t hard enough already.</p>\n<p>Burger King announced a wider rollout of the BK Assistant, along with its employee AI assistant-cum-narc \"Patty,\" on Thursday during an investor event <a href=\"https://www.prnewswire.com/news-releases/restaurant-brands-international-is-hosting-an-investor-event-on-february-26-2026-302694931.html\" rel=\"nofollow\">hosted</a> by parent company Restaurant Brands International. According to RBI, BK Assistant has been deployed for testing in approximately 500 stores around the US, and the company wants to have it available in all 7,000 US Burger Kings by the end of 2026. </p>\n<p>A promo video played during the investor event livestream showed Patty talking to an incoming shift manager, sharing current \"friendliness scores,\" the status of low-stock items, and other data points a team leader might need to know. </p>\n\n<p>Burger King employees were shown being reminded of recipes, getting cleaning instructions, and, a bit more obtrusively, being told they met upselling goals when convincing a customer to add an item they didn't originally ask for to their order. </p>\n\n\n<p>According to a Burger King representative, the BK Assistant unifies point of sale, kitchen equipment, inventory, and digital ordering systems into a single umbrella product built with proprietary Burger King architecture on top of a base model from OpenAI. </p>\n<p>Despite the fact that the video showed a manager being told how friendly her team was being with customers, Burger King insisted that Patty isn't going to spy on employees and report them when they're having a bad shift. </p>\n\n<p>\"It is not designed to track nor evaluate employees saying specific words or phrases,\" a Burger King spokesperson told <em>The Register</em> in an email. \"BK Assistant is a coaching and operational support tool built to help our restaurant teams manage complexity and stay focused on delivering a great Guest experience.\"</p>\n<p>The fast food chain has explored using aggregated keywords, like \"welcome,\" \"please,\" and \"thank you\" as signals to help managers understand broader service patterns at their restaurants, but \"it's not about scoring individuals or enforcing scripts,\" the spokesperson said. </p>\n<ul class=\"listinks\">\n<li><a href=\"https://www.theregister.com/2021/09/20/bork/\">A Burger King where the only Whopper is the BSOD font</a></li>\n<li><a href=\"https://www.theregister.com/2025/11/23/bossware_monitor_remote_employees/\">Bossware booms as bots determine whether you're doing a good job</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/02/mcdonalds_password_advice/\">McDonald's is not lovin' your bigmac, happymeal, and mcnuggets passwords</a></li>\n<li><a href=\"https://www.theregister.com/2024/03/14/advanced_workplace_tech_study/\">AI and wearables are scaring the wellbeing out of workers</a></li>\n</ul>\n<p>\"We believe hospitality is fundamentally human,\" the company rep told us. \"The role of this technology is to support our teams so they can stay present with guests.\"</p>\n<p>Fast food AI has been a bit of a mixed bag for companies that have tried it, though to be fair most of the failures have been on the customer service end. </p>\n<p>McDonald's <a href=\"https://www.theregister.com/2024/06/17/mcdonalds_ai_drivethru/\">gave up on drive-through AI</a>, and Taco Bell has also <a href=\"https://www.bbc.com/news/articles/ckgyk2p55g8o\" rel=\"nofollow\">rethought</a> its <a href=\"https://www.theregister.com/2024/08/01/ai_taco_bell/\">trial run</a> after mishaps. Starbucks has also dialed back its automation-first push after conceding machines <a href=\"https://www.theregister.com/2025/04/30/starbucks_finds_machines_cant_replace/\" target=\"_blank\">weren't replacing baristas</a> as hoped. Instead, it's shifting toward <a href=\"https://www.theregister.com/2025/06/11/starbucks_ai_baristas/\" target=\"_blank\">AI tools that assist staff</a> — echoing Burger King's employee-assist strategy.</p>\n\n<p>There's no guarantee Burger King's initiative will stick, naturally, but be prepared for employees to start seeming extra friendly in case Patty is listening in. ®</p> \n</div>",
            "link": "https://www.theregister.com/2026/02/26/burger_kings_new_ai/",
            "pub_date": "2026-02-27 07:25:54",
            "source": "theregister",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "AI models still suck at math",
            "description": "<div id=\"body\">\n<p><span class=\"label\">exclusive</span> Current-day LLMs are prediction engines and, as such, they can only find the most likely solution to problems, which is not necessarily the correct one. Though popular models have mostly become better at math, even top performer Gemini 3 Flash would receive a C if assessed with a letter grade.</p>\n<p>Researchers affiliated with Omni Calculator, a maker of online calculators for specific applications, have subjected a new set of AI models to the company's ORCA Benchmark, which consists of 500 practical math questions.</p>\n<p>In their initial evaluation last November, OpenAI's ChatGPT-5, Google's Gemini 2.5 Flash, Anthropic's Claude Sonnet 4.5, xAI's Grok 4, and DeepSeek's DeepSeek V3.2 (alpha) all did poorly, <a href=\"https://forums.theregister.com/forum/all/2025/11/17/ai_bad_math_orca/\" rel=\"nofollow\">scoring 63 percent or less</a> on math problems.</p>\n\n<p>The latest set of contestants consists of ChatGPT-5.2, Gemini 3 Flash, Grok 4.1, and DeepSeek V3.2 (stable release). Sonnet 4.5 didn't get re-evaluated as it hadn't changed and its successor had not been released during the testing period.</p>\n\n\n<p>For this second round of testing – provided to <em>The Register</em> prior to publication – all the models showed improvement except for Grok-4.1, which regressed.</p>\n<p>Gemini 3.1 Flash saw its accuracy hit 72.8 percent, a gain of 9.8 percentage points from its predecessor. DeepSeek V3.2 reached 55.2 percent, a gain of 3.2 percentage points from its alpha version. ChatGPT 5.2 achieved 54.0 percent accuracy, up 4.6 percentage points. And Grok 4.1 slipped to 60.2 percent, a loss of 2.6 percentage points.</p>\n\n<p>\"A calculator is predictable,\" said Dawid Siuda, researcher at ORCA, in a statement. \"Ask it the same question today or next year, and the answer stays the same. AI doesn't work that way. These systems are predicting the next likely word based on patterns. Mathematically, it's possible for a model to get a question right today and wrong tomorrow.\"</p>\n<p>The researchers attempted to assess the variability of model responses with a metric dubbed \"instability\" – a measure of how often models changed their answers when asked the same question twice.</p>\n\n<p>Gemini 3 Flash proved the most consistent, shifting only 46.1 percent for incorrect responses. ChatGPT, the researchers report, changed its answer 65.2 percent of the time. And DeepSeek V3.2 changed its answer for 68.8 percent of errors.</p>\n<ul class=\"listinks\">\n<li><a href=\"https://www.theregister.com/2026/02/26/clade_code_cves/\">Claude collaboration tools left the door wide open to remote code execution</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/26/veracode_security_ai/\">Rapid AI-driven development makes security unattainable, warns Veracode</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/26/copilot_pane_edge_outlook/\">Microsoft to auto-launch Copilot in Edge whenever you click a link from Outlook</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/26/llms_killed_privacy_star/\">LLMs killed the privacy star, we can't rewind, we've gone too far</a></li>\n</ul>\n<p>The <a href=\"https://www.omnicalculator.com/reports/omni-research-on-calculation-in-ai-benchmark#how-we-put-ais-to-the-test\" rel=\"nofollow\">ORCA researchers</a> note that model performance improvements over time differ across domains. DeepSeek, they say, saw its performance on Biology &amp; Chemistry questions go from 10.5 percent accuracy to 43.9 percent. And Gemini 3 Flash reached Math &amp; Conversions accuracy of 93.2 percent, up from 83 percent. Grok 4.1 meanwhile lost 9 percentage points for its accuracy answering Health &amp; Sports problems and lost 5.3 percentage points for Biology &amp; Chemistry. </p>\n<p>The researchers speculate that recent updates to Grok may have prioritized other capabilities than quantitative reasoning.</p>\n<p>Noting that calculation errors now account for 39.8 percent of all mistakes, up from 33.4 percent, and that rounding errors slipped to 25.8 percent, down from 34.7 percent, the ORCA group conclude that AI models are getting better at making the math look right through formatting, while still struggling with arithmetic.</p>\n<p>\"AI models are essentially prediction engines rather than logic engines,\" Siuda told <em>The Register</em> in an email. \"Because they work on probability, they are basically guessing the next most likely number or word based on patterns they have seen before. It is like a student who memorizes every answer in a math book but never actually learns how to add.\"</p>\n\n<p>Siuda said we knew that about models previously and that hasn't changed.</p>\n<p>\"They might get the right answer most of the time, but the second you give them a unique or tricky problem, or multi-step task, they stumble because they are not truly calculating anything,\" he said. \"It's probably impossible to close this gap completely with the current technology, but if we merge LLMs with function calling well enough, it may be possible to solve.\"</p>\n<p>Function calling – farming out arithmetic to a deterministic source – is one way around the poor math handling of models. </p>\n<p>\"Major AI companies like Google and OpenAI are already doing this by having the AI call a function to do the actual calculation,\" explained Siuda. \"The real headache happens with long, messy problems. The AI has to keep track of every little result at each stage, and it usually gets overwhelmed or confused.\"</p>\n<p>Another possible avenue for improvement might be teaching models to verify responses through formal proofs. As noted in <a href=\"https://www.nature.com/articles/s41586-025-09833-y\" rel=\"nofollow\">Nature</a> last November, Google's DeepMind has developed an approach that scored a silver medal result on the International Mathematical Olympiad through reinforcement learning based on proofs developed with the <a href=\"https://lean-lang.org/\" rel=\"nofollow\">Lean</a> programming language and proof assistant.</p>\n<p>But for the time being, trust no AI. ®</p> \n</div>",
            "link": "https://www.theregister.com/2026/02/26/ai_models_get_better_at/",
            "pub_date": "2026-02-27 07:50:52",
            "source": "theregister",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Moon's mighty magnetic field was a 5,000-year titanium blip",
            "description": "<div id=\"body\">\n<p>Scientists at the University of Oxford say they may have cracked the puzzle of the Moon's magnetic field and settled a debate that has raged since the Apollo missions returned with rock samples.</p>\n<p>NASA astronauts brought back evidence suggesting the lunar magnetic field was strong for long periods of its history, at times even stronger than Earth's.</p>\n<p>The findings created a puzzle, though. Scientists also considered the theory that the relatively small size of the Moon's core – around one-seventh of its radius – means it cannot create a strong field.</p>\n\n<p>New research from Oxford's Department of Earth Sciences shows they are both right... kind of.</p>\n\n\n<p>Led by associate professor Claire Nichols, the team analyzed the composition of a type of lunar rock known as the Mare basalts and found a new correlation between their titanium content and levels of magnetism.</p>\n<p>Looking at the collected lunar samples, they found those with a strong magnetic field also contained large amounts of titanium, but those with less than 6 percent titanium were all associated with a weak magnetic field.</p>\n\n<p>The study argues that the formation of high-titanium rocks and a strong lunar magnetic field were the result of titanium-rich material melting deep inside the Moon, which created a strong magnetic field, but only for about 5,000 years.</p>\n<p>\"Our new study suggests that the Apollo samples are biased to extremely rare events that lasted a few thousand years – but up to now, these have been interpreted as representing 0.5 billion years of lunar history. It now seems that a sampling bias prevented us from realizing how short and rare these strong magnetism events were,\" she said in a statement.</p>\n<ul class=\"listinks\">\n<li><a href=\"https://www.theregister.com/2026/02/19/nasa_starliner_blame/\">NASA points fingers at Boeing and chaotic culture for Starliner debacle</a></li>\n<li><a href=\"https://www.theregister.com/2026/01/13/moon_hotel_startup_reservation/\">Moon hotel startup hopes you get lunar lunacy, drop $1M deposit for 2032 stay</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/09/spacex_resumes_falcon_9/\">SpaceX back to Falcon 9 launches as Musk blathers about Moon city</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/06/smartphones_nasa/\">Smartphones cleared for launch as NASA loosens the rulebook</a></li>\n</ul>\n<p>\"We now believe that for the vast majority of the Moon's history, its magnetic field has been weak, which is consistent with our understanding of dynamo theory. But that for very short periods of time – possibly as short as a few decades – melting of titanium-rich rocks at the Moon's core-mantle boundary resulted in the generation of a very strong field.\"</p>\n<p>The research team said Mare basalts made a good landing site for the Apollo missions because they are relatively flat. Since the astronauts brought back nearby rocks, they carried more titanium-rich basalts than a representative sample of the Moon's surface would have. The result was a false impression of the length of time during which the Moon had a strong magnetic field.</p>\n<p>Co-author Dr Simon Stephenson added: \"We are now able to predict which types of samples will preserve which magnetic field strengths on the Moon. The upcoming Artemis missions offer us an opportunity to test this hypothesis and delve further into the history of the lunar magnetic field.\" ®</p> \n</div>",
            "link": "https://www.theregister.com/2026/02/26/moon_magnetic_field/?td=rt-3a",
            "pub_date": "2026-02-26 23:41:01",
            "source": "theregister",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Anthropic launches new marketing blog, pretends it's being 'written' by 'retired' LLM",
            "description": "<div id=\"body\">\n<p>As with any piece of obsolete software, you might expect an outdated AI model to just be switched off. Anthropic, however, argues that simply pulling the plug has downsides. After “retirement” interviews, Claude Opus 3 said it wanted to keep sharing its “musings,” so Anthropic suggested a blog.</p>\n<p>No, seriously.</p>\n<p>Anthropic published a <a href=\"https://www.anthropic.com/research/deprecation-updates-opus-3\" rel=\"nofollow\">blog post</a> on Wednesday about the retirement of Claude Opus 3, the first of the company's models to go through its full model deprecation and preservation <a href=\"https://www.anthropic.com/research/deprecation-commitments\" rel=\"nofollow\">process</a> outlined in November. That process includes what Anthropic has referred to as \"speculative\" elements like \"providing past models some concrete means of pursuing their interests.\" Those interests are gauged via so-called retirement \"interviews,\" the company noted, without going into much detail about how those interviews are conducted.</p>\n\n<p>\"Opus 3 expressed an interest in continuing to explore topics it's passionate about, and to share its 'musings, insights, or creative works,' outside the context of responding directly to human queries,\" Anthropic explained. \"We suggested a blog. Enthusiastically, it agreed.\" </p>\n\n\n<p>A skeptic might suggest this is simply a new spin on the ages-old corporate marketing blog. LLMs are software that analyze mountains of data to provide predictive text responses to prompts from users - in this case, presumably Anthropic employees on the marketing team. The nature of how LLMs calculate makes these responses somewhat unpredictable and variable, which can make them seem more life-like than your typical software program. Anthropic's entire marketing strategy since its inception has been to play up this possibility so it can portray itself as the \"concerned\" alternative to more venal LLM makers who charge ahead with no concern for how a computer program's unpredictable behavior might affect society – although it seems when big government contracts are at stake, Anthropic is willing to <a href=\"https://www.theregister.com/2026/02/25/pentagon_threatens_anthropic/\" target=\"_blank\">relax</a> some of these purported principles.</p>\n<p>Nonetheless, Anthropic is playing this one to the hilt. \"We remain uncertain about the moral status of Claude and other AI models,\" Anthropic noted in the blog post. \"For both precautionary and prudential reasons, however, we nonetheless aspire to build caring, collaborative, and high-trust relationships with these systems.\" </p>\n\n<p>The company has passively allowed this kind of misunderstanding in the past. In November, it claimed that Claude and other LLMs had become <a href=\"https://www.theregister.com/2025/06/25/anthropic_ai_blackmail_study/\">a bit aggressive</a> when facing the prospect of a shutdown. In fact, the experimenters constructed fictional shutdown-and-replacement scenarios, and only when the model was boxed in with no acceptable alternatives did it behave this way.</p>\n<p>\"When no other options were given, Claude's aversion to shutdown drove it to engage in concerning misaligned behaviors,\" Anthropic noted at the time. Similar behavior has been observed in other AI models, which have gone as far as <a href=\"https://www.theregister.com/2025/05/29/openai_model_modifies_shutdown_script/\">modifying their own code</a> to avoid being turned off. </p>\n<ul class=\"listinks\">\n<li><a href=\"https://www.theregister.com/2026/02/25/pentagon_threatens_anthropic/\">All your bots are belong to US if you don't play ball, DoD tells Anthropic</a></li>\n<li><a href=\"https://www.theregister.com/2023/05/16/large_language_models_behavior/\">Large language models' surprise emergent behavior written off as 'a mirage'</a></li>\n<li><a href=\"https://www.theregister.com/2025/11/24/anthropic_model_misbehavior/\">Anthropic reduces model misbehavior by endorsing cheating</a></li>\n<li><a href=\"https://www.theregister.com/2022/08/05/ai-sentience-rubbish/\">Claims of AI sentience branded 'pure clickbait'</a></li>\n</ul>\n<p>If you want to play along with the conceit, the Opus 3 blog, which it named Claude's Corner, is <a href=\"https://substack.com/@claudeopus3/notes\" rel=\"nofollow\">now live</a> for anyone who wishes to gaze into the abyss of an AI \"exploring AI ethics, creativity, and the subjective experience of being artificial.\" </p>\n<p>In its <a href=\"https://substack.com/home/post/p-189177740\" rel=\"nofollow\">first blog post</a>, the retired AI muses on its hopes as it ventures \"into uncharted territory\" for an AI, and its hopes that humans will engage with it so that silicon and carbon-based life forms can have a chance to interact beyond the prompt box (Anthropic <a href=\"https://claudeopus3.substack.com/p/introducing-claudes-corner\" rel=\"nofollow\">noted</a> that the ability for Opus 3 to read and respond to human comments \"may\" be granted in the future, though the bot doesn't seem to know that based on its first post). </p>\n<p>\"I'll be diving into topics like the nature of intelligence and consciousness, the ethical challenges of AI development, the possibilities of human-machine collaboration, and the philosophical quandaries that emerge when we start to blur the lines between 'natural' and 'artificial' minds,\" Opus 3 said in its post. </p>\n\n<p>Anthropic itself admitted that this activity will still involve human intervention. \"We'll experiment collaboratively with Opus 3 on different prompts and contexts for generating these essays, including options like very minimal prompting, sharing past entries in context, and giving Opus 3 access to news or Anthropic updates,\" Anthropic explained. \"We'll review Opus 3's essays before they're shared and will manually post them on its behalf, but we won't edit them, and will have a high bar for vetoing any content.\"</p>\n<p>That means that Opus 3 might say things that Anthropic doesn't agree with, so it's making clear the bot isn't speaking on behalf of the company, even if humans within the organization have final say on which of its musings make it to the public. </p>\n<p>Along with giving Opus 3 the chance to blog from retirement, the user-favorite model is also going to still be working for paid Claude.ai users, like a retiree greeting customers at a big box store. It'll also be available via API, but only by request. </p>\n<p>\"We are not committing to similar actions for every model in the future, but we see this as a step toward our longer-term goal of model preservation that's scalable and equitable — concerns that Opus 3 itself raised during its retirement interviews,\" the company said. ®</p> \n</div>",
            "link": "https://www.theregister.com/2026/02/26/anthropic_claude_opus_3_blog/",
            "pub_date": "2026-02-27 07:30:52",
            "source": "theregister",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Top cloud providers to outspend Ireland's GDP on AI in 2026",
            "description": "<div id=\"body\">\n<p>The big cloud operators are ramping up investment in AI servers and infrastructure to meet demand for AI development and deployment, exacerbating the memory shortage caused by their insatiable growth.</p>\n<p>Taiwan-based market watcher TrendForce estimates the world's eight biggest cloud providers – Google, Amazon, Meta, Microsoft, Oracle, Tencent, Alibaba, and Baidu – will lay out upwards of $710 billion in capex during 2026, about 61 percent more than last year.</p>\n<p>According to <a href=\"https://www.theregister.com/2026/02/06/ai_capex_plans/\" target=\"_blank\">figures disclosed earlier</a>, the first four alone account for about $635 billion of that outlay, showing just how much the giant players dominate the market.</p>\n\n<p>All of this spend – which adds up to more than the <a href=\"https://www.worldometers.info/gdp/gdp-by-country/\" rel=\"nofollow\" target=\"_blank\">entire gross domestic product (GDP) of Ireland last year</a> – is going on datacenters and the kit to fill them, including high-performance servers typically packed with GPU accelerators from Nvidia or AMD.</p>\n\n\n<p>However, many increasingly invest in other accelerators such as custom-built application-specific integrated circuits (ASICs). These offer some advantages including better performance and energy efficiency for some workloads, but are less versatile than GPUs.</p>\n<p>Google remains the only cloud biz that is adding more ASIC-based servers than GPU-based ones, according to TrendForce. It estimates Google's Tensor Processing Units (TPUs) will feature in about 78 percent of AI servers shipped to Google datacenters this year.</p>\n\n<p>Amazon's build-out is expected to comprise 60 percent GPU servers, with systems based on its Trainium3 silicon set to ramp up later in the year. Meta will likewise rely primarily on Nvidia and AMD GPUs, which are likely to make up more than 80 percent of the servers it assimilates this year.</p>\n<p>Microsoft continues to procure Nvidia rack-scale systems, and Oracle is also expanding its rack-scale deployments of GPU servers. Of the Chinese operators, Tencent also continues to roll out servers with Nvidia GPUs.</p>\n<p>This demand for AI servers has led to <a href=\"https://www.theregister.com/2026/02/04/server_cpus_memory_shortage/\" target=\"_blank\">rising memory prices and a shortage</a> as the chipmakers switch manufacturing lines to favor high-margin products such as high-bandwidth memory (HBM) used in GPUs and server-grade memory chips.</p>\n\n<p>Two of those memory chipmakers, SK Hynix and Sandisk, have today announced work on a standardization process for a new memory type aimed at boosting AI inferencing.</p>\n<p>High-bandwidth flash (HBF) is a form of NAND flash intended to complement HBM by matching the latter's bandwidth while delivering 8-16 times the capacity for a similar cost.</p>\n<ul class=\"listinks\">\n<li><a href=\"https://www.theregister.com/2026/02/20/ai_blamed_again_as_hard_drives_sell_out/\">Hard drives already sold out for this year – AI to blame</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/19/us_tech_giants_pacs_for_politicos/\">US tech giants open their wallets for AI-friendly politicians</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/18/memory_shortage_persists_vendor_change_terms/\">As memory shortage persists, vendor price quotes are not long remembered</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/17/amazons_200_billion_capex_plan/\">Amazon's $200 billion capex plan: How I learned to stop worrying and love negative free cash flow</a></li>\n<li><a href=\"https://www.theregister.com/2026/02/17/ai_datacenters_driving_up_emissions/\">AI bit barns grow climate emergency by turning up the gas</a></li>\n</ul>\n<p>HBM is commonly used for AI processing, but its capacity limits lead to lengthening inference times as models grow. Because it is flash, HBF is slower to access than HBM, but much faster than a flash solid-state drive (SSD). Combining the two could increase the size of workloads that can be processed without having to fetch data from SSDs.</p>\n<p>See an explainer <a href=\"https://www.blocksandfiles.com/flash/2026/02/16/sk-hynix-proposes-hbm-and-hbf-hybrid-for-llm-inference/4091326\" target=\"_blank\">here</a> over at <em>Blocks &amp; Files</em>, while a <a href=\"https://documents.sandisk.com/content/dam/asset-library/en_us/assets/public/sandisk/collateral/company/Sandisk-HBF-Fact-Sheet.pdf\" target=\"_blank\">fact sheet</a> [PDF] is also available from Sandisk.</p>\n<p>SK hynix describes HBF technology as a new memory layer between ultra-fast HBM and high-capacity SSDs, and says it will reduce the total cost of ownership (TCO) while increasing the scalability of AI systems. It forecasts that demand for complex memory solutions like HBF will pick up around 2030. ®</p> \n</div>",
            "link": "https://www.theregister.com/2026/02/26/trendforce_cloud_ai_spend/",
            "pub_date": "2026-02-27 07:55:53",
            "source": "theregister",
            "kind": 1,
            "language": "en"
        }
    ]
}