{
    "data": [
        {
            "title": "MWC 2026: Telecom High-Stakes Bet on the ‘IQ Era’",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>On March 2, Fira Gran Via in Barcelona will host the 20th edition of Mobile World Congress (MWC). This year, the global telecommunications industry is coming together to discuss more than just faster download speeds or new smartphones.</p><p>This edition marks a turning point for the industry, as it shifts from hardware-focused networks to intelligent, AI-driven infrastructure. The <a href=\"https://www.gsma.com/\" rel=\"noreferrer noopener\" target=\"_blank\">GSM Association</a> (GSMA) has named this year’s theme “The IQ Era,” highlighting the move from just expanding bandwidth to building networks powered by AI and dynamic programming.</p><p>The organization structured the conference around <a href=\"https://www.mwcbarcelona.com/themes\" rel=\"noreferrer noopener\" target=\"_blank\">six fundamental thought leadership themes</a>: Intelligent Infrastructure, ConnectAI, AI 4 Enterprise, AI Nexus, Tech4All, and Game Changers.</p>\n<p>EE Times will be on the ground covering these developments. We will be dissecting the technical realities behind the marketing gloss, specifically investigating how the convergence of distributed compute, sensory data, and silicon is rewriting the sector’s economic models.</p>\n\n<p>The stakes are high. After last year’s event drew 109,000 attendees, the industry now faces pressure to show that its massive investments in 5G and data centers can finally pay off by making money from physical and agentic AI.</p>\n<h3 class=\"wp-block-heading\">Clash of semiconductor approaches</h3><p>For engineers and component makers, the main story at MWC 2026 is how semiconductor designs are becoming the foundation for 6G. There’s a strong debate between those who support general-purpose GPUs and those who prefer custom chips made for telecom.</p><p>Nvidia is arriving in Barcelona with the intent to disrupt the traditional vendor landscape. The company has introduced the “<a href=\"https://nvidianews.nvidia.com/news/nvidia-us-telecom-ai-ran-6g\" rel=\"noreferrer noopener\" target=\"_blank\">All-American AI-RAN Stack</a>,” working with T-Mobile and defense partners to bring advanced AI into both hardware and software, helping manage the rapid growth of AI-driven traffic.</p><p>The company has executed a highly disruptive maneuver by open-sourcing its Aerial software on GitHub. By allowing researchers to bypass proprietary bottlenecks, Nvidia aims to accelerate the transition from prototyping to product development, especially for 6G.</p><p>“6G is being built from the ground up with AI at its core—unlocking extreme spectral efficiency, massive connectivity, and breakthrough applications,” <a href=\"https://nvidianews.nvidia.com/news/nvidia-us-telecom-ai-ran-6g\" rel=\"noreferrer noopener\" target=\"_blank\">said Ronnie Vasishta</a>, senior VP telecom at Nvidia. “Working with industry leaders, we’ve created an AI-native wireless stack with advanced features to ensure that America will play an instrumental role in the journey to 6G.”</p><p>Contrastingly, Intel is focusing its narrative on the industry’s most pressing operational headache: power consumption. With AI workloads threatening to blow out energy budgets, Intel is showcasing its Xeon 6 processors with Efficient-cores, promising a 60% boost in energy efficiency. Intel is also teasing its “<a href=\"https://newsroom.intel.com/artificial-intelligence/intel-to-expand-ai-accelerator-portfolio-with-new-gpu\" rel=\"noreferrer noopener\" target=\"_blank\">Crescent Island</a>” data center GPU, optimized for inference workflows and air-cooled servers, signaling a commitment to the edge rather than just the centralized cloud.</p><p>At the same time, Qualcomm is moving beyond its role as a mere handset chipmaker. CEO Cristiano Amon wants the company to be a major player in data centers and physical AI, launching the AI200 and AI250 processors to compete with Nvidia in inference workloads.</p><p>Amon’s vision creates a new paradigm in which generative AI serves as the “universal user interface,” with Snapdragon processors orchestrating “always-sensing” networks that enable autonomous industrial robotics.</p><h3 class=\"wp-block-heading\">AI-native vs. software freedom</h3><p>This split in chip strategies also shows up in the radio access network (RAN) market, where Ericsson and Nokia have taken very different approaches to AI. (EE Times will focus on this divide in our coverage during the week.)</p><p>Both companies are now working on leveraging their 5G investments while developing the new systems for the upcoming 6G networks.</p><p>Ericsson is doubling down on software independence. The Swedish infrastructure giant has made a calculated decision to avoid deep architectural reliance on Nvidia GPUs, favoring its own purpose-built silicon and an agreement with Intel to maintain control over its software stack and avoid vendor lock-in.</p><p>Ericsson unveiled its “<a href=\"https://www.ericsson.com/en/blog/2026/2/agentic-rapp-as-a-service\" rel=\"noreferrer noopener\" target=\"_blank\">Agentic rApp as a Service</a>,” a solution hosted on AWS that is claimed to allow operators to optimize networks using natural language commands. Erik Ekudden, Ericsson’s CTO, dismissed the notion that the industry must pause for the next generation of standards. “There is no need to wait for 6G when it comes to AI,” Ekudden stated. “AI is here, both in the network and as a business opportunity.”</p><p>Nokia, on the other hand, has fully adopted the GPU approach. With a major <a href=\"https://www.eetimes.com/nvidia-1-billion-bet-on-nokia-to-rewire-global-telecom/\" rel=\"noreferrer noopener\" target=\"_blank\">$1 billion investment from Nvidia</a>, Nokia is showing how networks can act as a distributed nervous system.</p><p>Nokia’s demonstration shows that regular communications and heavy AI tasks can run together on the same GPU setup. This is important for integrated sensing and communications (ISAC), where the network can act like a radar to spot drones or robot swarms without extra sensors.</p><h3 class=\"wp-block-heading\">Monetizing the programmable networks</h3><p>Beyond the hardware wars, the business imperative for MWC 2026 is to find new revenue streams for cellular service providers (CSPs). </p><p>Operators must embrace total network programmability to survive. The monetization of massive 5G capital expenditures relies entirely on transitioning away from flat-rate consumer broadband toward specialized, deterministic enterprise services.</p><p>The GSMA <a href=\"https://www.gsma.com/solutions-and-impact/gsma-open-gateway/\" rel=\"noreferrer noopener\" target=\"_blank\">Open Gateway initiative</a>, now in its third year, has matured from a concept into a scalable monetization engine, with APIs now covering 81% of global mobile connections. The focus has shifted from simple anti-fraud measures to “Quality-on-Demand” APIs that guarantee network performance for drone flights and industrial robotics.</p><p>T-Mobile US is going further by introducing Kinetic Tokens, a new way to make money from physical movement. As AI moves from just creating text to controlling things such as self-driving cars or logistics robots, networks need to provide extremely reliable connections.</p><p>John Saw, T-Mobile’s CTO, <a href=\"https://www.fierce-network.com/wireless/exclusive-t-mobiles-john-saw-explains-kinetic-tokens-and-why-they-matter-6g\" rel=\"noreferrer noopener\" target=\"_blank\">explained the lucrative potential</a> of this shift: “When tokens move—when things move with AI—I think it gives telecom operators a license to play in a big opportunity with physical AI.” By using network slicing to tokenize physical motion, operators hope to unlock the enterprise value that eluded them during the initial 5G rollout.</p><h3 class=\"wp-block-heading\">Focusing on digital independence</h3><p>But even amid all the global innovation, geopolitics remains a major topic. European companies such as Telefonica, Deutsche Telekom, TIM, Orange, and Vodafone are using the event to show the “<a href=\"https://www.telefonica.com/en/communication-room/press-room/deutsche-telekom-orange-telefonica-tim-vodafone-logran-el-primer-edge-continuum-federado-paneuropeo-en-el-mwc-2026/\" rel=\"noreferrer noopener\" target=\"_blank\">Edge Continuum</a>,” a network of edge computing nodes across Europe.</p><p>The initiative is explicitly designed to bolster digital sovereignty, offering a trusted alternative to the hyperscalers. “Our European Edge federation demonstrates that Europe is concretely building sovereign digital solutions,” noted Claire Catherine Chauvin, director of strategy architecture and standardization at Orange, highlighting the tension between reliance on American cloud providers and the need for regional autonomy.</p><p>“This federation proves that Europe is not just talking about digital sovereignty; we are building it,” said Christine Knackfuß-Nikolic, chief sovereign officer at T-Systems. “By uniting our networks and expertise, we are creating a secure, open, and trusted digital ecosystem made in Europe—for Europe’s digital future.”</p><h3 class=\"wp-block-heading\">A new horizon for 6G and AI</h3><p>As MWC Barcelona 2026 begins, the telecom industry faces a major turning point. Moving into the IQ Era means rethinking the network as more than just a pipeline—it’s now a programmable, sensing, and computing platform.</p><p>“As March approaches, I’m excited to see the industry come together again in Barcelona,” <a href=\"https://www.mwcbarcelona.com/articles/gsma-mwc26-barcelona-draws-closer-as-ceos-from-character-ai-oura-and-qualcomm-confirmed-as-speakers\" rel=\"noreferrer noopener\" target=\"_blank\">said John Hoffman</a>, CEO of GSMA, reflecting on the event’s evolution. “From Airport of the Future to our New Frontiers zone, there are so many exciting new elements to explore this year.”</p><p>Whether it’s the strength of GPUs or the efficiency of custom chips, the new infrastructure revealed next week will decide which companies make it through the shift from connecting people to managing machines.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><h5 class=\"wp-block-heading\">See also:</h5><p><a href=\"https://www.eetimes.com/nvidia-1-billion-bet-on-nokia-to-rewire-global-telecom/\" rel=\"noreferrer noopener\" target=\"_blank\">Nvidia’s $1 Billion Bet on Nokia to Rewire Global Telecom</a></p><p><a href=\"https://www.eetimes.com/at-mwc-intel-shows-xeon-6-power-for-telecom/\" rel=\"noreferrer noopener\" target=\"_blank\">At MWC, Intel Shows Xeon 6 Power for Telecom</a></p><p><a href=\"https://www.eetimes.com/china-sharpens-strategy-in-the-global-6g-race/\" rel=\"noreferrer noopener\" target=\"_blank\">China Sharpens Strategy in the Global 6G Race</a></p><p><a href=\"https://www.eetimes.com/6g-connectivity-of-intelligence-and-integration/\" rel=\"noreferrer noopener\" target=\"_blank\">6G Connectivity of Intelligence and Integration</a></p> </div>",
            "pub_date": "2026-02-27 17:50:38",
            "link": "https://www.eetimes.com/mwc-2026-telecom-high-stakes-bet-on-the-iq-era/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "SambaNova Abandons Intel Acquisition, Raises Funding Instead",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>SambaNova has joined forces with Intel, but contrary to <a href=\"https://www.eetimes.com/intel-eyeing-ai-catchup-in-inference-with-sambanova-acquisition/\">earlier reports</a> that the AI chip startup was to be acquired by Team Blue for around $1.6 billion, the two companies have instead decided on a multi-year partnership, with Intel also making a strategic investment as part of SambaNova’s $350-million E-round.</p><p>SambaNova also announced a new chip, the SN50, designed for economic agentic inference at scale. This chip features new interconnect technologies that allow more chips to be connected in a single memory space, with increased computing power and the same memory capacity and hierarchy as SambaNova’s previous inference-focused chip, the SN40L.</p><h3 class=\"wp-block-heading\"><strong>Funding round</strong></h3><p>Neither Intel nor SambaNova would confirm reports Intel was set to acquire the startup at the end of 2025. However, a funding round is absolutely the right decision for the company, SambaNova CEO Rodrigo Liang told EE Times.</p>\n<p>“As we saw in the market at the end of last year, the world is still very excited about chips and very excited about the value we can bring,” Liang said. (<a href=\"https://www.eetimes.com/groq-nvidias-20-billion-bet-on-ai-inference/\">Nvidia all-but acquired competitor Groq</a> for a reported $20 billion in December.) “[SambaNova] ended up having a record year last year, which gave us a lot of confidence that the path we are on, selling infrastructure for service providers with the right economics, the right efficiency and the right performance, allowed us to build a lot of momentum behind this [business] model.”</p>\n\n<p>SambaNova’s $350-million series E was “grossly, grossly oversubscribed,” Liang said.</p>\n<p>“For a modest strategic round, we had so much interest,” he added. “We were also able to bring partners in who are bringing unique access to types of customers and applications that are important to us, and we’re excited about that.”</p><p>Like other companies, SambaNova had taken a “pit stop” to refocus its offering on inference rather than training, spending most of 2025 focusing on tokenomics (the economies of serving LLM inference at scale).  </p><p>“What we saw in the back half of the year was tremendous market momentum, which allowed us to feel very confident that the fixes and changes we made were starting to really take hold,” Liang said. “So it really made sense for us to do another round of funding. We saw incredible interest from top-notch investors … That gave us a lot of confidence that this was the right path for us to take for the foreseeable future.”</p><p>The round was led by Vista Equity Partners and Cambium Capital with participation from Intel Capital and sovereign wealth funds from countries including Qatar and the Kingdom of Saudi Arabia. SambaNova also currently has sovereign partnerships in Germany, U.K., Australia, Japan, and France.</p><p>“Our sovereign business has taken off in the last four or five months,” Liang said. “We’re selling into regions where it’s a SambaNova cloud, but it’s operated by our partners or customers. That will continue to be our primary business model, selling infrastructure into cloud and sovereign cloud [rather than becoming a major cloud provider in our own right].”</p><p>Part of SambaNova’s multi-year partnership with Intel is an expansion of SambaNova Cloud, the company’s developer cloud offering, based on Intel Xeon infrastructure. <a href=\"https://www.eetimes.com/sambanova-shifts-to-inference-but-leaves-cloud-to-customers/\">SambaNova said previously that production workloads can and do run on SambaNova Cloud</a>, but Liang confirmed that the startup has no intention of becoming a major AI cloud provider in the same vein as Groq and Cerebras. Developers working on agentic AI will be able to access SambaNova hardware in various regions, giving them access to those markets via SambaNova’s partners and customers, Liang said.</p><h3 class=\"wp-block-heading\"><strong>Intel partnership</strong></h3><p>A second part of the agreement with Intel is an as-yet undefined joint go-to-market strategy, which is likely to include joint selling and marketing through Intel’s enterprise, cloud, and partner channels. “We’ll work through the right operating model and joint plans in the coming months,” an Intel spokesperson told EE Times.</p><p>“Intel’s scale and access plus what we bring to the table with our unique technology and competitive performance in the market, I think there’s a great synergy in being able to leverage those two things to become a very serious player at scale in the marketplace,” Liang said.</p><p>Thirdly, Intel and SambaNova are also working on a technology collaboration. An immediate combination of Intel Xeon CPUs plus SambaNova RDUs would seem like an obvious first step, but beyond that, Liang alluded only to “other things on the roadmap,” while declining to give further details.</p><p>“We need to figure out how to create seamless solutions that allow customers to take what they already have running on a CPU or GPU and run on this integrated platform seamlessly and very efficiently,” Liang said. “Those are the kind of things we want to do to unlock inference and the agentic market at scale.”</p><p>Per current industry trends for disaggregated inference, LLMs can be split into prefill and decode parts of the workload, with different characteristics. Some industry rumors suggest Groq’s accelerator could be used during the decode stage to complement Nvidia-accelerated prefill; it’s not hard to imagine a combination of a future Intel GPU plus SambaNova RDU operating as a disaggregated inference solution, but there’s been no suggestion from either company that this is what they’re working towards.</p><p>Intel’s statement to EE Times said the planned collaboration is intended to “give customers a powerful alternative to GPU-centric solutions.”</p><p><a href=\"https://newsroom.intel.com/data-center/intel-and-sambanova-planning-multi-year-collaboration-for-xeon-based-ai-inference\">Intel’s blog on the announcement</a> didn’t explicitly mention its Gaudi AI accelerator product line, which directly competes with SambaNova’s inference offering today. Intel’s reportedly planning to merge Gaudi IP into its data center GPUs going forward (the first part in this series is codenamed Jaguar Shores). It’s even less clear what the Intel-SambaNova partnership will look like beyond a future Jaguar Shores launch.</p><p>“The combination of Intel CPUs and SambaNova’s AI platform can provide a compelling rack-level inference option as Intel’s GPU-based solutions come online,” Intel’s blog said. “This collaboration complements Intel’s existing data center GPU commitments and does not alter its path forward to competing in AI.”</p><p>“[Intel] continues to invest across GPU IP, architecture, products, software, systems, and strengthen its roadmap as part of its edge-to-cloud AI engagements,” according to the blog.</p><p>New Intel CEO Lip-Bu Tan faced criticism when it looked like Intel would acquire SambaNova, since his venture capital firm had invested in SambaNova and he’s also the chair of SambaNova’s board. In a statement to EE Times, Intel clarified that Tan recused himself from this process with Intel’s new data center chief Kevork Kechichian acting as executive sponsor for the deal. Intel’s board “believes it’s important that Intel fully leverage [Tan’s] vast network,” the statement said.</p><h3 class=\"wp-block-heading\"><strong>New chip</strong></h3><p>SambaNova also announced its fifth chip, the SN50, billed as 5× faster than competitive chips and 3× cheaper than GPUs for agentic AI. Japan’s SoftBank will be the first customer to deploy the SN50.</p><p>The SN50 is designed for LLM token generation in multi-agent workflows. Agentic applications with lots of custom agents have very, very low latency requirements, which become cost prohibitive for businesses to run, Liang said.</p><p>“SN50 can hit this golden zone where we’re providing latency and throughput but the service provider’s tokenomics is in the right place,” Liang said. “We’ve seen this in the last couple of years, where you are either really, really fast, but the service provider cannot monetize it profitably, or you can batch it like crazy for good economics but there’s a delay to the end user. SambaNova’s customers can turn this into a proper profitable inference service.”</p><p>The new focus on agentic inference meant several significant changes versus the SN40L, Sumti Jairath, chief architect at SambaNova, told EE Times.</p><p>The SN50 has the same dataflow architecture as the SN40L, SambaNova’s previous LLM inference accelerator, with the same three-tier memory hierarchy. Compute per chip has been increased, support for lower precision math added, and there’s a new proprietary interconnect protocol that further enables scale-out. All of these changes drive interactivity and economics for fast tokens in agentic systems.</p><p>Agentic systems need to call multiple models but these models are usually spread across different chips and different systems. Today, this means standing up multiple racks with agents on individual machines, which is very inefficient since most of the hardware is effectively idling while specific agents are called. This is the limiter for today’s agentic systems, Jairath said.</p><p>“The way we see the future is that everybody wants all the models and parameters sitting on the same machine so they can call whatever model they want to interact with,” he added.</p><p>SambaNova’s three-tier memory hierarchy supports what it calls “agentic caching,” running multiple models while serving many requests with prompt caching.</p><p>The SN50 has the <a href=\"https://www.eetimes.com/sambanova-adds-hbm-for-llm-inference-chip/\">same memory as the SN40L</a>: 1.5 TB DDR5 DRAM, 520 MB SRAM (across both compute chiplets), and 64 GB HBM3.</p><p>“The way we use HBM is different from how Nvidia uses HBM,” Liang said. “Because we have a big DRAM sitting right next to it, we don’t use [HBM] purely as capacity, we use it as a hot cache. We’re doing N minus one generation of HBM because of cost and supply availability. We don’t want to be fighting for HBM with the latest GPUs. We are able to achieve the performance with N minus one, so we can achieve the economics we want to achieve.”</p><p>GPUs have notoriously poor memory bandwidth utilization for HBM, which Jairath puts down to “launching new kernels and synchronizing things.” Good utilization for SambaNova is close to 90%, he added.</p><p>The SN50 will have 2.5× more compute at 16-bit compared to the SN40L, and will add support for 8- and 4-bit.  </p><p>“We increased the compute for the SN50, but we still had headroom on the memory side, both in terms of capacity and bandwidth, so it works out perfectly fine with the higher amount of compute,” Jairath said.</p><p>A new interconnect protocol will enable scale-out beyond the 16 sockets the SN40L allowed. Per the new protocol, 256 sockets can share the same memory space, Jairath said. (Nvidia Blackwell GPUs can be in domains of up to 72.)</p><p>“It’s pretty much the standard Ethernet SerDes, but with our proprietary protocol, we get both direct connectivity among RDUs in the same rack, and we use similar links for rack-to-rack connectivity,” Jairath said. “This allows us to scale up and scale out resources, depending on what configurations we want to build.”</p><p>Evolving workloads mean flexibility in system design is valuable, Jairath said. Ethernet’s ubiquity gives the company more choice in network and switching hardware versus a fully custom solution, and it allows customers to integrate SambaNova racks easily. Layer 2 switching uses standard Ethernet components, Jairath said.</p><p>The new interconnect scheme provides low latency access to SRAM across up to 256 chips. While <a href=\"https://www.eetimes.com/as-nvidia-stock-drops-15-has-the-ai-chip-bubble-finally-burst/\">DeepSeek famously had to work at the instruction set level</a> to overlap compute and data transfer for Nvidia GPUs, with dataflow architectures, this overlap is an inherent feature, Jairath said.</p><p>“Low latency access to SRAM across chips makes it possible to cache big models, so there is no upper limit to token speed,” he said. “Techniques like tensor parallel, pipeline parallel, expert parallel—the basis of these techniques is having low latency access to that memory. You can build any form of parallelization easier than you could with other architectures.”</p><p>SN50, and rack-scale systems featuring the SN50, will ship in the second half of 2026.</p> </div>",
            "pub_date": "2026-02-26 21:45:24",
            "link": "https://www.eetimes.com/sambanova-abandons-intel-acquisition-raises-funding-instead/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Startup Wants to Speed Up Chip Design With AI in RTL to GDSII Path",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>A London, U.K.-based startup has emerged from stealth with a mission to reduce the time it takes to design next-generation chips from years to weeks by using AI in the process of converting system RTL to GDSII. Tattvam AI, which has received $1.7 million from early investors, including veteran entrepreneur Stan Boland, has begun initial discussions with customers and plans to raise a larger round later this year.</p><p>In a briefing with EE Times, co-founder Bragadeesh S. emphasized that the chip design process is slow despite the billions of dollars spent on global R&amp;D. He said that parts of the physical design process (in which you convert the RTL code created by design tools into the final GDSII file for tape-out) can take months at a time, each with several iterations, resulting in two-to-three-year design cycles for complex chips.</p><p>Physical design involves everything from floor planning, placement, and routing to clock tree synthesis (CTS), power planning, and design rule checking. According to Bragadeesh, even seemingly simple tasks like timing closure and CTS in physical design can take months due to iterative loops. Aligning communications between different teams working on different parts of the physical design slows down the entire process even more.</p>\n<p>Bragadeesh said that Tattvam AI is addressing this by building a local AI system that can understand the circuit and suggest actions and insights in just a few hours based on design goals. He calls it the intelligence layer for chip design, where Tattvam AI is training an AI tool to understand circuits from first principles and autonomously solve the design problems that today consume years of expert engineering time.</p>\n\n<p>He said that physical design is one of the most labor-intensive stages of the chip development process, in which engineers manually navigate thousands of interdependent decisions around placement, routing, and timing, with each iteration taking weeks. Tattvam AI aims to build a reasoning engine to eliminate this bottleneck, with the ambition of compressing processes that currently take years into a matter of weeks.</p>\n<p>“Chip design is fundamentally a reasoning problem over an enormous search space, not unlike the kind of reasoning that’s needed to solve hard problems in mathematics. Current AI tools, even the most advanced LLMs, struggle with the deep structural understanding that chip design demands,” Bragadeesh said in prepared remarks. “We’re building a reasoning model that actually understands circuits from first principles—the constraints, the tradeoffs, the interdependencies—the same way a world-class engineer would, and doing it in a fraction of the time.”</p><p>He told EE Times that the tool is expected to launch in a few months, but wasn’t specific as to what will be included at launch. “We have identified a specific wedge in physical design, where the problem structure is relatively well defined, and engineers currently spend a lot of time,” he said. “I can hint that the initial area will focus on timing closure-type challenges. I also think many techniques translate, that’s why we’ll look to scale from our wedge to multiple aspects of physical design after that. Our focus is to ensure the product integrates well within a customer environment, optimizing for AI thinking time and the customer’s data confidentiality requirements.”</p><p>When we met him in London a few weeks ago, you could see that Bragadeesh had real depth of knowledge and understanding of the challenge here, especially with several of the hyperscalers trying to do their own custom chips, and the bottlenecks they face in chip design. As is widely recognized in industry, designing a chip from scratch takes two to three years and costs tens of millions of dollars before a single one rolls off a fab.</p><p>Catching even a single design error in the later stages can mean months of delays and a $50 million respin. “The tools chip engineers use, called EDA software, handle the mechanics of design, but the hardest decisions still require expert engineers manually iterating through thousands of possibilities,” the company said in its press release.</p><p>There was emphasis in our discussion that Tattvam AI’s local AI system isn’t intended to compete with existing EDA tools from the major vendors, such as Synopsys or Cadence, but to dovetail into existing workflows. Where it will impact is the design services companies that employ hundreds of engineers to build the chips as a turnkey offering for ASIC design services, such as Alchip Technologies, Aion Technology (formerly Sondrel), and Ensilica.</p><p>On the technology itself, Bragadeesh said they use learnings from open-source AI to solve abstract reasoning tasks, such as ARC-AGI, to build an AI tool that deeply understands a given circuit and performs tasks like a human engineer. The company uses synthetic datasets inspired by ARC-AGI benchmarks and mathematical theorems to train domain-specific models at low cost. This enables the replication of real chip tape-out processes and the generation of high-quality training data.</p><h3 class=\"wp-block-heading\"><strong>Still early on its journey, but strong conviction and backing</strong></h3><p>The company just came out of stealth this week and announced a $1.7 million investment, but has yet to launch its product. So why should we take note of a two-person startup so early on?</p><p>Well, as most investors say, one key characteristic is the founders’ passion. Co-founder Bragadeesh certainly oozes passion and energy. He may be young, but after graduating from IIT Madras in India and stints at Texas Instruments and Imagination Technologies, he then built a compute device to process brain signals in real time at <a href=\"https://www.comind.io/\" rel=\"noreferrer noopener\" target=\"_blank\">CoMind</a>, went on to be a member of the technical staff at <a href=\"https://www.fractile.ai/\" rel=\"noreferrer noopener\" target=\"_blank\">Fractile</a>, and then turned down a role at Google’s TPU team to start Tattvam AI.</p><p>Second is the backing, with pre-seed funding of $1.7 million from Seedcamp, EWOR, Entropy Industrial Capital, Concept Ventures, and semiconductor angel investor and entrepreneur <a href=\"https://www.youtube.com/watch?v=A3dAU7Py7_A\" rel=\"noreferrer noopener\" target=\"_blank\">Stan Boland</a>.</p><p>“Bragadeesh is one of the most driven, energetic, and compelling young founders in today’s chip industry,” Boland said in the company’s press release. “His conviction that Tattvam AI will dramatically speed up the complex and iterative process of using EDA tools and models to design chips, cutting timelines from years to weeks, is sure to be embraced by the world’s top teams.”</p><p>You’d expect this in the official company announcement, but Daniel Dippold echoed this. “Bragadeesh is one of those rare founders who reasons from atoms up: He turned down Google’s TPU team because he’d rather build the infrastructure that makes the next decade of compute possible,” Dippold said. “When we met him, he’d already shipped across brain-computer interfaces, 5G, and AI silicon. We see thousands of founders a year at EWOR. Very few of them make you feel like the market is simply going to bend around them. He’s one of those.”</p><p>The company currently consists of Bragadeesh and his co-founder, Lannan Jiang, who previously worked on high-performance chip development at ETH Zürich. However, Bragadeesh told EE Times that Tattvam AI is hiring and expects to have five people working full time on this mission by the end of 2026.</p><p>Following the end of our chat, we asked what the meaning was behind the company name, Tattvam AI. Bragadeesh told EE Times, “My interpretation of the word Tattvam is that it means the ‘fundamental principle of things.’ The company’s origin was based on the intuition that if you can build an AI model that understands the structure of the problem you’re solving from first principles, there’s a wide variety of applications. For example, <a href=\"https://harmonic.fun/index\" rel=\"noreferrer noopener\" target=\"_blank\">Harmonic</a> is applying similar thinking in AI for math. We’d like to bring a similar approach to chip design, which is essentially a collection of puzzles. We’re building an AI that can reason through the structure of the circuit from first principles and solve physical design puzzles.”</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><h5 class=\"wp-block-heading\">See also:</h5> </div>",
            "pub_date": "2026-02-26 16:35:19",
            "link": "https://www.eetimes.com/startup-wants-to-speed-up-chip-design-with-ai-in-rtl-to-gdsii-path/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Supply Chain Leaders See Agentic AI Curbing Entry-Level Hiring",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>The integration of AI into global supply chains is poised to sever the traditional entry-level roles of the corporate ladder as companies move beyond simple automation toward autonomous decision-making systems.</p><p>A report released this week by Gartner indicates that the shift toward agentic AI—systems capable of taking independent action to achieve goals—is fundamentally altering workforce strategies.</p><p>More than half (55%) of the 509 supply chain leaders surveyed expect advancements in agentic AI to reduce their need to hire for entry-level positions. Furthermore, 51% of respondents believe the technology will drive a shift toward overall workforce reductions.</p>\n<p>The findings highlight a deepening bifurcation in the corporate world. As geopolitical tensions and economic shifts pressure margins, companies are turning to machines not just to analyze data, but to execute tasks previously reserved for junior analysts and logistics coordinators.</p>\n\n<h3 class=\"wp-block-heading\">Rise of the autonomous workforce</h3><p>The survey, conducted between July and October 2025, identifies the advancement of AI as the single most influential driver redefining supply chain strategy over the next two years. It outpaces other significant pressures, including economic policy, de-globalization, and evolving customer expectations.</p>\n<p>Unlike earlier waves of digitization that focused on visibility or simple process automation, agentic AI represents a leap toward systems that can negotiate, procure, and schedule logistics with minimal human oversight. This capability is leading executives to rethink the composition of their teams.</p><p>However, analysts warn against viewing this transition solely as a cost-cutting mechanism. Marco Sandrone, VP analyst specializing in supply chain operations at Gartner, cautioned that successful organizations are distinguishing themselves by how they deploy these tools.</p><p>“The highest performing supply chain organizations are using AI to reinvent how work gets done and how talent is developed. They are not treating AI as a blunt instrument for headcount reduction,” Sandrone said. “The priority for chief supply chain officers should be redesigning roles, skills, and workforce processes, so people and machines can create value together.”</p><h3 class=\"wp-block-heading\">Redefining the talent pyramid</h3><p>The report suggests that the traditional pyramid structure of supply chain organizations—broad at the bottom with junior execution roles—is eroding. In its place, a more diamond-shaped structure may emerge, prioritizing mid- to senior-level employees capable of managing AI outputs and complex stakeholder relationships.</p><p>Gartner’s analysis found that 86% of respondents agree that the adoption of agentic AI will require entirely new processes for developing future talent pipelines. If entry-level roles are automated, the industry faces a long-term challenge: How to train the next generation of senior leaders who historically learned the ropes through the very operational tasks now being automated.</p><p>Sandrone noted that while junior roles as understood today may fade in importance, supply chains will still need emerging talent that is highly adaptive. He argues that organizations must identify and attract talent capable of sustaining new working models, which include reskilling current staff to take on higher-value roles.</p><h3 class=\"wp-block-heading\">Productivity gap</h3><p>The data reveals a stark confidence gap between industry leaders and their peers. Gartner identified a cohort of high-performing organizations—those exceeding expectations in revenue growth, lead times, and sustainability. Among respondents who view AI-driven changes as a top transformation driver, 81% of these leaders expressed confidence in their ability to address their impact. In contrast, only 54% of other respondents reported the same level of confidence.</p><p>These high-performing organizations are adopting agentic AI at significantly higher rates across procurement, logistics, and planning. Crucially, they are redesigning the work itself rather than simply layering technology on top of existing inefficiencies. The report notes that organizations that redesign work are twice as likely to exceed revenue goals and achieve sustainable competitive advantage compared to those that do not.</p><p>The divergent approaches to workforce investment are evident in the data. While cost reduction remains a factor, high-performing leaders are prioritizing reinventing their talent strategy. Sixty percent of leading organizations are prioritizing upskilling talent for the AI era, compared to just 46% of other respondents.</p><h3 class=\"wp-block-heading\">Machine-to-machine commerce</h3><p>Beyond internal workforce dynamics, the rise of agentic AI is reshaping how companies interact with their trading partners. As supply chains become more automated, the need for strategic collaboration with suppliers increases. The report indicates that 76% of supply chain leaders have already implemented or plan to implement capabilities to automate interactions with “machine customers”—non-human economic actors that independently negotiate and purchase goods.</p><p>This shift necessitates a realignment of trading partner relationships. Routine interactions are increasingly automated, freeing up capacity for human professionals to focus on designing out supply chain complexity through product and material innovation. The report advises leaders to support strategic trading partners in developing the digital capabilities needed for machine-enabled collaboration, effectively building a digital ecosystem where data structures and incentives are aligned.</p><p>As the industry marches toward 2028, executives’ consensus is that the status quo is untenable. With 88% of leaders utilizing agentic AI in procurement alone, the supply chain sector is rapidly becoming a testing ground for the broader integration of autonomous intelligence in global business.</p><p>“As organizations identify new ways of working through the use of AI, they will also have an advantage in identifying and attracting the kinds of talent that will sustain these new working models,” Sandrone said.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><h5 class=\"wp-block-heading\">See also:</h5><p><a href=\"https://www.eetimes.com/supply-chain-and-digital-sovereignty-face-ai-triple-threat/\" rel=\"noreferrer noopener\" target=\"_blank\">Supply Chain and Digital Sovereignty Face AI Triple Threat </a></p><p><a href=\"https://www.eetimes.com/agentic-ai-as-key-to-autonomous-supply-chains/\" rel=\"noreferrer noopener\" target=\"_blank\">Agentic AI as Key to Autonomous Supply Chains</a></p><p><a href=\"https://www.eetimes.com/supply-chain-leaders-must-confront-divergence/\" rel=\"noreferrer noopener\" target=\"_blank\">Supply Chain Leaders Must Confront Divergence</a></p> </div>",
            "pub_date": "2026-02-25 22:31:15",
            "link": "https://www.eetimes.com/gartner-supply-chain-leaders-see-agentic-ai-curbing-entry-level-hiring/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "India AI Impact Summit: From IT Services to Sovereign AI and Silicon",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>For a long while now, India has been the world’s back office for coding and software services. Now, with AI writing code, debugging systems, and automating workflows, the country’s IT industry dreads an existential crisis. Investors are treading cautiously, companies are recalibrating hiring plans, and layoffs are beginning to signal structural shifts. AI is likely to compress lower-value IT tasks. Pressure is building on traditional services models, and demand for repetitive coding roles is set to decline.</p><p>At the India AI Impact Summit, Infosys Chairman Nandan Nilekani captured this tension and warned, “The resentment of the blue-collar workers led to the train wreck of globalization. The resentment of the white-collar worker is going to lead to the train wreck of artificial intelligence.”</p><p>The India AI Impact Summit unfolded in New Delhi as a policy response to the very disruption reshaping India’s software economy. From sovereign large language models and chip design incentives to subsidized AI compute and telecom modernization, the government signaled that India intends to move up the value chain—from coding services to foundational AI infrastructure.</p>\n<h3 class=\"wp-block-heading\"><strong>Sovereign AI infrastructure</strong></h3><p>If AI is compressing low-value coding work, control over the model layer becomes strategic. Prime Minister Narendra Modi launched the latest model releases of BharatGen, which was described by Union Minister for Science and Technology Jitendra Singh as a government-owned sovereign multilingual and multimodal large language model initiative.</p>\n\n<p>BharatGen isn’t a single model release. It’s a coordinated national AI stack whose architecture spans multiple modalities. The latest releases include Param-2, a 17-billion-parameter text foundation model trained across 22 scheduled Indian languages. The stack also includes Shrutam, a speech-to-text model in 12 Indian languages; Sooktam, a text-to-speech model; and Patramunder the DocBodhframework for multilingual document understanding.</p>\n<p>BharatGen operates through a consortium led by the TIH Foundation for <a href=\"https://www.embedded.com/ambient-iot-enables-real-time-asset-tracking\">IoT</a> and IoE at IIT Bombay, alongside partner institutions including IIIT Hyderabad, IIT Hyderabad, IIT Mandi, IIT Kanpur, IIM Indore, and IIT Madras. The Department of Science and Technology supports it with ₹235 crore (~$25 million) under the National Mission on Interdisciplinary Cyber-Physical Systems. It’s further strengthened under MeitY’s ₹10.585 crore (~$1.16 million) India AI Mission outlay.</p><p>Singh emphasized that BharatGen’s defining feature is sovereignty. The models are government-owned and the datasets are curated domestically. The initiative has transitioned into a Section-8 company, the BharatGen Technology Foundation, to operate at national scale. Data and model governance will be anchored through initiatives like Bharat Data Sagar.</p><p>For engineers, the significance lies in stack control. BharatGen supports multimodal pipelines. It enables Indian-language tokenization at scale as well as domain-specific fine-tuning, including Ayur Param, Agri Paramand Legal Param. It also enables deployment ready platforms designed for governance, healthcare, agriculture, and legal systems.</p><h3 class=\"wp-block-heading\"><strong>Design-led semiconductor strategy</strong></h3><p>Parallel to AI infrastructure, the government is reshaping semiconductor policy through the Design Linked Incentive (DLI) scheme. The focus isn’t fabrication—it’s design. Semiconductor design can contribute up to 50% of value addition and account for 15 to 35% of bill-of-materials cost. It also anchors architectural control.</p><p>India already hosts major global design centers. The DLI strategy aims to convert that service strength into IP ownership.</p><p>Vervesemi Microelectronics offers a working example. The fabless startup raised $10 million in a Series A funding round co-led by Ashish Kacholia and Unicorn India Ventures. The company has built a portfolio of more than 140 semiconductor IPs, developed 25 IC variants, and secured 10 patents. It has completed multiple tape-outs at UMC and TSMC.</p><p>Its chips include a BLDC controller built on an indigenous RISC-V core, an avionics data acquisition chip, an energy metering chip, and a motor control chip for electric vehicles and drones.</p><p>Indian Union Minister for Electronics and IT Ashwini Vaishnaw framed the transition from services to silicon bluntly. “India must now evolve into a product nation,” he said. He called for an incremental development approach spanning the full technology spectrum.</p><p>The shift is clear—from outsourcing to silicon ownership, from design centers to chip companies, and from licensing IP to building domestic RISC-V cores.</p><p>For engineers, the ripple effects are tangible. RISC-V acceleration gains momentum. Analog design converges with machine learning workloads. Edge AI silicon becomes a domestic opportunity. Defense- and avionics-grade chip design moves into sharper focus.</p><h3 class=\"wp-block-heading\"><strong>Subsidizing compute, not data centers</strong></h3><p>India’s also chosen a different economic lever for AI growth. Earlier last week, <a href=\"https://www.eetimes.com/india-to-add-20000-gpus-as-ai-mission-2-0-expands-compute-and-chip-push/\">EE Times reported</a> that  India will expand its AI compute capacity beyond 38,000 GPUs, adding 20,000 more units in the coming weeks, available at a subsidized rate of ₹65 per hour (approximately 72 cents per hour).</p><p>Rather than subsidizing GPU data centers directly, the Indian government is subsidizing compute access. MeitY Secretary S. Krishnan stated that AI compute in India is available at roughly one-third the cost paid elsewhere in the world.</p><p>That pricing strategy reshapes the ecosystem. It underwrites research markets, lowers entry barriers for startups, and avoids infrastructure lock-in. It encourages private data center investments without distorting ownership structures.</p><p>At the telecom layer, AI is becoming embedded inside network orchestration. Policy focus areas include AI-native network management, Open RAN, 6G, AI-driven cybersecurity, and non-terrestrial networks.</p><p>The hardware implication is structural. AI is no longer an overlay. It is becoming part of the network control plane.</p><h3 class=\"wp-block-heading\"><strong>Global commitments and deployment signals</strong></h3><p>Global and domestic AI enterprises, including OpenAI, Google DeepMind, Microsoft, Anthropic, Meta, Cohere, G42, and Sarvam, participated in the summit’s inauguration. Two commitments emerged: research into real-world AI usage, and multilingual, contextual model evaluation.</p><p>India is pushing for benchmarks that reflect the Global South. It demands cultural adaptation and multilingual safety standards.</p><p>On the exhibition floor, that ambition moved from policy to hardware. Humanoid robots, robotics processors, AI-ready high-performance computing infrastructure, industrial automation systems, smart agriculture robotics, and fintech AI security platforms filled the expo.</p><p>AI is no longer confined to cloud software. It’s becoming an industrial layer moving onto manufacturing floors, entering warehouses. It’s even appearing in public distribution systems and healthcare diagnostics in India.</p><p>India clearly doesn’t want to remain a peripheral IT services player. It wants to operate as a system-level sovereign technology power. However, to achieve that futuristic goal, speakers at the summit identified the need to establish offline AI capability and independence from internet connectivity. These requirements reflect operational realities in mission critical systems, such as space systems, defense, satellite imagery, and geospatial analytics.</p><p>They also pointed out the need for explainable AI, version control, data lineage tracking, and regulatory-aligned machine learning pipelines in public systems. While government adoption frameworks stress model drift detection, third-party audits, scientific validation, and due diligence before beneficiary targeting, it remains an open question whether this sovereign AI architecture will withstand the pressures of deployment and accountability.</p> </div>",
            "pub_date": "2026-02-25 13:31:07",
            "link": "https://www.eetimes.com/india-ai-impact-summit-from-it-services-to-sovereign-ai-and-silicon/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "India Summit Champions Global South AI Inclusion",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>Last week, the India AI Impact Summit was hailed by political leaders and tech CEOs as a landmark event in bringing minds, technology, and talent together to enable a global common good for AI, as well as to enable democratization and inclusion for the global south in the AI story, whether related to cost of compute or making ubiquitous edge intelligence available for the masses. The event also covered “human-centric” ethical AI guided by global cooperation and governance.</p><p>For example, the United Nations Secretary General António Guterres emphasized in his opening speech, “The future of AI cannot be decided by a handful of countries—or left to the whims of a few billionaires.” He said that clearly with the likes of Sam Altman in the auditorium.</p><p>President of the French Republic Emmanuel Macron gave a heartfelt speech on how he thought India was really shining in making technology available for the masses, highlighting the universal digital identity infrastructure India has deployed among 1.4 billion people, and the universal payments system that brings even street vendors without bank accounts into the national economy.</p>\n<p>Following this, on India’s potential for AI inclusion, Macron added, “The smartest AI is not the most expensive. It is the one built by the best people and for the right purpose. Models, infrastructure, talents, capital, and adoption. This is where the Indian model is truly revolutionary, providing solutions for everyone in the country. From 200 million of Indian farmers in their own dialect, to travel advice for 400 million of pilgrims, or AI diagnostics for rural clinics, all running on India’s digital public infrastructure. Open rates, near zero cost, adoption is key. And being inclusive is key.”</p>\n\n<h3 class=\"wp-block-heading\"><strong>On-the-ground reality, plus OpenAI-Anthropic rivalry</strong></h3><p>While AI inclusion, extremely low-cost compute, and edge intelligence for the masses with hundreds of billions of dollars pledged from global tech CEOs visiting India was the message inside the auditorium, what I heard was a tale of two events for two different Indias. Despite me having to cancel my attendance at the last minute due to unforeseen circumstances, I nevertheless followed the event from afar via recordings, social media, and conversations with people after the event.  </p>\n<p>What we heard was, on the one hand, frustration over restricted access to the conference and exhibition whenever the Prime Minister attended. On the other hand, there was a flurry of productive activity on the trade show floor during the two days when he wasn’t there.</p><p>But the overarching feeling among attendees was the poor management, communication, and bureaucracy of staff for the days of the event when the PM was attending. I heard stories of people being turned away from the keynote auditorium despite being assured the day before that they could. Many ended up watching livestream from nearby cafés and conducting meetings off-site, which really goes against the grain of having an event to bring everyone together.</p><p>Some of the CEOs I spoke to told me anecdotally and mused that you could only get in to the keynote talks, specifically on Thursday, if you had direct contact with government ministers, or if you were from the Bollywood (India’s version of Hollywood) or cricket fraternity, as India thrives on these two—fame and celebrity often gets you access over merit.</p><p>And you only had to scour social media to hear all the stories of people being turned away, of no Wi-Fi access, digital payments not working in many areas, and even the Indian Union Cabinet Minister for Electronics and Information Technology Ashwini Vaishnaw admitted the “challenges” in his official press conference.</p><p>It felt kind of ironic that on the one hand the political leaders and CEOs were talking about India championing the democratization of AI and inclusion of the global south, while excluding the masses who were trying to take part in the event on some of the days.</p><p>Also, another bit of controversy was seen at the end of the keynote speeches on Thursday, when Prime Minister Narendra Modi called all the tech leaders on stage to hold hands in the air celebrating the unanimity of thought—only to see OpenAI’s Sam Altman and Anthropic’s Dario Amodei apparently refusing to hold hands.</p><h3 class=\"wp-block-heading\"><strong>India has a real opportunity on leveraging AI</strong></h3><p>Despite the troubles we heard about at the India AI Impact Summit—similar, incidentally, to our experiences with Semicon India over the last two years—there’s an upside that people don’t talk about. This was probably best captured by Silicon Valley author, academic, and entrepreneur Vivek Wadhwa in an article <a href=\"https://www.hindustantimes.com/opinion/talent-in-the-ai-age-code-coolies-to-core-engineers-101771779044194.html\">published in India’s Hindustan Times</a> newspaper this week, arguing that AI will provide India with the tools to build a future that leverages its core engineering talent, whether it’s in electrical, civil, mechanical, or biotech.</p><p>Wadhwa quotes V Kamakoti, director of IIT Madras, who said at the India AI Impact Summit that AI disruption was a blessing in disguise that would return engineers to their core skills rather than resorting to generic IT roles to fix and build code for global companies using India for cheap software engineering resources. For most of the last few decades, he said, India’s engineers have turned to IT and coding just because that’s where the money was—even if their skills were in another discipline.</p><p>He notes that as AI automates coding, testing, documentation, and maintenance, core engineers will have stronger incentives to remain in their disciplines, reskill with AI, and apply these to real world problems in infrastructure, manufacturing, energy, and medicine.</p><p>In my personal opinion, there’s a real opportunity for India with AI in the same way that mobile telephony changed India in the mid-1990s. In that era, India leapfrogged from standard fixed telecommunications lines to 2G mobile networks, and now the country is one of the cheapest in the world for mobile data.</p><p>In the 1990s, the promise of mobile phones for farmers was that they would be able to check weather apps or, later in history, prices to help optimize their own efforts and growth. At the India AI Impact Summit, we heard how the availability of low-cost edge intelligence, with open access, could make that much more useful, and transform almost every industry.</p><p>I recall being told on many occasions in India that any technology tested and deployed in India could easily be deployed anywhere in the world. That is a result of Indian developers being able innovate in order to solve real world problems, test on hundreds of millions of users or customers, and then refine the business model to find one that works for an extremely price-sensitive market.</p><p>As the cellular operators in India eventually realized in the early 2000s, even on margins of just cents, there was significant profitability, as companies like Reliance now benefit through its Jio operation, probably India’s largest telecom network operator. Maybe this is why all the global tech big guns were in India for the India AI Impact Summit—for the rich pickings that India potentially offers through deployment and monetization of real-world AI applications on a huge population already geared up to take advantage of whatever that might offer.</p><h5 class=\"wp-block-heading\">See also:</h5><p><a href=\"https://www.eetimes.com/india-to-add-20000-gpus-as-ai-mission-2-0-expands-compute-and-chip-push/\">India to Add 20,000 GPUs as AI Mission 2.0 Expands Compute and Chip Push</a></p><p><a href=\"https://www.eetimes.com/india-deep-tech-alliance-pledges-2-5b-in-investment/\">India Deep Tech Alliance Pledges $2.5B in Investment</a></p><p><a href=\"https://www.eetimes.com/fabless-startup-aheesa-tapes-out-first-indian-risc-v-network-soc/\">Fabless Startup Aheesa Tapes Out First Indian RISC-V Network SoC</a></p><p><a href=\"https://www.eetimes.com/indian-chip-design-services-provider-mirafra-tapes-out-22-nm-soc/\">Indian Chip Design Services Provider Mirafra Tapes Out 22-nm SoC</a></p><p><a href=\"https://www.eetimes.com/what-makes-mmwave-practical-for-indias-5g-engineering/\">What Makes mmWave Practical for India’s 5G Engineering</a><a href=\"https://www.eetimes.com/indias-deep-tech-story-is-slow-not-stagnant/\">India’s Deep Tech Story Is Slow, Not Stagnant </a></p> </div>",
            "pub_date": "2026-02-24 21:16:51",
            "link": "https://www.eetimes.com/india-summit-champions-global-south-ai-inclusion/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Semidynamics Becomes 3-nm Ready, Moves Europe Toward Hardware Sovereignty",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>Semidynamics announced its expansion into full-stack AI infrastructure solutions and revealed that it has taped out its designs at 3 nm with TSMC. As a collaborator in various programs backed by the EuroHPC Joint Undertaking, the Barcelona, Spain-based company could mark a step forward for Europe’s semiconductor ambitions.</p><p>“We started as an IP company selling our processor, vector unit, and tensor unit designs,” Iakovos Stamoulis, CTO at Semidynamics, told EE Times. “We are now becoming a vertically integrated AI hardware supplier, where we use our already proven IP technology to create our own silicon products available in boards and racks in due course.”</p><h3 class=\"wp-block-heading\"><a></a><strong>3-nm ready</strong></h3><p>Semidynamics designs process-agnostic IP cores. The company’s IP cores, including Cervell NPU and Atrevido, are compatible with 5-nm and more mature process nodes. “To be competitive in today’s landscape, we need to use cutting-edge technology, such as 3 nanometers,” Stamoulis said. By taping out its designs with TSMC, Semidynamics is “testing all our technologies and ensuring we have a solution that is functional, operational, and within our power, performance, and area limits at 3 nm.”</p>\n<p>While the company hasn’t revealed a detailed timeline for its vertical integration and <a href=\"https://www.eetimes.com/crypto-mining-asic-goes-deep-sub-threshold-on-3-nm/\">3-nm tape-out</a>, the overriding implications of the announcement for European sovereignty are difficult to ignore.</p>\n\n<h3 class=\"wp-block-heading\"><a></a><strong>Is the push for AI cornering the EU’s digital sovereignty?</strong></h3><p>Europe’s chip independence remains uncertain amid the push to build AI factories, gigafactories, and supercomputers in Europe, as outlined in the EU’s AI Continent Action Plan. The continent accounts for only 4% of the global AI compute power, while it depends on others to fulfill its own demand [1]. The European Commission has pledged to have at least 13 operational AI factories by 2026 and up to five AI gigafactories [2]. Considering its world share of chip production is just under 10%, Europe might rush into these AI factories with non-sovereign hardware.</p>\n<p>“Reality check: A lot of GPU procurement for AI factories will probably be from the other side of the Atlantic, but we want to have European solutions—European silicon, European software, European racks—as part of these AI factories and gigafactories,” Stamoulis said, urging for Europe’s digital sovereignty. Yet the European HPC ecosystem is heavily reliant on non-EU chips; JUPITER, Leonardo, Lumi, and other European supercomputers deploy GPUs like Nvidia’s A100 Ampere and GH200 Grace Hopper superchips, while Finland’s Lumi is powered exclusively by 11,912 AMD MI250X GPUs.</p><p>Currently, Europe’s digital sovereignty strategy focuses on bolstering design capabilities and IP development within the region [3]. Public and private funding have aligned with this goal, supporting companies that are developing sovereign processors and accelerators for AI data centers and high-performance computing applications.</p><p>A notable example is France-based company SiPearl, which closed a €130 million (~$153 million) Series A to develop and produce its Rhea1 microprocessor—now powering Germany’s JUPITER supercomputer [1].</p><p>Semidynamics hasn’t publicly detailed its vertical integration plans, but unlike SiPearl, Semidynamics is likely to be using its own internal AI IP (core, vector, and tensor) for this development. The IP is not only fully European in origin, but also fully open, built on the RISC-V standard rather than the proprietary Arm architecture.</p><h3 class=\"wp-block-heading\"><a></a><strong>Open ISA may (better) keep pace with AI evolution</strong></h3><p>Stamoulis argued that choosing RISC-V has accelerated development by enabling access to a broad tooling and testing ecosystem. The open ISA advances collaboratively: compiler toolchains, simulators, and libraries are integrated upstream into LLVM, GCC, and the Linux kernel well before silicon chips. Semidynamics boasts delivering a beta RTL design and FPGA bitstream to customers ahead of first tape-out, thanks to RISC-V, allowing software development and hardware verification to run in parallel. Because AI and HPC workloads evolve rapidly, a proprietary ISA vendor’s approval cycle risks lagging behind industry needs. The industry’s demands and bottlenecks might have changed by then.</p><p>Stamoulis continued, “Adopting RISC-V is a relatively future-proof decision that allows our customers to develop tailored software for our hardware and benefit from continuous future optimizations provided by the broader RISC-V ecosystem.”</p><h3 class=\"wp-block-heading\"><a></a><strong>Resolving memory bottlenecks</strong></h3><p>Semidynamics claims it can “break the memory wall” with its proprietary memory subsystem for AI workloads. John Peddie Research describes the company’s memory approach as an “important differentiator,” citing the “increasing demand for inference solutions that can scale efficiently across real-world workloads and deployment environments.” Based on its Gazzillion Misses technology, Semidynamics claims to overcome significant memory bottlenecks.</p><p>“We combine a very large memory capacity with our Gazzillion Misses technology, which supports up to 128 simultaneous cache misses per core,” Stamoulis said. Typical processors support only 16 to 20 outstanding misses, which means they stall frequently when accessing large memory pools, requiring multiple connected GPUs to achieve similar performance.</p><p>“Gazzillion frees up the compute capabilities of the same GPU,” Stamoulis said. “Our innovation keeps a single chip continuously fed with data from very large memory. So, you don’t need multi-GPU configurations.” Fewer GPUs translate into lower infrastructure costs, a meaningful advantage as AI stack expenses continue to climb.</p><p>The pressure to rapidly keep pace with the accelerating demands of AI is intensifying. If Europe can better align public funding, sovereign IP development, and manufacturing ambitions within a coherent strategy, it may narrow the gap with U.S. and Asian competitors—not by replicating Silicon Valley or Taiwan’s model, but by carving out a distinctly European path to technological self-reliance.</p><h5 class=\"wp-block-heading\"><strong>References</strong></h5><ol class=\"wp-block-list\"><li><a href=\"https://www.seglerconsulting.com/europes-ai-gambit-an-in-depth-analysis-of-the-eurohpc-ai-factories-and-the-quest-for-digital-sovereignt%20fundingy\">https://www.seglerconsulting.com/europes-ai-gambit-an-in-depth-analysis-of-the-eurohpc-ai-factories-and-the-quest-for-digital-sovereignt fundingy</a></li>\n<li><a href=\"https://digital-strategy.ec.europa.eu/en/factpages/ai-continent-action-plan\">https://digital-strategy.ec.europa.eu/en/factpages/ai-continent-action-plan</a></li>\n<li><a href=\"https://www.ceps.eu/ceps-publications/eu-plans-for-ai-gigafactories-sanctuaries-of-innovation-or-cathedrals-in-the-desert/\">https://www.ceps.eu/ceps-publications/eu-plans-for-ai-gigafactories-sanctuaries-of-innovation-or-cathedrals-in-the-desert/</a></li>\n<li><a href=\"https://www.eurohpc-ju.europa.eu/supercomputers/our-supercomputers_en\">https://www.eurohpc-ju.europa.eu/supercomputers/our-supercomputers_en</a></li></ol> </div>",
            "pub_date": "2026-02-24 16:26:16",
            "link": "https://www.eetimes.com/semidynamics-becomes-3-nm-ready-moves-europe-toward-hardware-sovereignty/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "AI Impact Summit: From Minerals to Models, India Stakes Its AI Claim",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>Spanning critical minerals, semiconductor supply chains, sovereign compute infrastructure, and enterprise AI partnerships, India used the India AI Impact Summit 2026 in New Delhi to signal its intent to anchor itself across the entire AI value chain.</p><p>Agreements signed at the summit, including entry into the U.S.-led Pax Silica coalition, sovereign AI infrastructure partnerships with <a href=\"https://www.eetimes.com/openais-broadcom-pact-underscores-europes-strategic-chip-moment/\">OpenAI</a> and Tata Group, and enterprise collaborations involving Infosys and Anthropic, reflect a coordinated push spanning minerals, chips, compute, models, and governance.</p><p>Indian Union Minister for Electronics and Information Technology Ashwini Vaishnaw said investment commitments could bring more than $400 billion across the AI stack over the next two years.</p>\n<p>“Whatever investment commitments we have seen so far indicate that in the coming two years, we should see more than $400 billion worth of investment across the five layers of the AI stack,” Vaishnaw said.</p>\n\n<h3 class=\"wp-block-heading\"><strong>India joins Pax Silica coalition</strong></h3><p>On the sidelines of the summit, India signed a joint statement formalizing the “India-U.S. AI Opportunity Partnership” as a bilateral addendum to the Pax Silica Declaration, the U.S.-led initiative focused on AI and supply chain security.</p>\n<p>The documents were signed by Ministry of Electronics and Information Technology (MeitY) Secretary S. Krishnan, U.S. Ambassador to India Sergio Gor, and Jacob Helberg, U.S. Under Secretary of State for Economic Affairs, in the presence of Indian Union Minister Ashwini Vaishnaw and Michael Kratsios, director of the U.S. Office of Science and Technology Policy.</p><p>Pax Silica is the U.S. Department of State’s flagship effort to advance economic security among trusted partners, linking AI with critical minerals, semiconductors, advanced manufacturing, and energy systems. The declaration recognizes that AI is reorganizing global supply chains and that economic value will flow across the entire AI stack, from minerals and energy to compute, models and applications.</p><p>The declaration encourages coordination on software platforms, frontier foundation models, network infrastructure, semiconductors, logistics, mineral refining, and energy. It calls for reducing excessive dependencies, addressing non-market practices, strengthening investment security and protecting sensitive technologies.</p><p>In his keynote, Vaishnaw said cooperation under Pax Silica will deepen engagement on critical technologies and supply chain resilience under the India-U.S. Comprehensive Global Strategic Partnership.</p><p>Ambassador Gor said India’s entry into Pax Silica is “both strategic and essential,” adding that India contributes deep engineering talent, growing mineral processing capacity, and a strong trust factor to the coalition.</p><p>Other signatories include Australia, Greece, India, Israel, Japan, Qatar, the Republic of Korea, Singapore, the United Arab Emirates and the U.K. Non-signatory participants include Canada, the European Union, the Netherlands, the Organization for Economic Cooperation and Development, and Taiwan.</p><p>The India-U.S. AI Opportunity Partnership aims to promote pro-innovation regulatory approaches, strengthen the physical AI stack, and expand access to compute and advanced processors. It envisions joint R&amp;D, investment in next-generation data centers and support for AI developers and startups.</p><p>India Electronics and Semiconductor Association President Ashok Chandak called this a transformational opportunity for India. He said, “Pax Silica strengthens India’s digital sovereignty by ensuring secure access to semiconductors and AI infrastructure, which are now critical to defense, telecom, mobility, and economic growth. We are moving from being a large consumer of electronics to becoming a global design and manufacturing partner. With our strong semiconductor design talent, growing fabrication and packaging ecosystem, and large domestic demand, India can serve as the trusted bridge between East and West in this emerging alliance.”</p><p>India’s participation in Pax Silica aligns with its domestic push to secure critical minerals. The country recently secured lithium blocks in Argentina through Khanij Bidesh India. It has identified 5.9 million tons of lithium resources in the Indian Union Territory of Jammu and Kashmir and is negotiating mineral agreements with Australia, Namibia, and the Democratic Republic of Congo. These steps aim to reduce dependence on China, which supplies more than 90% of India’s rare earth imports.</p><p>The National Critical Mineral Mission aims to secure self-reliance across the value chain, from exploration and mining to processing and recycling. The mission includes plans for mineral parks, overseas asset acquisition, recycling initiatives, and a proposed center of excellence.</p><h3 class=\"wp-block-heading\"><strong>OpenAI, Tata Group announce sovereign AI infrastructure</strong></h3><p>Vaishnaw linked the investment momentum to India’s expanding semiconductor and engineering base. “Many global companies are looking at India as a trusted destination for engineering solutions and innovation. In semiconductors, global firms are already designing advanced chips and complete systems in India. That trend is continuing,” he said.</p><p>At the summit, OpenAI launched “OpenAI for India,” a nationwide initiative to broaden access to AI in partnership with leading Indian firms, starting with Tata Group. India now has more than 100 million weekly ChatGPT users, according to OpenAI.</p><p>As part of its global Stargate initiative, OpenAI will partner with Tata Group to build AI-ready data center capacity in India designed for data residency and security. OpenAI will become the first customer of Tata Consultancy Services’ HyperVault data center business, starting with 100 megawatts of capacity and with potential to scale to 1 gigawatt. With this partnership, OpenAI is securing local infrastructure that aligns with India’s data residency requirements and national policy priorities, while expanding its global compute footprint.</p><p>“This infrastructure will enable OpenAI’s most advanced models to run securely in India,” the company said.</p><p>Tata Group plans to deploy ChatGPT Enterprise across its workforce over the coming years, beginning with hundreds of thousands of Tata Consultancy Services employees. This also embeds OpenAI’s tools into one of the world’s largest IT services ecosystems, extending its reach to global enterprise clients. The partnership also includes certifications, education initiatives, and new OpenAI offices in Mumbai and Bengaluru.</p><p>For OpenAI, India represents market growth, infrastructure depth, and enterprise distribution in one of the world’s largest democracies.<strong></strong></p><p>Separately, Infosys announced a collaboration with Anthropic to deliver enterprise AI solutions in telecommunications, financial services, manufacturing, and software development. The partnership integrates Anthropic’s Claude models, including Claude Code, with Infosys Topaz AI offerings to automate complex workflows and support regulated industries.</p><p>Dario Amodei, CEO and co-founder of Anthropic, said, “There’s a big gap between an AI model that works in a demo and one that works in a regulated industry. If you want to close that gap, you need domain expertise.”</p><p>Salil Parekh, CEO of Infosys, said the collaboration will help enterprises modernize legacy systems and deploy agentic AI across complex operations.</p><h3 class=\"wp-block-heading\"><strong>Global deep-tech and governance partnerships</strong></h3><p>The partnership momentum extended beyond technology companies to governments. India and Finland agreed to collaborate across AI, quantum technologies, space, high performance computing, and the circular economy. In a sovereign deep-tech session with Finland, Prime Minister Petteri Orpo said, “Finland and India, together with international partners, will be able to lead the way in resilience, sustainability, and human-centric technological progress.”</p><p>With the U.K., India deepened cooperation on AI-native telecom networks, Open RAN, 6G, and quantum communications, alongside coordination at global standards bodies including the International Telecommunication Union and 3rd Generation Partnership Project. The two sides are operationalizing the <a href=\"https://www.gov.uk/government/publications/india-uk-connectivity-and-innovation-centre/india-uk-connectivity-and-innovation-centre\">India–U.K. Connectivity and Innovation Centre</a>.</p><p>Switzerland, Lithuania, and the Netherlands discussed AI governance frameworks centered on accountability, explainability, public sector responsibility and human rights-based approaches to AI regulation.</p><p>Estonian President Alar Karis emphasized the centrality of digital systems to governance. “Digital public infrastructure is no longer just a technical backbone. It is a foundation of how modern states operate,” he said.</p><p>Vaishnaw said that India is also working on guardrails. “Through our AI Safety Institute, which is a virtual institute, we are working with many academic institutions to create technical solutions to prevent harmful impact,” he said.</p><p>By aligning supply chain security through Pax Silica, expanding semiconductor capacity, anchoring hyperscale compute partnerships, and embedding frontier models into major IT services firms, India is tying industrial policy to commercial AI adoption. The strategy carries execution risks, particularly around capital deployment, energy supply and regulatory balance, but it also reflects an attempt to shape how AI value is created and distributed across the stack. For global technology companies and governments, India is signaling its intent to influence not just AI consumption, but the architecture of the next digital supply chain.</p> </div>",
            "pub_date": "2026-02-24 13:31:50",
            "link": "https://www.eetimes.com/ai-impact-summit-from-minerals-to-models-india-stakes-its-ai-claim/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Supreme Court Strikes Down Trump’s Tariff Regime",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>Today, the Supreme Court of the United States strongly rejected President Donald Trump’s protectionist economic agenda, ruling that the executive branch doesn’t have the authority to impose broad global tariffs under federal emergency powers.</p><p>In a 6-3 decision that unsettled global markets and the electronics industry, the Supreme Court ruled that the <a href=\"https://www.congress.gov/crs-product/R45618\" rel=\"noreferrer noopener\" target=\"_blank\">International Emergency Economic Powers Act</a> (IEEPA) doesn’t give the president the authority to impose import duties. This decision removes a key part of the administration’s “America First” trade strategy.</p><p>The ruling in <a href=\"https://www.supremecourt.gov/opinions/25pdf/24-1287_4gcj.pdf\" rel=\"noreferrer noopener\" target=\"_blank\"><em>Learning Resources, Inc. v. Trump</em></a> (No. 24–1287) immediately raises questions about the fate of hundreds of billions of dollars in collected duties. It also requires global supply chains, already disrupted by months of volatility, to adjust. While the decision gives American importers, especially in technology and semiconductors, some relief, administration officials have indicated they may use other legal tools to keep trade barriers in place.</p>\n<h3 class=\"wp-block-heading\">Constitutional rebuke</h3><p>Writing for the majority, Chief Justice John Roberts declared that the Constitution vests the taxing power exclusively in the legislative branch. The court rejected the administration’s argument that the statutory authority to “regulate […] importation” granted by IEEPA included the power to impose tariffs.</p>\n\n<p>“The Framers gave Congress alone access to the pockets of the people,” Chief Justice Roberts wrote, citing the <a href=\"https://guides.loc.gov/federalist-papers/full-text\" rel=\"noreferrer noopener\" target=\"_blank\">Federalist Papers</a>. He noted that while the president has broad powers to regulate foreign commerce, the administration’s reading of the statute would effect “a sweeping delegation of Congress’s power to set tariff policy—authorizing the President to impose tariffs of unlimited amount and duration, on any product from any country.”</p>\n<p>The majority invoked the “major questions” doctrine, reasoning that such a “transformative expansion of the President’s authority” required clear congressional authorization, which IEEPA doesn’t have. “We are therefore skeptical that in IEEPA—and IEEPA alone—Congress hid a delegation of its birthright power to tax within the quotidian power to ‘regulate,’” the Chief Justice wrote.</p><p>“It is a strong and overdue rebuke to a president who, from the day he retook power last January, has been ignoring the plain language of the constitution that he swore an oath to uphold,” <a href=\"https://www.ft.com/content/d00d0663-8716-4323-bda3-2cbfb5b30285\" rel=\"noreferrer noopener\" target=\"_blank\">wrote Brooke Masters</a>, U.S. managing editor of the Financial Times.</p><h3 class=\"wp-block-heading\">Supply chain relief and complexity</h3><p>For the global electronics and semiconductor industries, the ruling offers important but complex relief. The technology sector has been at the center of the trade war. The administration had previously <a href=\"https://www.eetimes.com/trump-tariffs-unleash-shift-in-global-economy/\" rel=\"noreferrer noopener\" target=\"_blank\">imposed a 34% reciprocal tariff on Chinese goods</a>, which later rose to 145% after retaliatory actions, and started investigations into semiconductor imports.</p><p>Supply chain managers have experienced “significant policy whiplash” due to the rapid pace of tariff announcements. Analysts predicted these changes could cost U.S. importers billions each year. The electronics market, with its “highly intricate and globally dispersed supply chains,” was especially at risk, since moving production from China is “astronomically expensive” and often “impractical” in the short term. </p><p>The ruling invalidates the IEEPA-based tariffs; industry experts warn that the supply chain disarray may not be resolved overnight. “Shifting things around is going to be quite complicated. That process will be slow, expensive, and challenging,” <a href=\"https://www.wsj.com/economy/trade/trump-tariffs-us-global-manufacturing-294b0f55\" rel=\"noreferrer noopener\" target=\"_blank\">noted Derrick Kam</a>, an Asia economist at Morgan Stanley, regarding the broader tariff environment.</p><h3 class=\"wp-block-heading\">Tech sector adjustment and refunds</h3><p>Furthermore, major tech companies like Apple have already begun exploring ways to diversify, such as moving some <a href=\"https://www.eetimes.com/china-stands-firm-amidst-tariff-chaos/\" rel=\"noreferrer noopener\" target=\"_blank\">iPhone production to India</a>. This trend may persist even after the importers’ legal win, due to ongoing geopolitical tensions. </p><p>A massive financial question hangs over the Treasury Department following the decision: the fate of the billions of dollars collected under the now-illegal framework. Estimates from the American Action Forum suggest that tariff <a href=\"https://www.americanactionforum.org/insight/the-supreme-court-strikes-down-ieepa/\" rel=\"noreferrer noopener\" target=\"_blank\">refunds could reach $175 billion</a>, creating a logistical and budgetary nightmare for the federal government.</p><p>The majority opinion did not explicitly address the mechanism for refunds, but the dissent acknowledged the looming chaos. “The United States may be required to refund billions of dollars to importers who paid the IEEPA tariffs,” Justice Kavanaugh wrote in his dissent, noting that the refund process “is likely to be a ‘mess,’ as was acknowledged at oral argument”.</p><p>Scott Lincicome, a trade expert at the Cato Institute, said that <a href=\"https://www.cato.org/news-releases/cato-experts-react-supreme-court-overruling-president-trumps-tariffs\" rel=\"noreferrer noopener\" target=\"_blank\">while the decision is good news for the economy</a>, the refund process will probably mean “more litigation and paperwork,” which puts a “particularly unfair burden for smaller importers that lack the resources to litigate tariff refund claims.”</p><h3 class=\"wp-block-heading\">Trump administration’s Plan B</h3><p>Even after this legal setback, the trade war is likely to continue. The Trump administration has found other legal ways to keep pressure on trading partners. The Supreme Court’s decision only addressed the use of IEEPA and did not affect other trade laws, such as Section 232 of the Trade Expansion Act.</p><p>“Make no mistake: This ever-protectionist administration will explore other trade statutes to impose more reckless tariffs,” <a href=\"https://www.cato.org/news-releases/cato-experts-react-supreme-court-overruling-president-trumps-tariffs\" rel=\"noreferrer noopener\" target=\"_blank\">warned Clark Packard</a>, a research fellow at the Cato Institute. Indeed, the administration has already utilized Section 232 to impose 50% tariffs on steel and aluminum and has initiated investigations into semiconductors and pharmaceutical manufacturing equipment under the same authority.</p><p>The ruling means the White House must now use slower, more formal processes to carry out its trade plans instead of acting quickly through emergency powers. “The Court has done its job, reminding the executive branch that unilaterally declared emergencies cannot erase the separation of powers,” Packard said.</p><h3 class=\"wp-block-heading\">Global economic resilience</h3><p>Internationally, the ruling may ease some immediate tensions, but the ongoing rivalry with China continues. Beijing has stressed its “economic resilience” amid the trade dispute, with Chinese officials confident they can better handle economic challenges than the U.S. The IEEPA tariffs had reached a total rate of 145% on most Chinese imports before the court stepped in.</p><p>However, the “Plan B” approach shows that targeted actions against specific sectors, especially semiconductors, remain possible. The administration had threatened 100% tariffs on chips unless companies agreed to build manufacturing capacity in the U.S., and this threat is still possible under national security laws.</p><h3 class=\"wp-block-heading\">Return to congressional authority</h3><p>Ultimately, the decision marks a major return of trade policy authority to Congress. “The power to impose tariffs is ‘very clearly . . . a branch of the taxing power,’” Chief Justice Roberts wrote, quoting the 1824 case <em>Gibbons v. Ogden</em>.</p><p>As Justice Kavanaugh noted in his dissent, while the specific legal vehicle of IEEPA has been blocked, “the decision might not substantially constrain a President’s ability to order tariffs going forward,” given the myriad other authorities at the executive’s disposal.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><h5 class=\"wp-block-heading\">​See also:</h5><p><a href=\"https://www.eetimes.com/china-stands-firm-amidst-tariff-chaos/\" rel=\"noreferrer noopener\" target=\"_blank\">China Stands Firm Amidst Tariff Chaos</a></p><p><a href=\"https://www.eetimes.com/u-s-chip-tariffs-the-impacts-on-the-u-k-and-beyond/\" rel=\"noreferrer noopener\" target=\"_blank\">U.S. Chip Tariffs: The Impacts on the U.K. And Beyond</a></p><p><a href=\"https://www.eetimes.com/trumps-tariff-fallacy-a-self-inflicted-defeat/\" rel=\"noreferrer noopener\" target=\"_blank\">Trump’s Tariff Fallacy, a Self-Inflicted Defeat</a></p><p><a href=\"https://www.eetimes.com/u-s-tariffs-push-canadas-chip-sector-to-collaborate-abroad/\" rel=\"noreferrer noopener\" target=\"_blank\">U.S. Tariffs Push Canada’s Chip Sector to Collaborate Abroad</a></p> </div>",
            "pub_date": "2026-02-21 03:31:35",
            "link": "https://www.eetimes.com/supreme-court-strikes-down-trumps-tariff-regime/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Discipline Will Keep Memory Market Tight",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>AI will keep high-bandwidth memory (HBM) in tight supply, but it’s by design.</p><p>During a recent virtual roundtable of TechInsights analysts, the consensus was that memory makers have learned from previous boom-and-bust cycles and are showing greater discipline in ramping up to meet AI-driven demand. <a href=\"https://www.eetimes.com/dram-cannot-keep-up-with-ai-demand/\" rel=\"noreferrer noopener\" target=\"_blank\">Forecasted HBM and DRAM shortages</a> stem not from supply chain disruptions, but from unprecedented and largely unanticipated adoption.</p><p>AI growth will continue to be driven primarily by the cloud, with the focus gradually shifting from training to inference.</p>\n<p>Cameron McKnight-MacNeil, process analyst at TechInsights, said <a href=\"https://www.eetimes.com/nvidia-star-attraction-at-ces-2026/\" rel=\"noreferrer noopener\" target=\"_blank\">Nvidia’s Rubin platform</a>, announced in September 2025 and touted as a new class of GPU, appears to be positioned as a platform for inference in the cloud.</p>\n\n<p>He said the <a href=\"https://www.eetimes.com/the-state-of-hbm4-chronicled-at-ces-2026/\" rel=\"noreferrer noopener\" target=\"_blank\">massive scale of HBM</a> required for AI platforms will continue in 2026, noting that each of Nvidia’s Blackwell accelerator packages contains eight HBM modules, with each module comprising eight DRAM dies plus a controller die. With hyperscalers deploying super pods of racks full of GPUs, “HBM4 is going to be the memory flavor as it were for AI in ’26.”</p>\n<p>McKnight-MacNeil said <a href=\"https://www.eetimes.com/hbm-innovation-outpaces-standards-development/\" rel=\"noreferrer noopener\" target=\"_blank\">the hunger for HBM</a> and the corresponding packaging raises yield concerns. “TechInsights has highlighted the potential concerns around the poor yields from stacking [DRAM dies] and the impact of sustainability,” he said.</p><p>JEDEC’s recent update increased the allowable package height for HBM, enabling <a href=\"https://www.techinsights.com/blog/toweringmemory-hbmandverticality\" rel=\"noreferrer noopener\" target=\"_blank\">stack heights up to 16 dies</a>, made possible in part by thinning DRAM dies and hybrid bonding.</p><p>There is extensive R&amp;D aimed at improving system efficiency, alongside increased<a href=\"https://www.eetimes.com/ai-clusters-spur-optical-connectivity/\" rel=\"noreferrer noopener\" target=\"_blank\"> adoption of optics</a> in data centers, McKnight-MacNeil added. “They’re going to make the transistors more efficient.”</p><p>But any improvements are immediately snapped up by the request for more compute, he said. “It’s a growth situation.”</p><p>Dan Kim, chief strategy officer at TechInsights, said that despite discussions about a potential AI bubble, the current memory market is not speculative. “This is purely a function of demand and supply.”</p><p>He said the market began recovering in 2023 after a cyclical downturn, with growth expected to remain healthy through 2026. “What we’re seeing is just a robust demand for accelerated compute that is being manifested in the growth of GPUs and ASICs.”</p><p>This “extraordinary” growth is happening primarily in the data center and the cloud, Kim added, contributing to rising prices for both DRAM and NAND.</p><p>“There is also an insatiable demand to lower the power consumption of AI compute,” he added.</p><p>That means innovation in more efficient power systems is becoming just as critical as scaling GPU and memory bandwidth.</p><p>Growth outside the data center space is not as aggressive, Kim said, but all segments are competing for the same foundry space. “This will shape up to be a very interesting growth market for 2026.”</p><p>But <a href=\"https://www.eetimes.com/ai-drives-av-momentum-at-ces-2026/\" rel=\"noreferrer noopener\" target=\"_blank\">if CES is any indication</a>, edge AI is seeing remarkable and even transformative growth, Jack Narcotta, head of consumer electronics at TechInsights, said. “In the consumer market, there’s always that phrase ‘bigger, better, faster, more’. A handful of things were just outright vaporware.”</p><p>He said the smart home industry is rapidly approaching a point where brands must consider some fundamental questions about their relationship with AI and where it will be located.</p><p>Putting AI directly on the device raises power and thermal issues, Narcotta said, but this would provide a smoother, faster, and significantly more reliable experience by reducing dependence on cloud communication.</p><p>He said the consumer segment for AI is the “wild west” right now and that <a href=\"https://blogs.nvidia.com/blog/2026-ces-special-presentation/\" rel=\"noreferrer noopener\" target=\"_blank\">the Nvidia Rubin announcement seemed of place</a> at CES given the platform is for the data center. But it will have a ripple effect, as consumer brands will need to understand what its capabilities mean for their products, Narcotta said. “Consumer electronics tends to lag some of the macro trends that are happening.”</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><h5 class=\"wp-block-heading\">Read also</h5><p><a id=\"_msocom_1\"></a></p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/> </div>",
            "pub_date": "2026-02-20 22:19:55",
            "link": "https://www.eetimes.com/discipline-will-keep-memory-market-tight/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "India Deep Tech Alliance Pledges $2.5B in Investment",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>The India Deep Tech Alliance (IDTA) has committed $2.5 billion in capital for deep-tech investments, including at least $1 billion dedicated to AI.</p><p>Ministry of Electronics and Information Technology (MeitY) Secretary S Krishnan said the alliance’s capital commitment reflects growing confidence in India’s deep-tech sector. “It shows confidence and recognition that there is a vibrant deep-tech sector with innovators and experienced investors who believe investments can be made,” Krishnan said.</p><p>The alliance has yet to disclose further details on the allocation strategy, but revealed the reason behind this decision at the India AI Impact Summit 2026. The IDTA presented a report stating that India’s deep-tech sector has attracted $27.9 billion since 2016 and now accounts for 15% of total private equity and venture capital (PE-VC) activity, up from 4% a decade ago.</p>\n<p>In 2025 alone, AI startups drew $1.2 billion across 188 deals, a 58% year-on-year increase in funding value, even as overall venture capital activity remained flat at around $10 billion, the report said.</p>\n\n<p>The report marks the first comprehensive effort to compile data on deep-tech-related investments in Indian startups. Krishnan said the findings indicate that “the critical mass of projects, talent, and investors is coming together,” allowing startups to raise funds domestically rather than relocating abroad.</p>\n<h3 class=\"wp-block-heading\"><strong>Defining momentum in numbers</strong></h3><p>Since 2016, $27.9 billion has been invested across 2,178 deals involving 1,217 companies. Deep tech’s share of total PE-VC funding rose from 4% in 2016 to 15% in 2025.</p><p>AI has emerged as the breakout theme, with $12 billion invested across 966 deals over the decade. AI’s share of total venture funding increased from 4.5% in 2020 to 12.3% in 2025.</p><p>While deal volumes have broadened, overall funding value remains concentrated in later-stage companies, particularly those operating across borders. By volume, most investments are early-stage. By value, capital is concentrated in growth and late-stage rounds. The report describes this as a barbell structure—a wide base of experimentation with a narrower pipeline of scaled global companies.</p><p>Private equity participation is increasing, including large rounds in AI analytics, semiconductor design, and medical technology. However, the report notes that India lacks sufficiently large specialist deep-tech funds capable of underwriting large-scale growth.</p><p>A significant share of AI and deep-tech funding continues to flow to India-founded companies headquartered overseas. The report attributes this to access to global enterprise customers, predictable AI governance regimes, and deeper late-stage capital pools. This reflects commercial structuring decisions rather than capital flight, with India remaining the research and talent base, while scaling often requires global incorporation structures.</p><h3 class=\"wp-block-heading\"><a></a><strong>Silicon Valley skepticism</strong></h3><p>Some U.S.-based growth investors question whether India’s deep-tech ecosystem has yet produced a sufficient pipeline of defensible, globally scalable product innovation.</p><p>“From where I sit, I’m not seeing enough product differentiation that would justify long-term growth capital,” one Silicon Valley-based venture capitalist said, adding that growth-stage investors typically seek scalable intellectual property with clear global market pull.</p><p>In a separate conversation, the founder of a Silicon Valley semiconductor startup drew a distinction between architectural breakthroughs and incremental system integration in semiconductors.</p><p>He said real innovation occurs at the processor and architecture level—new CPU designs, GPU platforms, and novel controller architectures. In contrast, many startups focus on customizing or integrating SoC designs for specific applications.</p><p>“Building a SoC for a particular requirement is valuable engineering,” he said. “But it’s not the same as creating a fundamentally new compute architecture.”</p><p>Investors argue that sustained product leadership requires ownership of core intellectual property at the architecture layer, where defensibility and long-term value are created. Custom SoC integration often follows market requirements and represents adaptive engineering rather than foundational invention. Without breakthroughs at the architecture level, differentiation can be difficult to sustain.</p><p>Successful startup exits, the founder noted, do not necessarily indicate category-defining products. In many cases, acquisitions are driven by talent acquisition. Acqui-hires can signal strong engineering capability but do not always reflect durable standalone business models.</p><h3 class=\"wp-block-heading\"><a></a><strong>Patent trends</strong></h3><p>Kris Gopalakrishnan, co-founder of Infosys, pointed to patent filings as a leading indicator of deep-tech innovation growth. “In the last 10 years, patent filings have increased in AI, advanced computing, industrial robotics, biotechnology, medical technologies, and climate technologies,” he said. Applications in advanced computing and AI have risen sharply in the last three to four years. About 10% have been granted so far, as it typically takes two to five years for patents to be granted.</p><p>He said that of approximately 500,000 patent applications filed by Indian-origin applicants, more than two-thirds were filed in India, including those submitted through the Patent Cooperation Treaty system. A large share has come from the top five information technology services companies.</p><p>“This shows what is coming in the next 10 years,” Gopalakrishnan said, adding that around $20 billion is expected from the private sector for deep tech.</p><h3 class=\"wp-block-heading\"><a></a><strong>Policy and patient capital</strong></h3><p>The Indian government is positioning its ₹1 Lakh Crore (~$12 billion) Research Development and Innovation (RDI) Fund as a catalytic intervention aimed at bridging the “pilot-to-commercialization” gap in hardware- and science-led innovation.</p><p>Arun Kumar, chairman of IDTA, said long-term capital is central to the sector’s next phase. “The key is patient capital and long-term commitment,” Kumar said.</p><p>He said that even a $20 billion capital base, leveraged seven to eight times, could translate into more than $100 billion over 10 to 15 years. “This is about transforming India from a services nation to a product and technology nation, with deep-tech products solving real problems,” he said.</p><p>Deep tech requires not only financial capital but also “knowledge, experience, and connections,” he added, emphasizing that scaling remains a challenge. With the government indicating willingness to remain invested for 15 to 20 years, he said he expects more patient capital from private sources, including family offices.</p><p>Beyond AI, IDTA’s report states that capital is also flowing into semiconductors, space and aerospace, robotics and automation, Industry 4.0, and energy and climate technologies. Unlike consumer software cycles, deep-tech capital tracks deployment pathways that include hospitals, original equipment manufacturers, defense buyers, and industrial integrators. This typically results in slower growth but more structurally embedded adoption. Sectors such as defense and space are increasingly sourcing from Indian startups.</p><h3 class=\"wp-block-heading\"><strong><strong>Product depth versus execution strength</strong></strong></h3><p>The base capital is present. Policy intent is visible. Talent is available. Yet global investors remain unconvinced that India’s deep-tech ecosystem has crossed into durable product leadership.</p><p>India’s structural strength lies in the application and services layer. The country’s depth of engineering talent makes it competitive in applied AI, enterprise services, and vertical integration. The ecosystem has demonstrated strength and resolve in execution, while original product invention at scale remains an area of development.</p><p>The open question is whether growth-stage capital deepens sufficiently, whether intellectual property ownership remains anchored in India, and whether research translates into manufacturing and global industrial scale.</p><p>The capital is assembling, but its translation into globally defensible product companies will depend on sustained execution and long-term ownership of intellectual property.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><h5 class=\"wp-block-heading\">Read also:</h5><hr class=\"wp-block-separator has-alpha-channel-opacity\"/> </div>",
            "pub_date": "2026-02-20 14:06:01",
            "link": "https://www.eetimes.com/india-deep-tech-alliance-pledges-2-5b-in-investment/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Taalas Specializes to Extremes for Extraordinary Token Speed",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>AI chip startup Taalas, co-founded by Tenstorrent co-founder and former CEO and CTO Ljubisa Bajic, is demonstrating its first chip with extraordinary performance. Taalas’ HC1 can achieve more than 16,000 tokens per second per user on Llama3.1-8B, multiples of what competitors Nvidia, Cerebras, and Groq can do, but there’s a catch—the chip <em>only</em> runs Llama3.1-8B.</p><p>Superlative performance is achieved by effectively hardwiring an entire model, including its weights onto a chip, removing almost all programmability (the HC1 has a small SRAM which can be used to store fine-tuned weights and a KV cache).</p><p>There are LLM-inference-focused chips already on the market from companies like SambaNova and D-Matrix, but most focus on the optimal balance of memory and compute, and the bandwidth between them, retaining programmability to run any model. Startup Etched is pursuing a more transformer-specific architecture, trading off flexibility for performance, but it’s still unclear how much programmability they will offer. Taalas’ approach is the most extreme specialization the AI chip industry has seen so far.  </p>\n<p>Taalas’ extreme approach won’t suit every application, Taalas CEO Ljubisa Bajic told EE Times, but it might well suit some.</p>\n\n<p>“Essentially, we looked for ways to make painful tradeoffs in flexibility for the sake of economics and speed,” Bajic said.</p>\n<p>This hasn’t been done before for AI, despite the workload being so significant that there are already serious discussions about launching GPU-laden satellites into space, he said.</p><p>“Nobody went into this [flexibility-performance] corner because everybody felt AI was changing so rapidly that it would be a massively risky thing to do… which it is, to some extent,” Bajic said. “But we wanted to see what’s hiding in that corner, what we could get if we explored it, and you can get a lot. We concluded there’s almost certainly going to be a set of applications that’s going to benefit from this degree of optimization. How many that’s going to end up being… we’ll find out.”</p><h3 class=\"wp-block-heading\"><strong>Tokens per second</strong></h3><p>Taalas’ online chatbot demo hit 15,000+ tokens per second when EE Times tried it out, but internal testing has achieved closer to 17,000 under some conditions, the company said (Taalas admits its version of Llama3.1-8B is quantized “aggressively”). The company’s fastest competition today is Cerebras, which can achieve close to 2,000 tokens per second per user for Llama3.1-8B, with SambaNova at around 900 and Groq at around 600 (these figures are from <a href=\"https://artificialanalysis.ai/models/llama-3-1-instruct-8b/providers#speed\" rel=\"noreferrer noopener\" target=\"_blank\">Artificial Analysis</a>). Taalas said it had tested Nvidia Blackwell-generation hardware internally at around 350.</p><p>Taalas’ HC1 is built on TSMC N6 with a die size of 815 mm<sup>2</sup>, and one chip fits the entire 8B model. The chip uses around 250W, so 10 HC1 cards in a server need about 2.5 kW, meaning they can be deployed in standard air-cooled racks.</p><p>Total cost of ownership (TCO) also comes out favorably per Taalas’ figures, even assuming a GPU refresh cycle of four years against the cost of taping out and replacing Taalas chips every year. One million Llama3.1-8B tokens on Taalas comes out at 0.75 cents.</p><h3 class=\"wp-block-heading\"><strong>Two masks</strong></h3><p>Taalas is borrowing some ideas from the structured ASICs of the early 2000s to make its hardwired model-specific chips. <a href=\"https://www.eetimes.com/how-hybrid-structured-asics-provide-low-cost-solutions-for-mid-range-applications/\">Structured ASICs</a> used gate arrays and hardened IP blocks, changing only the interconnect layers to adapt the chip to a specific workload. At the time, this was seen as a more cost-effective alternative to a full-custom ASIC that was more performant than an FPGA.</p><p>“There are definitely parallels,” Bajic said. “It’s a similar idea to <a href=\"https://www.eetimes.com/is-it-an-asic-is-it-an-fpga-no-its-easic/\">eASIC</a> and gate arrays, but the underlying technology looks really different.”</p><p>Taalas changes only two masks to customize a chip for a specific model, but the two masks can change both model weights and dataflow through the chip. On the HC1, the model and its weights are stored on the chip using a mask-ROM-based recall fabric paired with a (programmable) SRAM, which can be used to hold fine-tuned weights and/or the KV cache. Future generations of chips may split the SRAM onto a separate chip, meaning they could be denser than the HC1.</p><p>The idea is scalable beyond 8B models, Paresh Kharya, VP of product at Taalas, told EE Times.</p><p>“The goal of [the HC1] was to prove our architecture works and show how our approach can be used to scale to larger models,” Kharya said. “We had to make a lot of technological breakthroughs to make this whole approach work. By choosing a smaller model, we’ve basically flushed the pipe, if you will, on the whole process.”</p><p>The HC1 fits the entire 8B version of Llama3.1 on a single chip, but bigger models will need more chips. Taalas has simulated what a multi-chip solution for DeepSeekR1-671B would look like. Splitting the SRAM portion out onto separate chips could increase density to around 20B parameters per Taalas chip (in MXFP4 format). Taalas’ density is also helped by an innovation which stores a 4-bit model parameter and does multiplication on a single transistor, Bajic said (he declined to give further details but confirmed that compute is still fully digital). Even with these density advantages, holding the entire 671B model would still need somewhere in the order of 30 custom tape-outs.</p><p>“It means 30 incremental tape-outs, which is the annoying part, but the tape-outs are pretty cheap because it’s only two masks,” Bajic said. “The big thing at the root of this idea is the assumption that the customer is willing to commit to this [chip/model] for a year. There will definitely be a lot of people who won’t, but some people will.”</p><p>Taalas’ simulations of its 30 or so chips for DeepSeekR1 suggest it could achieve around 12,000 tokens per second per user (state of the art on a GPU would be around 200 tokens per second per user today, according to the company). Taalas’ calculations have DeepSeekR1 running on the 30 chips at 7.6 cents per million tokens, less than half of a throughput-optimized GPU-based equivalent, even accounting for taping out 30 new chips every year versus a four-year GPU refresh cycle.</p><p>Typically, speed is traded off with cost today, but Taalas wants to provide advantages for both.</p><p>“From our standpoint, we already come with this fairly serious compromise, so we wanted to make it the only one,” Bajic said. “Yes, it’s less flexible, but on the flip side, everything else is better.”</p><p>Part of keeping the cost down is regular, fast tape-outs, with potentially multiple tape-outs required for any model bigger than 8B, and this is where part of Taalas’ secret sauce lies.</p><p>“We built a bunch of automation so that we can quickly go from a model to [RTL],” Bajic said. “It’s not fully push-button, but it’s about a week’s worth of effort as things stand.”</p><p>Taalas expects to be able to offer custom model-specific chips with a two-month turnaround, Bajic said. This speed has required significant innovation, he added.</p><p>“It’s a reticle-sized chip where you’re moving and changing connections, and just verifying that it’s going to work in the normal way was vastly too slow,” Bajic said. “How do you do [design rule checking] on these things in a way that doesn’t take six months?”</p><p>Compounding the problem is Taalas’ deliberate lack of programmability.</p><p>“Because we’re hardwiring it, the margin for error is basically zero,” Bajic said. “The only way you can be confident it works, given that you can’t change anything post tape-out, or your ability to change is really limited, is to run the entire model in simulation.”</p><p>But simulating 30 chips working together is also no mean feat. Taalas’ workflow enables it to run simulations this large on big computing clusters, and the simulations are structured such that they can be exposed to potential customers without revealing Taalas’ secret sauce or violating the terms of their EDA tool contracts, Bajic said.</p><p>“The end result is we built something others haven’t,” Bajic said. “From an engineering standpoint, we shone a light at the corner where no one else was, where there were a hundred fairly difficult problems to solve… the challenges were mainly that no one had ever done this before.”</p><p>A side effect of hardwiring a model into hardware is that Taalas’ software stack is very, very simple, both for running a model on the chip and for serving inference at scale, especially versus large, complex disaggregated GPU systems, Bajic said.</p><p>“Software sort of disappeared as a thing,” he said, noting that Taalas has one engineer working on its software stack, and that person also has other responsibilities.</p><p>Hardwired chips can reduce complexity at the system level, especially when everything is done on a single chip, as this reduces data movement and simplifies I/O. Fast clock speeds aren’t required for performance, so power consumption is less extreme, and simpler (air) cooling can be used.</p><p>“There’s a lot of complexity behind why systems are so complicated now and so big, and why there’s miles of cables and why it’s so hard to get it to work to begin with,” Bajic said. “Our approach just makes it all disappear. The problem becomes simpler at every level… except for the rapid creation of new chips.”</p><h3 class=\"wp-block-heading\"><strong>Selling chips</strong></h3><p>Taalas is still defining its business model, but the company has a number of options, Kharya said.</p><p>“We want to engage developers,” Kharya said. “Going forward, we could build our own infrastructure running open-source models and offer API access [as well as selling chips]. At the same time, we are also open to working with model developers to create custom chips for their models, for their own serving infrastructure. Both of these possibilities are open.”</p><p>Model-optimal silicon, as Kharya calls it, is inevitable, he said. While it won’t replace big data centers full of GPUs, it will suit some applications, particularly as models mature enough to become useful, as it makes them stickier.</p><p>“It’s pretty reasonable to expect that for the kinds of people we’ve been speaking to, that the affinity of a model running important use cases could be a year or maybe even more,” Kharya said.</p><p>The HC1, intended largely as a demo, is running inference today. Taalas is aiming to produce a second chip for a “reasonable-size” reasoning model by early summer and plans to be running frontier-level models by the end of the year. The company has raised in excess of $200 million and currently has around 25 employees.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><h5 class=\"wp-block-heading\">Read also:</h5><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><p></p> </div>",
            "pub_date": "2026-02-20 00:19:43",
            "link": "https://www.eetimes.com/taalas-specializes-to-extremes-for-extraordinary-token-speed/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Nvidia: Star Attraction at CES 2026",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>Nvidia was the most influential company at CES 2026 with major AI-related and automotive product introductions.</p><p>The next table summarizes Nvidia’s announcements and information, with a focus on automotive impact. For the autonomous vehicle (AV) industry, Nvidia’s Alpamayo AV platform was the most important announcement at CES. Nvidia described it as the first AI model with reasoning based on vision language action (VLA) or using a chain-of-thought reasoning strategy. Alpamayo also has simulation capabilities and comes with a physical AI open dataset based on 1,700 hours of driving data.</p><p>The Vera Rubin platform consists of six superchips that are summarized in the table above. These chips are designed to greatly increase system performance across large AI systems that use thousands of Vera Rubin platform chips. This requires increased communication performance of Ethernet connections, as well as switches and related communication chips across rack systems in IT centers.</p>\n<p>Three subsystems are listed in the table above, with additional details provided below. The Vera Rubin Superchip is a board with two Rubin GPUs, one Vera CPU, substantial memory, and NVLink switch chips.</p>\n\n<p>The Vera Rubin Compute Tray has two superchips, a BlueField-4 data processing unit (DPU), and ConnectX-9 SuperNICs. The main purpose of a SuperNIC is to accelerate the network performance of multiple GPUs working on AI applications. It is a key factor in combining many Rubin GPUs into larger AI systems. The Compute Tray is the core building block for racks of GPU-based AI systems.</p>\n<p>The Vera Rubin NVLink Switch Tray includes four NVLink Switches and is another core network subsystem for connecting GPUs in rack systems. It uses four NVLink switch chips and provides 3.6 TB/s per GPU.</p><p>Vera Rubin NVL72 rack is the flagship of the Rubin platform. It is a system for building a range of AI systems in IT data centers. It is engineered to make the entire rack operate as a coherent machine within a larger AI factory.</p><p>The DGX SuperPOD is a complete system for AI training or inference and consists of eight Vera Rubin NVL72 racks. It has 576 Rubin GPUs and 288 Vera CPUs. Many DGX SuperPODs can be connected into AI factories with dozens to hundreds of DGX SuperPODs.</p><p>The following image provides summary information on the six chips that are the building blocks of Nvidia’s AI factories and physical AI systems. This overview is based on Nvidia’s detailed 30‑plus‑page description of the Rubin platform. The article provides details on the performance specifications of each chip and information on the architecture of AI systems built from large numbers of Rubin platform chips. The full Nvidia Rubin platform description is available <a href=\"https://developer.nvidia.com/blog/inside-the-nvidia-rubin-platform-six-new-chips-one-ai-supercomputer/\" rel=\"noreferrer noopener\" target=\"_blank\">here</a>.</p><p>The next figure displays the six superchips described in the linked article: Vera CPU, Rubin GPU, ConnectX-9, NVLink 6 Switch, BlueField-4 DPU, and Spectrum Ethernet Switch. Summary descriptions for each chip are provided below.</p><figure class=\"wp-block-image aligncenter size-full\"><a href=\"https://www.eetimes.com/wp-content/uploads/Nvidia-Vera-Rubin-Platform_Six-New-SoCs_Egil-Juliussen.jpg\" rel=\"noreferrer noopener\" target=\"_blank\"><img alt=\"Nvidia Vera Rubin Platform_Six New SoCs_Egil Juliussen\nCES 2026\" class=\"wp-image-1497507\" data-recalc-dims=\"1\" decoding=\"async\" height=\"388\" src=\"https://www.eetimes.com/wp-content/uploads/Nvidia-Vera-Rubin-Platform_Six-New-SoCs_Egil-Juliussen.jpg?resize=640%2C388\" width=\"640\"/></a><figcaption class=\"wp-element-caption\">Click to enlarge</figcaption></figure><p>A key strategy was to co-design the six Rubin platform chips in terms of networking, security, software, power delivery, and cooling together as a single system rather than optimized in isolation. By doing so, the Rubin platform treats the data center as the unit of compute—not a single GPU server as the unit of compute.</p><p><strong>Vera CPU</strong></p><p>Vera is the new CPU generation and is purpose-built for AI factories to manage high utilization across thousands of GPUs and AI factories. Instead of functioning as a traditional general-purpose host, Vera is optimized for orchestration, data movement, and coherent memory access across racks of GPU-based systems, such as agentic processing AI use cases.</p><p>Vera has 88 Olympus CPU cores with full Arm v9.2 compatibility. Vera has 2 MB of L2 cache per core that can provide 176 Spatial Multithreading operations. Vera has 162 MB of unified L3 cache. Memory capacity is up to 1.5 TB of LPDDR5X memory. Memory bandwidth is up to 1.2 TB per second.</p><p>Vera introduces Spatial Multithreading, a new type of multithreading that runs two hardware threads per core by physically partitioning resources instead of time-slicing.</p><p><strong>Rubin GPU</strong></p><p>Rubin is the latest GPU generation and is designed for continuous training, post-training, and inference in always-on AI factories. Current AI workloads are limited by peak floating-point operations (FLOPS). They are also constrained by whether execution efficiency can be sustained across compute, memory, and communication. The Rubin GPU is purpose-built to solve this problem, optimizing execution paths that turn power, bandwidth, and memory into better AI workload performance.</p><p>To sustain throughput, the Rubin GPU improves its architecture across three tightly coupled dimensions: compute density, memory bandwidth, and rack-scale communication. At the silicon level. Rubin scales every critical subsystem for transformer-era workloads. The GPU integrates 224 Streaming Multiprocessors (SMs) equipped with sixth-generation Tensor Cores optimized for low-precision NVFP4 and FP8 execution.</p><p>Chip complexity is measured by the number of transistors, which is 336 billion for Rubin or 1.6× more than the Blackwell GPU. Rubin uses the latest generation of high-bandwidth memory, HBM4, which was introduced in April 2025.</p><p>Performance improvements are 2× to 5× compared to Blackwell. Bandwidth per GPU is 3.6 TB/s. HBM4 bandwidth is 22 TB/s or 2.8× higher than Blackwell.</p><p><strong>NVLink 6 Switch</strong></p><p>The NVLink 6 Switch is the sixth-generation technology for increasing GPU-to-GPU bandwidth. It is the chip that provides scale-up technology for racks of Rubin platform chips and systems. It enables 72 Rubin GPUs within an NVL72 system to operate as a single, tightly coupled accelerator with uniform latency and sustained bandwidth under communication-dominated workloads. Each Rubin GPU connects to NVLink 6 with 3.6 TB/s of bidirectional bandwidth. NVLink 6 switch trays form a single all-to-all topology across the rack, allowing any GPU to communicate with any other GPU with consistent latency and bandwidth. From the software perspective, the rack behaves as one large accelerator, simplifying scaling for communication-heavy AI models. 36 NVLink 6 Switches are used to connect GPUs in NVLink 72 systems.</p><p><strong>ConnectX-9</strong></p><p>ConnectX-9 serves as the intelligent endpoints of the Spectrum-X Ethernet fabric, delivering predictable performance while enforcing traffic isolation and secure operation. In the Vera Rubin NVL72 rack-scale architecture, each compute tray contains four ConnectX-9 SuperNIC boards, delivering 1.6 Tb/s of network bandwidth per Rubin GPU. ConnectX-9 also plays a central role in securing AI factory networking. Integrated cryptographic engines support high-throughput encryption for data in motion and data at rest, enabling secure operation.</p><p><strong>BlueField-4 DPU</strong></p><p>The BlueField-4 DPU is a dual-die package combining a 64-core Grace CPU for infrastructure offload and security. Memory capacity is 168 GB, and bandwidth is 250 GB/s. BlueField-4 has an integrated ConnectX-9 high-speed networking chip for tightly coupled data movement. BlueField-4 delivers up to 800 Gb/s of ultra-low-latency Ethernet or InfiniBand connectivity. Nvidia calls it: “Powering the operating system of the AI factory.”</p><p>BlueField-4 introduced Advanced Secure Trusted Resource Architecture (ASTRA), a system-level trust architecture that establishes a trust domain within the compute tray. ASTRA provides AI infrastructure builders with a single, trusted control point to securely provision, isolate, and operate large-scale AI environments without compromising performance.</p><p>Within the Rubin platform, BlueField-4 operates as a software-defined control plane for the AI factory. It enforces security, isolation, and deterministic operation, independently of host CPUs and GPUs. By offloading and accelerating infrastructure services onto a dedicated processing layer, BlueField-4 enables AI factories to scale and maintain consistent performance, strong isolation, and efficient operations.</p><p><strong>Spectrum-6 Ethernet switch</strong></p><p>AI factories must scale beyond a single Vera Rubin NVL72 system and may need to scale across geographically dispersed data centers. Performance is then determined by how predictably the network behaves under synchronized, bursty AI traffic. To support both scale-out and scale-across AI factory deployments, the Rubin platform introduces Spectrum-X Ethernet Photonics. It is a new generation of Spectrum-X Ethernet switching based on co-packaged optics that advances Nvidia’s purpose-built Ethernet technology for accelerated computing.</p><p>Spectrum-6 Ethernet delivers predictable, low-latency, high-bandwidth connectivity through advanced congestion control, adaptive routing, and lossless Ethernet behavior. Spectrum-6 doubles per-switch-chip bandwidth to 102.4 Tb/s by using 512 ports with a 200 Gb/s data transfer rate.</p><h3 class=\"wp-block-heading\"><strong>From chips to systems: Vera Rubin superchip to DGX SuperPOD</strong></h3><p>Nvidia’s Rubin platform chips are used to build subsystems and systems. The systems become the building blocks for AI factories. The next figure shows three subsystems and two systems that are the core building blocks for AI factories. The Vera Rubin Superchip is the key building block at the subsystem level. The Vera Rubin NVL72 Compute Tray is the next step, as it is built from multiple Superchips. The Vera Rubin Switch Tray uses NVLink 6 switch chips to increase GPU bandwidth and scale up the performance of Vera Rubin NVL72 Systems. One Vera Rubin NVL72 rack system is shown in the lower left picture.</p><figure class=\"wp-block-image aligncenter size-large\"><a href=\"https://www.eetimes.com/wp-content/uploads/Nvidia-Vera-Rubin-Platform_From-Chips-to-Systems_Egil-Juliussen.jpg\" rel=\"noreferrer noopener\" target=\"_blank\"><img alt=\"Nvidia Vera Rubin Platform_From Chips to Systems_Egil Juliussen\nCES 2026\" class=\"wp-image-1497508\" data-recalc-dims=\"1\" decoding=\"async\" height=\"383\" src=\"https://www.eetimes.com/wp-content/uploads/Nvidia-Vera-Rubin-Platform_From-Chips-to-Systems_Egil-Juliussen.jpg?w=640&amp;resize=640%2C383\" width=\"640\"/></a><figcaption class=\"wp-element-caption\">Click to enlarge</figcaption></figure><p>The bottom right picture shows a DGX SuperPOD with the DGX Vera Rubin NVL72 system, which is the main building block for AI factories.</p><p><strong>Vera Rubin superchip</strong></p><p>The Vera Rubin superchip is the foundational compute building block for large AI systems. Each superchip combines two Rubin GPUs with one Vera CPU through memory-coherent NVLink-C2C interconnect, which collapses traditional CPU-GPU boundaries into a unified, rack-scale execution domain.</p><p>By integrating GPU compute with a high-bandwidth CPU data engine on a single host processing motherboard, the superchip improves data locality, reduces software overhead, and sustains higher utilization across heterogeneous execution phases. It becomes the architectural bridge between chip-level innovation and rack-scale intelligence.</p><p><strong>Vera Rubin NVL72 compute tray</strong></p><p>The NVL72 compute tray makes the Vera Rubin superchip into a deployable, serviceable unit for building AI factories. Each tray integrates two superchips, power delivery, cooling, networking, and management into a modular, cable-free assembly. The trays are optimized for density, reliability, and ease of operation.</p><p><strong>Vera Rubin NVL72Link switch tray</strong></p><p>The NVL72Link switch tray transforms multiple compute trays into a single coherent system and ensures that performance scales predictably as models, batch sizes, and reasoning depth increase.</p><p><strong>Vera Rubin NVL72 system</strong></p><p>The flagship of the Rubin platform is the Vera Rubin NVL72 rack system. It is engineered so that the entire rack operates as a coherent machine within a larger AI factory. The NVL72 system rack is the building block for DGX SuperPODs. Each NVL72 rack has eight Rubin GPUs. Spectrum-X Ethernet co-packaged optics is a key sub-system for connecting NVL72 Compute Trays into Vera Rubin NVL72 rack systems.</p><p><strong>DGX SuperPOD with DGX Vera Rubin NVL72</strong></p><p>DGX SuperPOD represents the blueprint for large-scale deployment of the Rubin platform. Built with eight DGX Vera Rubin NVL72 systems, it defines the minimum unit at which AI factory economics, reliability, and performance converge in production environments. DGX SuperPODs are turnkey systems, and many such systems are used to deploy AI factories.</p><p>DGX SuperPOD is designed as a complete system. Every layer, from silicon and interconnects to orchestration and operations, is co-designed and validated to deliver sustained utilization, predictable latency, and efficiency. The NVL72 designation is based on 72 Rubin GPUs, which are included in the name.</p><p>The DGX SuperPOD hardware is complemented by a full-stack data center platform that includes computing, storage, networking, software, and infrastructure management.</p><p><strong>Nvidia software</strong></p><p>Nvidia also emphasized its growing software portfolio for AI model expansion, with a focus on models for automotive and industrial applications, which includes physical AI. A simple explanation of physical AI is that it follows the laws of physics, such as motion, speed, timing, and related. Nvidia is focusing on AVs and robotics as the first segments to develop physical AI software and systems.</p><p>Nvidia has an impressive ecosystem around its GPU platform, including the leading software ecosystem due to the prevalence of the CUDA platform for GPU software. The next figure shows Cuda-X libraries that work with the Vera Rubin platform.</p><figure class=\"wp-block-image aligncenter size-large\"><a href=\"https://www.eetimes.com/wp-content/uploads/Nvidia-CUDA-X-libraries.jpg?w=640\" rel=\"noreferrer noopener\" target=\"_blank\"><img alt=\"Nvidia CUDA-X libraries\nCES 2026\" class=\"wp-image-1497509\" data-recalc-dims=\"1\" decoding=\"async\" height=\"247\" loading=\"lazy\" src=\"https://www.eetimes.com/wp-content/uploads/Nvidia-CUDA-X-libraries.jpg?w=640&amp;resize=640%2C247\" width=\"640\"/></a><figcaption class=\"wp-element-caption\">Click to enlarge</figcaption></figure><p>The CUDA platform encompasses a programming model, core libraries, and communication stacks that accelerate applications and show the capabilities of the rack-scale system. Developers can program Rubin GPUs as individual devices or as part of a single 72-GPU NVLink domain using Nvidia’s Collective Communications Library (NCCL), Inference Transfer Library (NIXL), and other NVLink-aware software. This design enables models to scale across the rack without custom partitioning, topology-aware workarounds, or manual orchestration.</p><p>The core of Nvidia’s training and customization stack is the NeMo framework. It provides an end-to-end workflow for building, adapting, aligning, and deploying large AI models. NeMo unifies data curation, large-scale distributed training, alignment, and parameter customization into a single, production-oriented framework.</p><p>These libraries and software platforms allow developers to focus on model behavior rather than hardware-specific tuning, while still gaining excellent performance from the underlying GPU platforms.</p><p><strong>Nvidia AV ecosystem</strong></p><p>The next figure shows Nvidia’s AV ecosystem, as presented during Jensen Huang’s keynote on Jan. 5, 2026. On the left, there are nine companies listed with AV software platforms. There are three U.S. companies: Aurora, Nuro, and Plus. Plus has a significant China operation. Waabi is headquartered in Canada but primarily operates in the U.S. Wayve is a U.K. company with European and U.S. testing activities. The remaining five companies are based in China: Deeproute.ai, Momenta, Pony, WeRide, and Zyt.</p><figure class=\"wp-block-image aligncenter size-full\"><a href=\"https://www.eetimes.com/wp-content/uploads/Global-L4-and-robotaxi-ecosystem-building-on-Nvidia.jpg\" rel=\"noreferrer noopener\" target=\"_blank\"><img alt=\"Global L4 and robotaxi ecosystem building on Nvidia\nCES 2026\" class=\"wp-image-1497510\" data-recalc-dims=\"1\" decoding=\"async\" height=\"120\" loading=\"lazy\" src=\"https://www.eetimes.com/wp-content/uploads/Global-L4-and-robotaxi-ecosystem-building-on-Nvidia.jpg?resize=640%2C120\" width=\"640\"/></a><figcaption class=\"wp-element-caption\">Click to enlarge (Source: <a href=\"https://s201.q4cdn.com/141608511/files/doc_downloads/2026/01/05/JHH-CES-2026-FINAL.pdf\" rel=\"noreferrer noopener\" target=\"_blank\">Jensen Huang’s keynote at CES 2026</a>)</figcaption></figure><p>The middle of the chart lists Nvidia’s 11 OEMs and mobility platforms. Again, most of the companies are based in China, with only two European OEMs (Mercedes-Benz and Stellantis) and Uber as the only U.S.-based company. Several new OEM customers were announced at CES 2026, including JLR and Lucid.</p><p>The right side lists 14 Tier 1 and hardware partners. Key Tier 1s are included, such as Aumovio (formerly Continental), Bosch, Denso, Magna, and ZF.</p><h3 class=\"wp-block-heading\"><strong>Summary and perspectives</strong></h3><p>CES continues to be the premier tradeshow for electronic products, including automotive electronics and AVs, with robotaxis drawing the most attention. The number of exhibitors topped 4,100, down by 400 from 2025, primarily due to a significant decline in Chinese participation. Attendance reached 148,000, up from 141,000 the previous year.</p><p>Nvidia had the leading presence at CES 2026 and announced many new products or upgraded versions of other products. The six chips for the Vera Rubin platform will have a big impact on AI technology, including physical AI such as AVs and robotics.</p><p>Nvidia’s six new chips were designed to enhance system performance across racks of AI systems running both training and inference models. To do so, Nvidia has leveraged its knowledge from designing and running AI models, from chips to systems. The Nvidia paper referenced above provides extensive data and details on all six Rubin platform chips. It also demonstrates how the Vera Rubin platform is used to build new AI systems for IT data centers for AI model training and inference. Nvidia refers to these data centers as AI factories.</p><p>The next figure is a summary of Nvidia’s Vera Rubin platform—from chips and how they are used in subsystems, systems, and AI factories. It is a simplified block diagram of how the systems are built.</p><p>The left side of the next figure shows the six chips in the Vera Rubin platform with summary information of what is included in the previous sections. The green boxes in the middle show three important subsystems. The Vera Rubin superchip has two Rubin GPUs and one Vera CPU. Two Vera Rubin superchips are used in the Vera Rubin Compute Tray along with two ConnectX-9 chips and one Bluefield-4 DPU.</p><p>The Vera Rubin NLV72 rack system, in the blue box, uses 18 Compute Trays and nine NVLink Switch trays. This hardware adds up to 36 Vera CPUs and 72 Rubin GPU chips. The NLV72 rack system has an amazing 220 billion transistors combined from all the chips and subsystems. To put this number in perspective: With the current world population at roughly 8.17 billion, a single Nvidia Vera Rubin NLV72 contains the equivalent of 26,900 transistors for every person on Earth. For the U.S. population, that same system equates to about 640,000 transistors per person.</p><p>The top red block shows that one Nvidia DGX SuperPOD with DGX Vera Rubin NVL72 uses eight NVL72 racks. AI factories are expected to deploy hundreds of DGX SuperPODs.</p><p>What strikes me from reviewing Nvidia’s CES 2026 product introduction is the tremendous capabilities that Nvidia has acquired—from leading-edge chips to computer systems that serve as building blocks for AI factories. Nvidia is showing state-of-the art chips, subsystem hardware building blocks that scale into sophisticated AI-focused computer systems and rack systems that form a major driving force for AI-focused IT centers. On top of this, Nvidia is also providing software platforms that complement its hardware portfolio and is open to creating extensive ecosystems for many industries. In my 40-plus years of analyzing computer-centric industries, I have not seen a company acquire such a wide range of expertise and know-how to leverage, use it, and grow it.</p><p>Nvidia will face increasing competition in the future, but it will remain the leader for a long time, thanks to its extensive platform strategy spanning multiple levels of the hardware and software hierarchy—even if the AI bubble is deflated.  </p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><h5 class=\"wp-block-heading\">Read also:</h5><hr class=\"wp-block-separator has-alpha-channel-opacity\"/> </div>",
            "pub_date": "2026-02-19 19:20:57",
            "link": "https://www.eetimes.com/nvidia-star-attraction-at-ces-2026/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "How China Struggles to Reach WFE Self-Sufficiency",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>Late last year, a <a href=\"https://www.reuters.com/world/china/china-mandates-50-domestic-equipment-rule-chipmakers-sources-say-2025-12-30/\" rel=\"noreferrer noopener\" target=\"_blank\">Reuters</a> report revealed that China had discreetly rolled out a policy requiring local chipmakers to source no less than 50% of their wafer fabrication equipment (WFE) from domestic vendors when building new fabs—a surprising mandate, given that China remains far from self-sufficient in fab tools. However, with some foreign equipment procured by Semiconductor Manufacturing International Corp. (SMIC) sitting idle due to sanctions, the requirement may have merit. The main question is whether China can really accelerate its WFE industry under such measures and whether this industry can really catch up with its Western rivals.</p><p>The requirement to use at least 50% of tools produced domestically in new wafer fabs has not been formalized in public regulations, but one analyst told EE Times that Chinese authorities require that new capacities use at least 50% of domestic tools by value. Companies applying for state approval to construct or expand fabs in recent months have been asked to demonstrate that at least half of their semiconductor production equipment (SPE) comes from Chinese equipment manufacturers. Submissions falling short of this threshold are generally rejected, according to Reuters, citing sources familiar with the approval process. Moreover, officials have reportedly indicated that the 50% figure should be viewed as a baseline rather than an ultimate goal, as the longer-term plan is to exclusively use tools made in the People’s Republic of China (PRC).</p><p>However, because no formal rule mandates that at least 50% of WFE in new fabs must be sourced domestically, authorities reviewing applications can make case-by-case adjustments when local tools are unavailable, which represents a key caveat. The requirement is reportedly enforced most rigorously for production lines targeting mature process technologies, while capacities for advanced nodes are granted waivers, as domestically produced tools cannot yet meet the demands of sophisticated manufacturing nodes, such as 28 nm and below.</p>\n<p>“[As for meeting] the 50% domestic equipment requirement, it depends on how it is [measured], for which there are multiple variants: yuan, units, chambers, like for like,” G. Dan Hutcheson, vice chair and senior fellow at TechInsights, told EE Times. “As far as I can tell, China has been very flexible and fluid in how it determines this. It would be fairly easy to hit a 50% target of chambers in the fab. If you break it down to specific types of tools, it gets more difficult. In yuan, it also is more difficult, as China has far lower costs and margins, so its tools sell for far less than Western competitors.”</p>\n\n<p>Chinese authorities and semiconductor manufacturers have yet to formally confirm that the “50% rule” exists and is rigorously enforced by authorities. As China tends to be rather consistent with its semiconductor self-sufficiency plans, it may well impose certain rules that stimulate the domestic SPE sector but give some exceptions to producers of advanced logic. Although China’s WFE industry can produce world-class deposition and etching tools, it still cannot match the sophistication of ASML’s lithography machines. Yet beyond stimulating local producers of chipmaking tools, the Chinese government may have other reasons for requiring the procurement of domestic tools.</p>\n<h3 class=\"wp-block-heading\"><strong>Problems at SMIC</strong><strong></strong></h3><p>Earlier this month, SMIC said that some of the tools it had proactively acquired from foreign suppliers prior to the imposition of certain export restrictions could not be put to work because the company was unable to procure “supporting” equipment. As a result, these tools may remain idle for the remainder of the fiscal year.</p><p>“However, due to the impact of external factors, the company has procured some key equipment in advance while the supporting equipment may not be purchased yet,” Zhao Haijun, co-CEO of SMIC, said in the group’s Q4 2025 earnings call on Feb. 10. “This timing difference has brought an even situation that the procured equipment may not be able to form production lines this year.”</p><p>Although SMIC did not directly blame export controls imposed by the U.S. government and its allies, this is certainly not the first time SMIC has been affected by Western sanctions. Last year, for example, the company experienced output disruptions and yield losses caused by maintenance of existing tools and validation of new systems.</p><p>SMIC encountered two setbacks. First, an accident during scheduled annual maintenance disrupted fab operations and reduced process accuracy, leading to lower yields. Second, performance issues causing yield fluctuations were uncovered during the qualification of newly installed tools. Neither of the issues could be fixed promptly, impacting the company’s revenue.</p><p>While annual maintenance is normally routine, U.S. export rules prevent American WFE vendors from servicing advanced tools in China. As a result, SMIC’s engineers must handle work without full vendor support, which greatly increases the risks of mishaps. Newly delivered systems face similar risks, especially if tools are not acquired directly from their manufacturers. Typically, tools are fully assembled and tested at manufacturers such as ASML before being taken apart and reinstalled at the customer’s site. Although SMIC’s in-house teams can handle certain maintenance and installation procedures, the absence of proper vendor support elevates the risks of accidents.</p><p>Given the growing risks associated with obtaining and servicing tools from foreign suppliers, one of the ways to reduce global risks for foundries is to encourage them to use equipment from Chinese vendors, or at least vendors with manufacturing bases in the PRC.</p><h3 class=\"wp-block-heading\"><strong>China’s WFE spending to continue growing</strong><strong></strong></h3><p>China emerged as a leader in WFE spending in 2020. After the U.S. government announced new export control rules that mandated an export license for advanced chip production tools bound to China-based entities in 2021, Chinese semiconductor vendors all accelerated their procurement of SPE to a degree that many tools ended up in their hands through questionable channels.</p><p>“Of course, nobody knows exactly how [shipments of restricted tools] happen today, except those involved, because it is a criminal activity,” Hutcheson said. “That said, it is likely the classic re-exporting or trans-shipping methods that date back to the Cold War. Early on, there was a lot of confusion about what counted and what did not. One idea was that if the tool was built and exported from a foreign country, it would be OK, even though the company was based in the U.S.”</p><p>Typically, heavy tool purchases are followed by digestion phases, during which companies minimize their purchases. However, six years into the rally, Chinese chipmakers continue to invest in new tools, driven by the semiconductor self-sufficiency plan, albeit with an increasing share of domestically made systems, according to a UBS report seen by EE Times.</p><p>UBS believes that China’s semiconductor industry—including leading logic producers SMIC and Hua Hong, leading memory manufacturers CXMT and YMTC, and smaller chipmakers mainly focused on trailing nodes—spent about $42.75 billion on new wafer fabrication tools last year, encompassing both Chinese and multinational suppliers. This figure is well above earlier, mid-$30 billion assumptions.</p><p>For 2026, UBS projects spending of $47.05 billion, rising to $50 billion in 2027 and $50.35 billion in 2028. Depending on how much Taiwan, South Korea, and the U.S. spend on WFE in the coming years, China will consume about one-third of that, likely remaining the single-largest maker for semiconductor production tools in the coming years despite tightening export restrictions.</p><p>UBS believes that capacity additions span advanced logic (which the company determines as 28/22 nm and below), memory, and 300-mm trailing-edge production. This means continued buildout of mature-node infrastructure as well as catchup efforts at sub-28-nm nodes. How Chinese chipmakers will secure the necessary tools for 14-nm logic and more advanced process technologies, however, remains uncertain.</p><p>“We expect strong growth in China’s WFE spend from 2026 based on: 1) strong memory and multi-year capacity expansion cycles for Chinese memory companies; 2) high utilization rates at domestic foundries/IDMs; and 3) China’s drive to catch up in leading-edge logic/memory production,” the UBS report to clients states. “We also expect continued upside from China’s pursuit of self-sufficiency in production in the longer term.”</p><p>Among the key factors highlighted in the report is the accelerating substitution of foreign tools with domestic equipment: A growing portion of WFE spending is flowing to Chinese equipment makers rather than U.S., Japanese, or Dutch suppliers. Foreign vendors are seeing their China revenue decline, while local players are expanding at significantly faster rates. The absolute spending level may look stable, but the market share mix is changing in favor of domestic vendors.</p><p>Among Chinese WFE vendors, UBS tracks ACM Research, Advanced Micro-Fabrication Equipment (AMEC), and Naura Technology Group. ACM makes tools for cleaning, electrochemical plating, wafer polishing, and plasma-enhanced chemical-vapor deposition (CVD), whereas AMEC and Naura specialize in etching and CVD systems.</p><p>UBS estimates that ACM Research earned $986 million* in 2025 and expects its revenue will reach $1.185 billion* in 2026 and $1.703 billion* in 2027. AMEC is projected to earn $1.785 billion* in 2025, increase its revenue to $2.62 billion* in 2026, and achieve earnings of $3.653 billion* in 2027. When it comes to Naura, UBS models its 2025 revenue to be about $5.706 billion*, 2026 revenue to reach $7.664 billion*, and 2027 revenue to hit $10.276 billion*.</p><p>While forecasting total sales of all wafer fabrication tools manufactured in China over the next few years remains challenging, the share of spending on tools produced by ACM Research, AMEC, and Naura is projected to increase from about 20% of China’s total WFE outlays in 2025 to 24% in 2026 and 31% in 2027. It remains unclear whether UBS’s model already incorporates the proposed 50% domestic tools requirement into these projections.</p><p><em>*Figure converted from Chinese yuan</em></p><h3 class=\"wp-block-heading\"><strong>Lithography remains a key bottleneck</strong></h3><p>While the reportedly informal 50% domestic tools policy could certainly help local producers of SPE, the larger discussion revolves around one central question: How fast—and how far—can China realistically advance toward WFE self-sufficiency for advanced-node manufacturing?</p><p>The rapid revenue growth of ACM Research, AMEC, and Naura stems not only from high demand for their tools but also from the fact that these companies produce world-class cleaning, deposition, etching, and plating equipment.</p><p>“These [tools] are a different vector [compared with lithography]; [they are about] cleanliness, how many atoms of dirt can your process tolerate,” Jon Peddie, president of Jon Peddie Research, told EE Times.</p><p>“In chip equipment categories like deposition, etching, cleaning, and annealing, Chinese suppliers already have compellingly competitive systems,” Hutcheson said.</p><p>As a result, substitution is already occurring in non-lithography tools. There is an important caveat, though: A significant portion of fab tools produced in China still relies on components sourced from Western suppliers, many of which are already restricted under export controls or are expected to face similar restrictions in the near future. Thus, Chinese companies must replace these components sooner than later.</p><p>Import-export controls have also accelerated another shift: component-level localization. Over the past several years, Chinese toolmakers have qualified non-U.S. alternatives for pumps, mass-flow controllers, and other subsystems. While fully eliminating foreign content remains complex, dependence on U.S. components has fallen to low-single-digit levels in many cases, some analysts told EE Times.</p><p>By contrast, Shanghai Micro Electronics Equipment (SMEE), China’s leading lithography tool maker, can produce steppers good enough only for 90-nm, 110-nm, and more mature nodes. There were claims of progress toward a 28-nm-capable immersion ArF DUV SSA/800- to 10-W lithography machine in late 2023, but the company has not disclosed the device’s performance. Yet resolving 28-nm features in demonstration mode is fundamentally different from sustaining 200 wafers per hour in high-volume manufacturing. Furthermore, SMEE has never confirmed that mass production of this tool has begun.</p><p>There have been <a href=\"https://www.ft.com/content/8fd79522-e34f-4633-bc87-ef0aae2d9159\">reports</a> that SMIC was testing a 28-nm-capable tool from Shanghai Yuliangsheng Technology Co., but it was unclear how close this scanner—which seems to have capabilities similar to those of ASML’s Twinscan NXT:1950i from 2008—was to mass production. To that end, lithography and advanced inspection tools continue to be a bottleneck for China’s semiconductor industry.</p><p>That said, it is unsurprising that some Chinese fabs are upgrading stages on their ASML Twinscan NXT:1980i tools to improve overlay accuracy, throughput, and effective resolution using components sourced from the secondary market. For now, this might be the only way to get something better compared with what they already operate.</p><p>“You work with what you have got,” Peddie said.</p><p>An added benefit of such actions is that engineers from these fabs (or their domestic suppliers) can attempt to reverse-engineer these components and build spare parts for their existing tools, which will reduce or even eliminate their dependence on ASML and/or other Western suppliers.</p><p>“Given China’s machining expertise, it is fairly easy to clone components from existing machines and reproduce spares,” Hutcheson said. “All they need is an existing tool to clone its parts. What is harder is mastery of special coatings, engineered material systems, and software.”</p><h3 class=\"wp-block-heading\"><strong>Decades away</strong><strong></strong></h3><p>With its 7-nm-class process technologies, SMIC has proven that it can brute-force advanced nodes through multipatterning on ASML’s immersion DUV tools, but throughput and economics remain limiting factors. Yet given the state of domestic lithography tools, it is unlikely that China’s advanced nodes will rely on them in the foreseeable future.</p><p>“Assuming that we are talking about state-of-the-art lithography tools, look at how fast ASML can generate a new generation—and they own the secret sauce,” Peddie said. “The complexity of today’s lithography tools rests somewhere between black magic and cold fusion. China has demonstrated it can get up to speed pretty fast, but you can use aircraft carriers, Gen 7 fighter jets, moon launches, and AI as a guide: It does not happen fast, regardless of the resources you pour into it.”</p><p>Advancing from 28 nm to 16 nm or 7 nm is not a straightforward step for any equipment manufacturer, as the transition introduces entirely new requirements in precision and process control that fundamentally increase complexity and costs. While ASML’s 28-nm-class immersion DUV tools, such as the Twinscan XT:1930i and XT:1950i, were already mature by about 2010, achieving 5-nm-class capability with DUV systems, such as the Twinscan NXT:2000i, by about 2020 required far more advanced techniques. These included self-aligned quadruple patterning, sophisticated optical proximity correction, new masks, and even new resists.</p><p>While each of these capabilities can be developed, it is an incremental step-by-step process without any shortcuts. Furthermore, systems such as the NXT:2000i—alongside the more advanced NXT:2050i and NXT:2100i—depend on a tightly integrated ecosystem of parts and raw materials tailored precisely for these tools.</p><p>In fact, it may be decades before the Chinese semiconductor industry catches up with the Western semiconductor industry in advanced immersion DUV lithography tools and even more for Low-NA EUV tools, Hutcheson said.</p><p>“When I got in the industry in 1978 to 1979, China’s lithography tool industry was a mere single generation behind the West and was anxious to get foreign tools,” TechInsights’ Hutcheson said. “Almost 50 years later, they are multiple generations behind. To answer your question directly on a like-for-like basis, I would say [it will take] three to five years for KrFlithography systems to catch up, a couple more for ArF, 10 to 15 years for ArF immersion, and 20 to 30 years, if at all, for Low-NA EUV. Even the Japanese could not pull off EUV, and they have some of the finest lens makers in the world.”</p><p>Such a huge gap naturally begs questions about whether China can catch up—and, if it does, whether it can sustain competitiveness in terms of volumes and costs.</p><p>“China has a vertically integrated supply chain,” Hutcheson said. “It does not have to forecast the market and align an external chain, all the while meeting requirements for quarterly earnings gains. People forget that it is still a communist economy when the CCP wants it to be. The real question is whether it can achieve tools that deliver acceptable yields. Quantitative and qualitative equality to Western tools has been the real limitation to China’s ability to deliver lithography tools. That’s why many are bought and said to be sitting idle in the fab.”</p><p>Ultimately, the trajectory is clear: Domestic substitution in equipment is accelerating, government backing remains firm, and the strategic intent has not wavered but reportedly became more flexible. The open question is not direction, but timing—particularly in lithography—and pricing at the leading edge remain a concern.</p><p>“Given U.S. restrictions, China needs to develop its own chip equipment vertical,” Hutcheson said. “But it cannot afford to do this at the expense of its chip industry.”</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><h5 class=\"wp-block-heading\">Read also:</h5> </div>",
            "pub_date": "2026-02-19 01:56:50",
            "link": "https://www.eetimes.com/how-china-struggles-to-reach-wfe-self-sufficiency/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Korean Startup Takes On Cost and Latency With LLM-Specific Chip",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>South Korean AI chip startup HyperAccel is preparing to launch its Bertha 500 chip, an LLM inference accelerator designed for economical token generation in the data center. The company already has an FPGA-based server on the market with a data center chip and an edge chip imminent.</p><p>Startup competitors in this field have found success delivering very fast tokens, attacking a perceived weakness in incumbent GPU architectures—their single-user token speeds. By contrast, rather than compete directly on performance, HyperAccel’s key value proposition is based on economics, Yongwoong Jung, chief strategy officer at HyperAccel, told EE Times.</p><p>“We are trying to be a more affordable provider… that’s why we chose LPDDR, which is only one-tenth of HBM’s bandwidth, but since we are utilizing that bandwidth twice as well as GPUs, and because of the architecture of our computation units, we can produce 5× more tokens per second [for the same amount of TOPS],” Jung said. “That’s how we overcome the weakness of our DRAM bandwidth, but we still achieve value for money; that’s our value proposition.”</p>\n<p>Making better use of DRAM bandwidth means HyperAccel has perfectly good performance at human-readable speeds, a key target application for LLMs today. The most expensive GPUs are often overkill in this scenario, Jung said.</p>\n\n<p>“Our approach is to reduce the cost, sacrificing a little performance if needed, but targeting a very large market,” he said. “For current GPU products, only big companies can use them because of the price.”</p>\n<p>That said, even big companies such as OpenAI have requirements for cheaper hardware so they can service users still at the free tier, Jung said. The result will be an increasingly heterogeneous AI data center.</p><p>“We are not trying to replace GPUs for the entire world, we are trying to find our own sweet spot,” Jung said. “Whether it’s the prefill stage or decode stage, or it could be the bigger model or smaller model—we are trying to find the sweet spots.”</p><p><strong>FPGA-based server</strong></p><p>HyperAccel was founded by KAIST Professor Jooyoung Kim, along with a group of his students at the beginning of 2023. After presenting at Hot Chips in 2023, the group received an offer for their AI accelerator IP, but chose instead to become a chip company and decided to raise a seed round, HyperAccel cofounder Seungjae Moon told EE Times.</p><p>HyperAccel’s first product is an FPGA-based server, Orion, with the company’s AI accelerator chip IP. FPGAs are fairly resource-limited by AI standards, but Orion was sufficient to get the attention of some big tech companies such as Korean hyperscaler Naver Cloud, with whom the company now has a joint development agreement, Moon said.</p><p>“We wanted to understand their needs instead of just creating the highest spec product we can make,” he said.</p><p>The startup also has a partnership with LG to make an edge chip for on-device AI acceleration.</p><p><strong>Architecture</strong></p><p>The key differences between HyperAccel’s LPU and leading GPUs lie in its use of LPDDR instead of expensive HBM, compensating for lower bandwidth by achieving around 90% memory bandwidth utilization. This is done largely by eliminating traditional memory hierarchies, Moon said. Further efficiencies come from specializing in inference and transformer/LLM workloads.</p><p>“GPUs have a huge structural mismatch [with LLM inference],” Moon said. “When running LLM inference, they are only able to achieve around 45% memory bandwidth utilization because of their complex hierarchy—going from memory to compute cores needs to go all the way through the hierarchy. They also have too many compute units for what LLM inference needs, so they only achieve around 30% compute utilization. And because they are too highly spec’d [for inference], they have a high price.”</p><p>HyperAccel has closely matched memory bandwidth to compute so that data can be streamed in quickly rather than having to go through caches. Local memory units are exactly sized for LLM inference, and the instruction scheduling unit is able to stream all the AI model data without any stalling, Moon said.</p><p>GPUs also require data to be reformatted or reshaped between HBM and SRAM, Moon said, whereas HyperAccel stores formatted data in its DRAM, which can be loaded directly into compute, bypassing SRAM, and avoiding any back-and-forth. HyperAccel also uses one large compute core instead of many small cores. These architectural features mean the company can get more tokens per second from less compute—around 5× the tokens per second when normalized to the amount of compute power, relative to Nvidia Hopper-generation GPUs, Moon said.</p><p>Bertha 500 has taped out on Samsung 4 nm. It will offer 768 TOPS (INT8, but also support FP16 and other 16, 8, and 4-bit formats) from 32 LPU cores with 256 MB SRAM. There are also quad Arm Cortex-A53 cores on the chip. It has DRAM bandwidth of 560 GB/s (8 channels of LPDDR5x). Batch sizes up to 1024 are supported.</p><p>The result should be around 20× the throughput per dollar versus an Nvidia H100 (the cost will be around one-tenth) and around 5× the power efficiency. Bertha 500 will run on around 250 W.</p><p>Future generations of the architecture may look at processor-in-memory technologies to help the decode stage get even closer to memory, Moon said.</p><p><strong>System and software</strong></p><p>For large models, accelerator-to-accelerator communication is required. GPUs can connect directly to each other using protocols like NVLink, but since they are programmed with kernels, they also require a runtime system call, which means there still has to be some communication with the host CPU. HyperAccel’s architecture doesn’t require any intervention from the host since the chip already knows where and when memory transitions need to happen, a side-effect of being LLM-specific. This transfer is controlled by a memory controller on the chip.</p><p>HyperAccel’s ESLink (expandable synchronization link, analogous to NVLink), which connects accelerator chips, can overlap communication and computation, because it knows when everything needs to happen. This makes for more scalability, Moon said.</p><p>HyperAccel’s software stack supports all models in the HuggingFace repository, and inference serving engine vLLM. The company is working on a domain-specific language (DSL) it calls Legato, which will give developers access to the lower levels of the stack. There will also be AI agents available to help them do this, including learning Legato, Moon said, once Bertha 500 is released.</p><p><strong>Edge SoC coming</strong></p><p>As well as Bertha 500, HyperAccel is also creating a scaled-down edge version for applications including automotive, consumer electronics, and robotics, as part of a joint development agreement with LG Electronics. This chip will be able to handle text-to-speech or speech-to-text models, for example.</p><p>The SoC jointly developed with LG will use HyperAccel’s accelerator IP paired with some of LG’s in-house IP (potentially blocks like PHY and memory controller IP) and an Arm Cortex-A55, with LG providing backend services and HyperAccel doing design services. (This is the first time LG is providing backend services to a third party, Jung said). HyperAccel will sell this chip into edge applications beyond LG. Dubbed Bertha 100, with the numbers relating to memory bandwidth, not compute cores, the SoC will use two channels of LPDDR5x. Samples are due in the fourth quarter of 2026, and the accelerator will come on an M.2 card.</p><p>HyperAccel has raised $45 million so far and is valued at $200 million with a team of 77. Bertha 500 samples are due around the end of the first quarter of 2026, with mass production due to start in early 2027.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><h5 class=\"wp-block-heading\">Read also:</h5> </div>",
            "pub_date": "2026-02-18 17:38:38",
            "link": "https://www.eetimes.com/korean-startup-takes-on-cost-and-latency-with-llm-specific-chip/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "India to Add 20,000 GPUs as AI Mission 2.0 Expands Compute and Chip Push",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p><strong>NEW DELHI, India </strong>— On Feb. 17, 2026, at the India AI Impact Summit in New Delhi, Union Minister for Electronics and Information Technology Ashwini Vaishnaw said India will expand its AI compute capacity beyond 38,000 GPUs, adding 20,000 more units in the coming weeks under its AI Mission 2.0.</p><p>Orders for the new GPUs will be placed within a week and are expected to be deployed over the next six months, Vaishnaw said at a press briefing, responding to an EE Times question on how India plans to strengthen the chip ecosystem to sustain AI-driven demand. The expansion forms part of what he described as “AI Mission 2.0,” with a stronger focus on R&amp;D, innovation, AI diffusion, and common compute infrastructure.</p><p>“It is a constant endeavor to provide high-quality resources to our startups, researchers, and students,” Vaishnaw said.</p>\n<p>The compute expansion also comes against the backdrop of the 2026 India-U.S. trade framework, under which the two countries agreed to significantly increase trade in technology products, including GPUs and other data center components. The agreement includes India’s intent to purchase $500 billion worth of U.S. energy products, aircraft, technology goods, and critical materials over five years, while expanding joint technology cooperation.</p>\n\n<p>Vaishnaw’s announcement builds on the progress of the <a href=\"https://indiaai.gov.in/\">IndiaAI Mission</a>, which was approved in March 2024 with a ₹10,371.92 crore (~$1.14 billion) outlay over five years. The mission initially targeted 10,000 GPUs but has already reached 38,000 deployed units, offered at a subsidized rate of ₹65 per hour (approximately 72 cents per hour). With an additional 20,000 GPUs in the pipeline, India’s publicly supported AI compute pool is set to expand further over the next six months, widening access for startups and academic researchers.</p>\n<p>Since its launch, the IndiaAI Mission has built out seven pillars, including subsidized compute, foundation model development, startup financing, and safe AI governance. Twelve startups have already been selected to develop indigenous multimodal foundation models using India-specific datasets.</p><p>The India AI Impact Summit itself serves as a showcase for these efforts, bringing together policymakers, researchers, and industry players to position India as both a consumer and <a href=\"https://www.eetimes.com/why-indias-deep-tech-moment-matters-to-the-global-chip-industry/\">producer of AI technologies</a>.</p><h3 class=\"wp-block-heading\"><strong>Semiconductor alignment under ‘Semicon 2.0’</strong></h3><p>Vaishnaw said earlier projections of $1 trillion in AI-related investment, including $90 billion already committed, are likely to be exceeded. Of that, $200 billion is linked to infrastructure investment, with an additional $70 million committed by venture capital firms in deep tech and application layers.</p><p>Based on current commitments, he said more than $400 billion in investment could materialize across five layers of the AI stack over the next two years.</p><p>Government reports project that India’s technology sector will cross $280 billion in revenue this year, while AI could add $1.7 trillion to the economy by 2035. The country currently employs around six million people in the tech and AI ecosystem.</p><p>Vaishnaw said the semiconductor strategy is being aligned with AI requirements. “In Semicon 2.0, design will be the primary focus,” he said, adding that at least 50 deep-tech companies are expected to emerge from India in the coming years.</p><p>The government is also preparing for commercial production at a large memory chip facility. “Announcements will be made shortly,” he said, adding that many companies have expressed interest in memory manufacturing in India.</p><p>Under the second phase of the India Semiconductor Mission (ISM 2.0), research projects are being undertaken to develop IP in high-density memory technologies. According to Vaishnaw, leading researchers have responded positively to these initiatives.</p><p>Vaishnaw stressed the need to control the technology stack and framed the approach around strategic autonomy. “The key point is that we should be able to control our own destiny,” he said. “Industry, government, and academia are committed to ensuring that our technological future remains in our own hands.”</p><p>That emphasis extends across the AI stack, from compute and semiconductors to energy infrastructure. The minister said 51% of India’s power generation capacity now comes from clean sources, positioning the country as an attractive destination for AI data center investments.</p><p>With compute capacity set to expand significantly and semiconductor design moving to the foreground, the next phase of India’s AI strategy will test whether infrastructure growth can translate into globally competitive products and IP.</p> </div>",
            "pub_date": "2026-02-18 08:21:37",
            "link": "https://www.eetimes.com/india-to-add-20000-gpus-as-ai-mission-2-0-expands-compute-and-chip-push/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Nvidia China Gamble Meets Washington’s Regime",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>After three years of stalled global semiconductor trade, the situation has changed. In mid-January 2026, the Trump administration <a href=\"https://www.eetimes.com/nvidia-bets-big-on-china-with-h200-push/\" rel=\"noreferrer noopener\" target=\"_blank\">reopened the Chinese market to American chip companies</a>. However, this move comes with high tariffs, strict logistics, and an uneasy legislative agreement.</p><p>Nvidia and AMD now face a series of new compliance rules as they try to meet strong demand from China. These requirements could change their supply chains as much as their chips have changed AI.</p><p>On Jan. 13, the U.S. Commerce Department’s Bureau of Industry and Security (BIS) codified a final rule moving the <a href=\"https://www.federalregister.gov/documents/2026/01/15/2026-00789/revision-to-license-review-policy-for-advanced-computing-commodities\" rel=\"noreferrer noopener\" target=\"_blank\">export licensing policy for advanced computing commodities</a> from a “presumption of denial” to a “case-by-case review.”</p>\n<p>The new rules, effective Jan. 15, focus on hardware like Nvidia’s H200 and AMD’s MI325X. These chips aren’t the most advanced, but they’re still important for training large language models. However, access comes at a cost: a 25% revenue-sharing tariff set by <a href=\"https://www.whitehouse.gov/fact-sheets/2026/01/fact-sheet-president-donald-j-trump-takes-action-on-certain-advanced-computing-chips-to-protect-americas-economic-and-national-security/\" rel=\"noreferrer noopener\" target=\"_blank\">Presidential Proclamation</a>, meant to profit from China’s tech growth and support U.S. industry.</p>\n\n<h3 class=\"wp-block-heading\">Architecture of the controlled access</h3><p>The new rules replace total embargoes with more targeted controls. The <a href=\"https://www.federalregister.gov/documents/2026/01/15/2026-00789/revision-to-license-review-policy-for-advanced-computing-commodities\" rel=\"noreferrer noopener\" target=\"_blank\">BIS guidelines</a> allow the sale of chips with a total processing performance (TPP) below 21,000 and memory bandwidth under 6,500 GB/s can be exported. This applies to the H200, which has high memory for AI tasks, but keeps Nvidia’s most advanced Blackwell chips off the Chinese market.</p>\n<p>The biggest challenge isn’t technical, but logistical. To prevent these chips from reaching military users, the administration requires all shipments to China to pass through the U.S. for third-party checks. This process also ensures the 25% tariff is collected, acting as a fee on China’s AI development.</p><p class=\"has-text-align-center\"><strong>Cost of doing business in China</strong></p><figure class=\"wp-block-table\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>Cost Component</strong></td><td><strong>2025 “Black Market” Era</strong></td><td><strong>2026 Authorized Regime</strong></td></tr><tr><td><strong>Regulatory Status</strong></td><td>Illegal / Smuggled</td><td>Licensed / Case-by-Case Review</td></tr><tr><td><strong>Logistics Flow</strong></td><td>Taiwan -&gt; Intermediary -&gt; China</td><td>Taiwan -&gt; USA -&gt; China</td></tr><tr><td><strong>Tariff / Surcharge</strong></td><td>N/A (Smuggling cut &gt;100%)</td><td>25% U.S. Treasury Tariff</td></tr><tr><td><strong>Payment Terms</strong></td><td>Cash / Crypto</td><td>100% Upfront, Non-Refundable </td></tr><tr><td><strong>Volume Cap</strong></td><td>Highly Constrained</td><td>Capped at 50% of U.S. Sales Volume</td></tr></tbody></table></figure><p class=\"has-text-align-center\"><em>Source: </em><a href=\"https://www.federalregister.gov/documents/2026/01/15/2026-00789/revision-to-license-review-policy-for-advanced-computing-commodities\" rel=\"noreferrer noopener\" target=\"_blank\"><em>Bureau of Industry and Security</em></a></p><p>Under Secretary for Industry and Security <a href=\"https://www.bis.gov/press-release/department-commerce-revises-license-review-policy-semiconductors-exported-china\" rel=\"noreferrer noopener\" target=\"_blank\">Jeffrey Kessler described the pivot as a necessary evolution</a>, stating, “Permitting the sale of the H200 to China under controlled conditions will strengthen the American technology ecosystem.”</p><p>The rationale suggests a strategic wager: that the U.S. can extract capital from Chinese tech giants like ByteDance and Alibaba to subsidize American R&amp;D, all while keeping Beijing technologically dependent on hardware that’s one generation behind.</p><h3 class=\"wp-block-heading\">China’s ‘matchmaking’ strategy</h3><p>China’s response is also strategic. At first, its government <a href=\"https://www.reuters.com/world/china/chinas-customs-agents-told-nvidias-h200-chips-are-not-permitted-sources-say-2026-01-14/\" rel=\"noreferrer noopener\" target=\"_blank\">stopped H200 shipments</a> to protest the new tariffs, but the need for more computing power soon led them to back down. By late January, Beijing authorized initial purchases for major technology firms, including a massive procurement of H200-class clusters by ByteDance, <a href=\"https://investor.wedbush.com/wedbush/article/tokenring-2026-1-27-the-great-ai-re-balancing-nvidias-h200-returns-to-china-as-jensen-huang-navigates-a-new-geopolitical-frontier\" rel=\"noreferrer noopener\" target=\"_blank\">reportedly valued at $14 billion for 2026 alone</a>.</p><p>However, access comes with limits. The Ministry of Industry and Information Technology now <a href=\"https://www.reuters.com/world/china/nvidia-requires-full-upfront-payment-h200-chips-china-sources-say-2026-01-08/\" rel=\"noreferrer noopener\" target=\"_blank\">requires Chinese buyers to purchase a set number of domestic chips</a>, mainly Huawei’s Ascend series, for every Nvidia H200 they import. This policy means Chinese companies must use local chips for basic tasks and save the costly Nvidia hardware for advanced training.</p><p>These rules put Chinese tech companies in a tough spot. They have to pass U.S. “Know Your Customer” checks to show they aren’t military users, while also meeting China’s demands to support local companies like Huawei.</p><p>This has created a split system with many obstacles, yet Western chips remain crucial. By early February, Chinese companies ordered <a href=\"https://www.taipeitimes.com/News/biz/archives/2026/01/01/2003849833\" rel=\"noreferrer noopener\" target=\"_blank\">more than 2 million H200 chips</a>, far more than Nvidia currently has available.</p><h3 class=\"wp-block-heading\">Supply chain and capacity wars</h3><p>For Nvidia and AMD, the new agreement has caused a supply chain problem. Sending chips made in Asia through the U.S. for testing before shipping them back to Asia adds time and cost. The increased demand from China also puts pressure on TSMC, the main manufacturer for both companies.</p><p>Nvidia has asked TSMC to make more H200 chips, but these use the same advanced packaging as the top Blackwell chips. TSMC’s lines are already almost fully booked to supply U.S. companies like Microsoft and Amazon, so making more H200s could reduce production of newer, more profitable chips.</p><p>“We ‍continuously manage our supply ⁠chain. Licensed sales of the H200 to authorized customers in China will have no impact on our ability to supply customers in the United States,” Nvidia ​said ‌in December in a <a href=\"https://www.reuters.com/world/china/nvidia-considers-increasing-h200-chip-output-due-robust-china-demand-sources-say-2025-12-12/\" rel=\"noreferrer noopener\" target=\"_blank\">statement to Reuters</a>.</p><p>The BIS rule makes things more difficult by imposing a “domestic protection” cap. Nvidia must prove that the total TPP sent to China is no more than half of what goes to U.S. customers. It links China’s chip supply to how much American data centers use. If U.S. demand drops, China’s export limit also falls, making it hard to predict supply needs.</p><p>Nvidia’s leaders still sound confident. In November 2025, on the company’s <a href=\"https://s201.q4cdn.com/141608511/files/doc_financials/2026/q3/NVDA-Q3-2026-Earnings-Call-19-November-2025-5_00-PM-ET.pdf\" rel=\"noreferrer noopener\" target=\"_blank\">Q3 2026 earnings call</a>, CFO Colette Kress said the company expects “visibility to half a trillion dollars in Blackwell and Rubin revenue,” and “last month, in partnership with TSMC, we celebrated the first Blackwell wafer produced on US soil,” but also said, “we are not assuming any data center compute revenue from China” for Q4. This cautious approach shows how unpredictable the market is under current regulations.</p><h3 class=\"wp-block-heading\">U.S. Congress’ guillotine</h3><p>The stability of this new trade regime is far from guaranteed. While the executive branch has utilized its regulatory authority to open the door, Congress is actively constructing a lock. On Jan. 21, the <a href=\"https://www.reuters.com/legal/litigation/us-house-panel-vote-bill-give-congress-authority-over-ai-chip-exports-2026-01-21/\" rel=\"noreferrer noopener\" target=\"_blank\">House Foreign Affairs Committee advanced</a> the AI Overwatch Act (H.R. 6875).</p><p>The legislation views the sale of H200 chips not as a commercial opportunity but as a national security lapse. “Should Congress have oversight when selling missiles to other countries? Yes, the same should be said for chips,” Rep. Mast argued during a Jan. 14 hearing.</p><p>The bill proposes a mandatory 30-day congressional review period for any license involving “covered integrated circuits” to countries of concern and, most critically, retains the authority to revoke licenses retroactively.</p><p class=\"has-text-align-center\"><strong>Key legislative threats to semiconductor exports</strong></p><figure class=\"wp-block-table\"><table class=\"has-fixed-layout\"><tbody><tr><td><strong>Bill / Act</strong></td><td><strong>Status (as of Feb 2026)</strong></td><td><strong>Key Provision Impacting Nvidia/AMD</strong></td></tr><tr><td><strong><a href=\"https://www.congress.gov/bill/119th-congress/house-bill/6875\" rel=\"noreferrer noopener\" target=\"_blank\">AI Overwatch Act</a> (H.R. 6875)</strong></td><td>Passed House Committee</td><td>Requires 30-day Congressional notification; allows retroactive license revocation.</td></tr><tr><td><strong><a href=\"https://www.congress.gov/bill/119th-congress/house-bill/2683\" rel=\"noreferrer noopener\" target=\"_blank\">Remote Access Security Act</a></strong></td><td>Passed House (Jan 12)</td><td>Extends export controls to cloud computing, closing the “rental” loophole for restricted chips.</td></tr><tr><td><strong><a href=\"https://www.congress.gov/bill/119th-congress/house-bill/5885/all-info\" rel=\"noreferrer noopener\" target=\"_blank\">GAIN AI Act</a></strong></td><td>Pending</td><td>Mandates U.S. businesses get first priority on chips before export.</td></tr></tbody></table></figure><p class=\"has-text-align-center\"><em>Source: Congress.gov, House Foreign Affairs Committee</em></p><p>Because of this legislative risk, Nvidia has changed its payment terms. The company now requires <a href=\"https://www.reuters.com/world/china/nvidia-requires-full-upfront-payment-h200-chips-china-sources-say-2026-01-08/\" rel=\"noreferrer noopener\" target=\"_blank\">Chinese customers to pay the full amount upfront</a>, with no refunds, before production starts. This means buyers take on all the risk. If Congress passes the AI Overwatch Act and cancels export licenses while chips are still in U.S. labs, companies like ByteDance or Alibaba would lose their money.</p><h3 class=\"wp-block-heading\">Fragile equilibrium</h3><p>Despite political uncertainty, the market is quickly reconnecting. AMD, hoping to benefit from increased demand, <a href=\"https://www.aastocks.com/en/stocks/news/aafn-con/NOW.1500911/\" rel=\"noreferrer noopener\" target=\"_blank\">reported $390 million in older-chip sales to China</a> in Q4 2025 and is working to get its MI325X accelerator approved under the new 21,000 TPP limit.</p><p>For AMD, China is a key growth opportunity to offset falling sales of older chips, as long as it can meet the same testing and tariff requirements as Nvidia.</p><p>Reopening the chip trade ends the period of complete separation and marks the start of a new phase of “managed entanglement.” The U.S. government now acts as a gatekeeper, hoping to slow China’s tech progress by selling better, but taxed, American chips.</p><p>Still, this balance is unstable. The AI Overwatch Act is moving forward in the Senate, and Chinese chipmakers like <a href=\"https://www.iluvatar.com/\" rel=\"noreferrer noopener\" target=\"_blank\">Iluvatar CoreX</a> are planning to match Nvidia’s technology by 2027. This means the current trade window could close soon.</p><p>Even though trade has resumed and production is underway, the number of H200 chips reaching China remains low as of mid-February due to complex new rules from both the U.S. and China. In 2026, the most valuable resource is no longer oil or data, but computing power.</p><p><strong><em>Author’s note</em></strong><em>: This article is based on publicly available information and doesn’t constitute financial or investment advice.</em></p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><h5 class=\"wp-block-heading\">See also:</h5><p><a href=\"https://www.eetimes.com/nvidia-bets-big-on-china-with-h200-push/\" rel=\"noreferrer noopener\" target=\"_blank\">Nvidia Bets Big on China With H200 Push</a></p><p><a href=\"https://www.eetimes.com/tsmc-to-lead-rivals-at-2-nm-node-analysts-say/\" rel=\"noreferrer noopener\" target=\"_blank\">TSMC to Lead Rivals at 2-nm Node, Analysts Say</a></p><p><a href=\"https://www.eetimes.com/risc-v-pivots-from-academia-to-industrial-heavyweight/\" rel=\"noreferrer noopener\" target=\"_blank\">RISC-V Pivots from Academia to Industrial Heavyweight</a></p><p><a href=\"https://www.eetimes.com/alibaba-unveils-own-ai-chip-mounting-direct-challenge-to-nvidia/\" rel=\"noreferrer noopener\" target=\"_blank\">Alibaba Unveils Own AI Chip, Mounting Direct Challenge to Nvidia</a></p><p></p> </div>",
            "pub_date": "2026-02-18 03:43:29",
            "link": "https://www.eetimes.com/nvidia-china-gamble-meets-washingtons-regime/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Former Altera CEO Joins French AI Chip Startup",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>Former Altera CEO Sandra Rivera has joined French AI inference chip startup Vsora as chair of the board.</p><p>Noting that Vsora has so far been flying under the radar somewhat, Rivera told EE Times that part of her remit will be to raise the company’s profile as it moves towards deploying its Jotunn8 data center AI inference accelerator product this year.</p><p>“I was not looking for a company like Vsora,” Rivera said. “Once we got to know each other…I became so [excited about] the possibility of helping them, because I believe they have the right product at the right time.”</p>\n<p>Led by Khaled Maalej, Vsora’s team has been through 14 successful tape outs together, both since founding Vsora in 2015 and at earlier wireless chip startup <a href=\"https://www.eetimes.com/french-conglomerate-buys-dibcom/\">DiBcom, acquired by Parrot</a> in 2011.</p>\n\n<p>“[Maalej] and the team are so impressive,” Rivera said. “From the perspective of building such a complex logic device with such a small team, and also, having access to the latest process technology from TSMC, which is a strong validation that they are doing something highly differentiated with an extremely tight set of resources.”</p>\n<p>Like other AI chip startups, Vsora will address the memory wall with its first data center design. <a href=\"https://www.eetimes.com/vsora-tapes-out-ai-inference-chip-for-data-centers/\">Jotunn8, which taped out on TSMC 5 nm in October</a>, is a chiplet-based design with eight stacks of HBM3 for a total of 288 GB.</p><p>“There are only three companies that have as much memory as [Jotunn8]—only Nvidia and AMD are in that camp,” Rivera said. “[Startup competitors] are focused on lower performance and much less memory, which means they can’t handle the size of models we can with our chip, they’re relegated to smaller models.”</p><p>Despite using HBM, the company is also focusing on cost-per-token, driven by exceptional performance, Rivera said.</p><p>“We are not going head-to-head with Nvidia, that’s a fool’s errand,” she said. “Our recognition of what’s happening in the industry is that it is going to be a world of opportunity in heterogeneous architectures.”</p><p>There will continue to be a need for the right architecture for the right part of the workload, she said.</p><h3 class=\"wp-block-heading\"><strong>Market pivot</strong></h3><p>Vsora has pivoted away from making <a href=\"https://www.eetimes.com/vsora-unveils-ai-chip-family-to-enable-l2-l5-autonomous-driving/\">the automotive chips it started with</a> after producing several automotive family members, nevertheless building on the architecture and experience with these first generations of product, Rivera said.</p><p>“The architecture was proven, Vsora validated it with customers in Europe, the U.S., and Japan, which proved they really did have a unique approach to addressing the memory wall problem,” she said. “That allowed Vsora to embrace generative AI as an opportunity, to adapt the architecture to a different part of the opportunity space.”</p><p>The pivot brings a proven architecture developed for low latency and determinism to the part of the market that is growing the fastest, Rivera said.</p><p>First samples of Jotunn8 should be back in time to submit <a href=\"https://mlcommons.org/2025/04/mlperf-inference-v5-0-results/\">MLPerf inference</a> scores later this summer, something Rivera said the team is actively looking forward to. The rest of 2026 will be spent working with ecosystem partners on developing card and system designs ready to ramp up in 2027. The company has raised €40 million (about $47 million) to date, by no means a large amount considering Jotunn8 is the company’s fourth tape out. Vsora will look to Silicon Valley venture capitalists for a more significant funding round towards the end of 2026, Rivera said.  </p><p>“It’s pretty special, and it should be a point of pride that a European company is able to be competitive with all of these other companies that probably have way more money and that are typically not coming from Europe,” she said. “I think there’s a lot of interest in investing more in Europe, in European companies, in Europe and for Europe.”</p><p>Vsora has been supported by funding from the European Innovation Council (EIC) Fund, and while the company isn’t a sovereign chip play, being French gives the company access to sovereign AI markets in the EU and beyond. Vsora plans to be part of Europe’s ambition to build AI “gigafactories,” Rivera said.</p><p>As chair of the board, Rivera’s first tasks are to raise Vsora’s profile, raise capital, and help guide a go-to-market strategy for the startup.</p><p>“It is very easy for a small company to get stretched too thin, too fast,” she said. “So, being very crisp about where we’re targeting, who we’re targeting, how we’re targeting our initial revenue sales and goals is super important, because we can’t afford to be stretched thin. We want to be excellent and make good first impressions and be good partners to our prospective customers.”</p><p>This will include hiring both sales and technical teams, especially in critical markets like the U.S., Rivera said.</p> </div>",
            "pub_date": "2026-02-17 22:16:37",
            "link": "https://www.eetimes.com/former-altera-ceo-rivera-joins-french-ai-chip-startup-vsora/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Chip Assembler ASE Sees Advanced Packaging Sales Doubling",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>ASE, the world’s largest chip assembler, expects its advanced packaging sales to double this year to $3.2 billion on strong demand from AI chipmakers, such as Nvidia. ASE said it will increase its 2026 capital expenditure by an undisclosed amount from the $5.5 billion it spent last year.</p><p>The Taiwanese company is buying cleanrooms and building its own to expand into advanced packaging. Top foundry TSMC is likely to transfer more of the business to ASE as both companies prepare to boost production for AI customers like Nvidia, AMD, and Amazon, according to JPMorgan managing director Gokul Hariharan.</p><p>“In 2026, we believe that the primary driver for growth is likely to be on-substrate outsourcing from TSMC for Nvidia GPUs, with a meaningful step up likely in second-half 2026 as [Nvidia] Rubin GPUs start ramping,” Hariharan said in a Feb. 6 report obtained by EE Times. “Full-process packaging should start in second-half 2026, in our view, with AMD Venice CPUs being the first products, and could reach $300 million to $400 million in revenues in 2026, with further room for growth in 2027, as multiple non-GPU products from AMD are moving to ASE’s 2.5D process. In 2027, we also expect to see some [Nvidia] Vera CPU outsourcing coming to ASE, while early discussions indicate potential share gains in [Amazon] Trainium3 ASICs.”</p>\n<p><a href=\"https://www.eetimes.com/tsmc-will-struggle-to-meet-ai-demand-for-years-analysts-say/\">TSMC has struggled to keep up with AI chip demand</a> even as it has budgeted a record $52 billion to $56 billion in capital expenditure this year. TSMC, which dominates advanced packaging for customers like Nvidia and AMD, is likely to transfer more of the business to outsourced assembly-and-test companies (OSATs), <a href=\"https://www.eetimes.com/ai-chips-shifting-from-round-to-rectangular/\">according to chip toolmaker Lam Research</a>. ASE is expanding into the business this year.</p>\n\n<p>“There’s a lot of technology collaboration with our partners upstream and downstream,” ASE CEO Tien Wu told the audience at the company’s quarterly earnings event on Feb. 5. “We are building factories from scratch. We’re also acquiring existing factories from our partners, even with cleanrooms already installed.”</p>\n<p>ASE will probably be the main beneficiary of TSMC’s transfer of advanced packaging business to OSATs.</p><p>“Given strong demand and a 15% to 20% gap still in supply demand for advanced packaging at TSMC, we believe that ASE is likely to be the primary beneficiary of outsourcing in the next two to three years,” Hariharan said. “We believe that TSMC would be increasingly willing to give up more of the lower-end CoWoS processes to ASE in 2027–2028, as it readies to migrate to 3D SoICs, as well as bringing up the CoPoS process in 2028 for a key customer, Nvidia.”</p><p>Similar to TSMC’s CoWoS advanced packaging that the company has been providing for years, CoPoS places logic chips and HBM stacks side by side on an interposer, using a panel-level substrate instead of a 300-mm wafer.</p><h3 class=\"wp-block-heading\"><strong>Paradigm shift</strong></h3><p>ASE and other companies in the Taiwan ecosystem, such as Global Unichip, are expanding into technologies for AI data centers, including <a href=\"https://www.eetimes.com/lightmatter-and-guc-partner-to-produce-co-packaged-optics-cpo-solutions-for-ai-hyperscalers/\">co-packaged optics (CPO)</a>, which Wu called a “paradigm shift.”</p><p>“There are different requirements between the <a href=\"https://www.eetimes.com/the-battle-to-scale-up-the-ai-data-center/\">scale-up, scale-out,</a> chip-to-chip level, chip-to-package level, for the package-to-package level, they’re all different,” Wu said in response to a question from EE Times. “Our job is to develop the toolboxes for the designers so they have the freedom to choose.”</p><p>ASE will be among the first companies to start production of <a href=\"https://www.eetimes.com/ai-chips-shifting-from-round-to-rectangular/\">panel-level packaging</a>, which will replace the round silicon wafers that the chip industry has used for decades. The company will open production at a “lights out,” fully automated, 310 × 310-mm panel facility by the end of this year, Wu said. The company may also move to a 620 × 620-mm format depending on customer demand, he added. Larger panels would allow more die per substrate, lowering packaging cost per chip. The panels are designed for AI processors requiring integration of multiple dies and HBM.</p><p>“We are responsible to provide more packaging-level toolboxes, including panels, including CPO, and including next-generation power delivery voltage-regulator modules,” Wu said.</p><h3 class=\"wp-block-heading\"><strong>Expanding in Asia</strong></h3><p>ASE aims to expand its production capacity primarily in Asia. The move underscores the challenges the U.S. government faces in trying to shift a larger share of Taiwan’s semiconductor industry to the U.S.</p><p>“ASE is building footprint primarily in Penang [Malaysia],” Wu said. “Mainly for automotive and in the future, potentially, robotics, to capture customers and wafers that are not produced in Taiwan. We’re also building footprint in [South] Korea and the Philippines. Penang will be the main sector that we’ll be ramping up. The Penang cluster has been well-established, second to Taiwan.”</p><p>ASE hasn’t committed to following TSMC to the U.S. In March last year, <a href=\"https://www.eetimes.com/tsmcs-100-billion-investment-in-u-s-shores-up-top-foundry/\">TSMC added $100 billion to its investment in the U.S.</a> to start domestic production of AI chips, a move partly aimed at dodging tariffs that U.S. President Donald Trump was threatening to levy on imports from the Taiwanese company. TSMC said the investment would include two advanced-packaging facilities.</p><p>Taiwan’s top tariff negotiator on Feb. 8 said it would be impossible to move 40% of Taiwan’s chipmaking capacity to the U.S., responding to a demand from U.S. Commerce Secretary Howard Lutnick in January.</p><p>Taiwan’s Vice Premier Cheng Li-chiun said in an interview with the Taiwanese television channel CTS that she had informed U.S. officials that Taiwan’s semiconductor ecosystem could not be relocated.</p><p>Taiwan’s near-90% share of advanced chip production is the result of an ecosystem developed over several decades that could not be moved to the U.S., Cheng specified.</p><p>“There’s no question about the leadership in semiconductor manufacturing in Taiwan this year, as well as a few years down the road,” Wu said. “I don’t think there’s any question about our position—Taiwan’s as well ASE’s. We understand that what is driving the business in AI is mainly system optimization, which includes chip-level optimization, packaging-level optimization, as well as power delivery, silicon photonics, manufacturing efficiency. I would like to remind you [that] Taiwan has manufacturing leadership in all sectors.”</p><p>ASE declined to estimate how much it will increase capex this year because it’s a moving target.</p><p>“Half of me is saying, ‘Spend more,’” Wu said. “Half of me is saying, ‘Are you sure we’re doing the right thing?’ We have 64,000 people in Taiwan and 100,000 worldwide. You want to stretch that, but you don’t want to stretch it to the point that we all go to the hospital.”</p> </div>",
            "pub_date": "2026-02-17 07:05:51",
            "link": "https://www.eetimes.com/chip-assembler-ase-sees-advanced-packaging-sales-doubling/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "NAND Reimagined in High-Bandwidth Flash to Complement HBM",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>In the AI era, where large models like ChatGPT with a massive appetite for compute and storage are reshaping the memory world, a new storage solution is poised to revolutionize memory capacity and performance for AI inference.</p><p>High-bandwidth flash (HBF) stacks multiple layers of NAND dies—each comprising hundreds of stacked 3D NAND cell layers—to create unprecedented memory capacity, alongside daunting engineering challenges. While <a href=\"https://www.eetimes.com/hbm-innovation-outpaces-standards-development/\" rel=\"noreferrer noopener\" target=\"_blank\">high-bandwidth memory (HBM)</a> stacks DRAM chips to maximize memory bandwidth, HBF stacks 3D NAND arrays to dramatically increase parallel I/O performance.</p><p>In a recent IEEE paper, SK Hynix has unveiled a hybrid memory architecture that places both HBM and HBF alongside the GPU responsible for computation. Eight HBM3E stacks are combined with eight HBF stacks alongside Nvidia’s latest GPU, the Blackwell (B200), and simulation tests show that performance per watt improved by as much as 2.69× compared with setups relying solely on HBM.</p>\n<p>SK Hynix, the <a href=\"https://www.eetimes.com/sk-hynix-maintains-memory-leadership-with-first-hbm4/\" rel=\"noreferrer noopener\" target=\"_blank\">leading supplier of HBM devices</a>, is expected to unveil a trial version of HBF in the first quarter of 2026. Both Samsung and SK Hynix have also joined hands with Sandisk to rapidly move R&amp;D to commercialization and standardize the HBF technology. So, with the steady growth of AI workloads, HBF is expected to accomplish an earlier-than-expected commercialization.</p>\n\n<p>Sandisk plans to deliver HBF samples in the second half of 2026, and the first HBF device serving AI inference is expected in early 2027. Likewise, both Samsung and SK Hynix are targeting commercial HBF products by 2027.</p>\n<h3 class=\"wp-block-heading\"><strong>NAND reimagined for AI</strong></h3><p>HBF is a clever architectural innovation that combines 3D NAND flash with advanced packaging and interconnection technologies, as previously employed in HBM, to offer high-capacity, low-cost flash memory with data transfer speeds close to those of high-end memory.</p><p>First and foremost, it borrows the HBM design concept for high-density vertical stacking of multiple NAND flash chips. That shortens the internal data transmission paths within the memory device, bolsters integration density, and lays the foundation for high bandwidth.</p><p>The second, and more important aspect, of HBF design relates to the parallel sub-array architecture, which divides the core structure of flash memory into a large number of storage sub-arrays that can operate independently and in parallel. Therefore, unlike traditional NAND flash, where the number of channels that can simultaneously read and write data is limited, each sub-array has its own independent read/write channels.</p><p>That makes it a high-performance storage solution for read-intensive tasks like inference. But while HBF supports unlimited read cycles, it’s limited to approximately 100,000 write cycles. That, in turn, calls for AI software to be optimized for read-intensive operations.</p><p>So, while HBF doesn’t offer the ultra-low latency and extreme write speed of HBM, it provides far greater capacity at much lower cost. Next, compared to traditional solid-state drives (SSDs), HBF delivers orders of magnitude higher bandwidth for designs that require fast read access to massive data.</p><p>In short, HBF isn’t a HBM replacement but a powerful complement that aims to address the compute-memory gap, widely known as the memory wall. It’s tackling AI memory from a different angle by fusing NAND flash to achieve DRAM-like bandwidth in a single stack.</p><p>Below is a quick take on what problems HBM solves, and what challenges lie ahead in its commercial realization.</p><h3 class=\"wp-block-heading\"><strong>The inference angle</strong></h3><p>HBF, a NAND flash memory technology built into HBM-like packages, largely targets read-intensive AI inference tasks rather than latency-sensitive applications. HBF’s high capacity and throughput characteristics align with AI model storage and inference requirements, especially when hyperscalers are pushing inference to the edge.</p><p>Edge AI, which commonly employs pre-trained models, aligns with HBF’s advantages of high read bandwidth and large capacity. It’s also immune to HBF’s weaknesses of slow writes and limited endurance. Moreover, HBF’s low power consumption is well-suited for energy-sensitive edge environments.</p><p>Next, in data centers, while HBF won’t replace HBM in training workloads, it can act as an effective capacity extension for HBM. In this hybrid model, HBM serves as a high-speed cache to hold massive amounts of data urgently needed for current computation, while the AI model is stored in HBF. That’s the premise of SK Hynix’s recently unveiled hybrid memory architecture dubbed H3.</p><p>When augmented with HBM, HBF enables attaching terabytes of memory to AI accelerators. But while HBF promises extraordinary capacity, it also faces extraordinary complexity. Especially the interconnect part will be massively challenging. Besides writing endurance constraints, there are issues like NAND’s block-level addressing limitations.</p><p>However, despite significant skepticism, this memory technology built on heterogeneous stacks is rapidly making inroads due to its suitability for AI inference. HBF could achieve bandwidths exceeding 1,638 GB/s, which is a major jump compared to SSDs. And it’s also expected to reach up to 512 GB, outpacing the 64 GB provided by HBM4.</p><p>No wonder Sandisk calls this new storage solution memory-centric AI.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><h5 class=\"wp-block-heading\">Read also:</h5><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><hr class=\"wp-block-separator has-alpha-channel-opacity\"/> </div>",
            "pub_date": "2026-02-16 16:32:42",
            "link": "https://www.eetimes.com/nand-reimagined-in-high-bandwidth-flash-to-complement-hbm/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        }
    ]
}