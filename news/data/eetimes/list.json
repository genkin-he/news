{
    "data": [
        {
            "title": "Aimtron’s Design-Led Approach Secures Manufacturing Wins",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>Aimtron Electronics is strengthening its India footprint with a new greenfield facility in the east-central Gujarat state<strong>, </strong>Vadodara, complementing its existing manufacturing base in Bengaluru. </p><p>“With the new facility, we expect to double our revenue capacity from $60 million to nearly $120 million, bringing us closer to our target of $1 billion in the next two to three years,” Sneh Shah, Wholetime Director of Aimtron Electronics, told EE Times at productronica India 2025.</p><p>The upcoming plant will span approximately 9,300 square meters (100,000 square feet) and include six to seven surface mount technology (SMT) lines with complete system integration facilities, including in-house plastics and sheet metal manufacturing, Shah said.</p>\n<p>The expansion comes at the heels of a major ₹97.55 crore ($11.7 million) original design manufacturing (ODM) agreement with a U.S.-based critical digital infrastructure and continuity solutions company<strong>. </strong>Under the agreement, Aimtron will lead the end-to-end development of a transformer-free 1 kVA, 2 kVA and 3 kVA uninterruptible power supply (UPS) system designed for modern data center and industrial applications.</p>\n\n<h3 class=\"wp-block-heading\"><strong>Design-led manufacturing model</strong></h3><p>With this order, the Chicago-headquartered manufacturer steps into a design-led manufacturing and original brand manufacturing model rather than remaining a pure electronics manufacturing services provider. Shah revealed that these systems, once imported by the customer, are now being developed and manufactured entirely in India, aligning with <em>Atmanirbhar Bharat </em>and “Make in India” objectives.</p>\n<p>“This product will make the customer self-reliant in India. We will retain the intellectual property for a few years and support them through our ESDM expertise and government skill initiatives,” Shah explained.</p><p>The UPS range involves complex power electronics with high voltage and current requirement areas where few Indian companies have full integration capabilities, according to Shah. “We expect to complete the project this year and begin full-scale production next year,” he added.</p><figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"></figure><h3 class=\"wp-block-heading\"><strong>Scaling India’s ESDM ambitions</strong></h3><p>Recent reports from <a href=\"https://www.careratings.com/uploads/newsfiles/1759304578_CAAPL_ESDM%20PR_01.10.2025.pdf\" rel=\"noreferrer noopener\" target=\"_blank\">CareEdge Ratings</a> indicate that the Indian ESDM market is expected to grow at a compound annual growth rate (CAGR) of 20–25% over the next five years (FY25-FY30). This growth trajectory is expected to double the market size from ₹7 to ₹8 lakh crore (approximately $85-95 billion) by 2030. Shah believes policy frameworks like Make in India, the production linked incentive (PLI) scheme and GST 2.0 have been instrumental in enabling this growth.</p><p>“These policies have encouraged domestic manufacturing,” he said. “We are exploring PLI opportunities, especially since we already serve multiple sectors such as telecommunications and automotive. Our growth has been around 35% year-on-year, and revenue increased by 70% last year. With rising domestic consumption, we expect this momentum to continue.”</p><p>Shah attributed the UPS design win to Aimtron’s U.S. presence, adherence to global quality standards and focus on niche technologies like AI, IoT and Industry 4.0. “Most of our open orders are in AI and IoT-driven domains,” Shah said. “That has been our forte since inception.”</p><p>The company received an international order worth about $3.9 million this week, to manufacture 200,000 IoT devices over the next year. Although this project is mainly manufacturing-led, it reflects Aimtron’s expanding global reach and ability to deliver high-quality, finished electronic systems from India.<strong></strong></p><p>Aimtron went public in June 2024 with a price band of ₹153 to ₹161 per share. The company had a stellar market debut, listing with a 50% premium. As of today, the company provides full-spectrum electronic system design and manufacturing services, covering hardware, mechanical and motor design, as well as software development, prototyping and testing.</p><p>Once the product design is finalized, the company handles mass production through system integration, including plastics, sheet metal and PCB assemblies. The surface-mount device process is central to its PCB assembly operations, supported by in-house cable assemblies and other component manufacturing capabilities.</p><h3 class=\"wp-block-heading\"><strong>Building a smart, connected supply chain</strong></h3><p>Shah said that the company’s latest SMT line runs fully automated, with minimal manual intervention, enhancing both efficiency and quality. Aimtron’s intranet-based global software platform connects all five global facilities, providing real-time visibility of shop floor operations across geographies.</p><figure class=\"wp-block-image size-full\"><img alt=\"\" class=\"wp-image-1444904\" data-recalc-dims=\"1\" decoding=\"async\" height=\"480\" src=\"https://www.eetimes.com/wp-content/uploads/image_aa2b5f.jpg?resize=640%2C480\" width=\"640\"/><figcaption class=\"wp-element-caption\">Aimtron’s new automated SMT Line. (Source: Aimtron Electronics)</figcaption></figure><p>For inventory management, Aimtron relies on cloud-based IoT software that offers real-time tracking of part availability, quantity, manufacturer and pricing across facilities. “If a part is needed in Bengaluru, we can instantly check its availability in Vadodara,” Shah said. “It ensures paperless visibility and predictive control.”</p><p>Aimtron’s global sourcing team manages procurement both internationally and domestically. About 90% of customized parts are sourced from India, while electronic components continue to be imported from Japan, Singapore, Malaysia and South Korea. “We are already part of the ‘<a href=\"https://www.eetimes.com/western-technology-giants-pivot-away-from-china/\" rel=\"noreferrer noopener\" target=\"_blank\">China Plus One</a>’ strategy,” Shah added. “And are working to diversify our supplier base.”</p><p>The company maintains less than 5% dead stock, with 90% of the bill of materials pre-scheduled, Shah said. “We keep active inventory only and plan based on annual orders divided into monthly schedules.”</p><p>Aimtron follows a partner-based engagement model, treating suppliers and clients as part of the same ecosystem. About 80% of current open orders are domestic but cater to international clients with facilities in India. “We begin at the design phase and support clients from concept to production,” Shah explained. “Our one-stop solution includes hardware, software, reliability testing and certifications like IP, CE and UL.”</p><p>Most testing is conducted in-house, while specific certifications are handled through preferred external partners.</p><p>Although Aimtron develops products end-to-end, IP ownership typically remains with the client, preventing conflicts of interest and maintaining customer trust. “The process starts with the client defining the scope,” Shah said. “We build around that, proposing upgrades when new technologies emerge, such as Industry 5.0, and integrating only after client approval.”</p><p>Long-term collaboration is built into Aimtron’s model. Clients invest heavily in tooling and non-recurring engineering costs, around $100,000 or more, which fosters ongoing relationships. “Nearly 80% of our first-day clients continue to work with us,” Shah said. “The investment and our partnership model ensure long-term continuity.”</p><p>As Aimtron expands its manufacturing capacity and design-led capabilities, its focus remains on scaling end-to-end system integration while deepening India’s role in the global ESDM supply chain. “We are committed to building technology-driven, design-led manufacturing capabilities that make India a global hub for high-value electronics,” Shah concluded.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><p>Also read:</p><p><a href=\"https://www.eetimes.com/policy-shifts-drive-sixfold-surge-in-indias-electronics-output/\" rel=\"noreferrer noopener\" target=\"_blank\"><em>Policy Shifts Drive Sixfold Surge in India’s Electronics Output</em></a></p><p><a href=\"https://www.eetimes.com/india-releases-guidelines-for-electronics-components-manufacturing-scheme/\" rel=\"noreferrer noopener\" target=\"_blank\"><em>India Releases Guidelines for Electronics Components Manufacturing Scheme</em></a></p><p><a href=\"https://www.eetimes.com/want-a-smart-factory-its-all-about-the-data/\" rel=\"noreferrer noopener\" target=\"_blank\"><em>Want A Smart Factory? It’s All About the Data</em></a></p> </div>",
            "pub_date": "2025-10-30 15:30:52",
            "link": "https://www.eetimes.com/aimtrons-design-led-approach-secures-manufacturing-wins/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Physical AI Needs An Ecosystem",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>The next wave of automation may or may not be humanoid, but the success of robotics – otherwise known as “physical AI” or “embodied AI” – will rely on a technology ecosystem that is still in its early stages, according to industry experts interviewed by EE Times.</p><p>Universal Robotics is the largest manufacturer of collaborative robots, or cobots, a type of industrial robot designed to work in the same space as humans. Anders Billesø Beck, vice president of technology at Universal Robots told EE Times that cobots unlock a range of applications he calls “human-scale automation”: augmenting or replacing humans in the workflow.</p><p>“We’ve been struggling with [the variability of tasks] in traditional engineering for decades – we can model things if they are consistent or expected, that’s a solvable engineering problem, [but where it isn’t,] AI can solve it,” Beck said.</p>\n<p>Logistics, whose warehouse operations are largely automated today, would have been impossible to automate without AI, Beck said. Packages are different shapes and sizes (some retailers Universal Robots works with have as many as half a million SKUs), and pallets in the real world look very different to what comes out of the pallet factory – they are dented, painted and shrink wrapped. A Universal Robots autonomous pallet jack had to be trained on 100,000 real images and 1.5 million synthetic images of pallets to create a robust pallet-recognition model.</p>\n\n<p>“The beauty [of AI] is that it is software-updateable,” Beck said. “Some customers might have their own custom pallet, which might not look like a pallet, it might look more like a big plastic tub with a pallet on the bottom. To add that to our model – we can augment the model, it doesn’t need a lot of work because the foundation layers are still there – just a bit of refinement, and off you go.”</p>\n<p>A team working without AI would need months of bespoke engineering to adapt to these changes, he said.</p><p>Future AI-enabled robots will be able to take on applications like assembly, which makes up around 40% of all industrial processes. Assembly of small parts – screwdriving, plugging in modules or cables, routing wires – has a very high mix of subtly different tasks, which is too varied for today’s cobots.</p><p>“We’re going to start to see robots moving into jobs where the workflows change daily or hourly, where some of these tasks are really complex, where human finesse is needed and where mobility and the task execution integrates that,” Beck said. “That will also spark growth in non-industrial applications.”</p><p>Non-industrial robotics could be home assistants that perform complex tasks like cooking, or hospitality helpers in restaurants and shops.</p><p><strong>Humanoid robots</strong></p><p>Visit the show floor at Nvidia’s GTC and you’ll hardly be able to turn a corner without coming across a humanoid robot. This form factor is complex and requires advanced AI, and while it seems to have captured the industry’s imagination, it’s not yet suitable for industrial automation applications, Beck said.</p><p>Industrial processes are already optimized for performance, making them relatively low-hanging fruit when it comes to automation, Beck said.</p><p>“What’s left is applications where you need the extreme flexibility and capabilities that humans have, because humans are amazing in their versatility – here, humanoids could fit,” Beck said. “But I do think there will be many form factors rolling out the deployment of AI, and there’s no doubt we’ve just started to overcome a lot of the technical hurdles that the humanoid form factor has to overcome.”</p><p>Safety is one of these hurdles. An unstable humanoid cobot could fall on a person in its working environment, which isn’t a trivial problem to solve, Beck said.</p><p><strong>Safety requirements</strong></p><p>Functional safety for robotics has historically been complex, Beck said, and AI is making it even more complicated. Processes like reasoning don’t need to be functionally safe, because this is balanced by safety measures like power and force limitations, but anything that increases unpredictability is at naturally at odds with safety, including AI.</p><p>“Everything around safety likes predictability,” Beck said. “If you can describe what the robot will do, then you can do your risk assessment based on an understanding of what it will do. If you can’t fully describe what it will do, then you need to be more open-minded in how you describe the behaviors of the robot.”</p><p>Innovation in safety will be a big topic for the next 10 years, Beck said, both from the silicon and the ecosystem side.</p><p>“We need to continuously make those workflows easier, because the barrier of entry for innovating on safety is high, it needs specialists,” he said. “We’ve seen that with many robot companies, that has been their stumbling block – they’re just getting to that point where certified safety is a core part of what they do.”</p><p>Arm is watching safety and security standards closely, Paul Williamson, senior vice president and general manager of IoT at Arm told EE Times.</p><p>Williamson said that while advanced safety features are needed for some robotics applications, some systems will handle safety at a different level, perhaps by separating humans’ workspace from the robots or by having a separate kill switch.</p><p>“It’s important to provide those different levels of safety capability to suit the different deployment models, because to drive innovation, you don’t want to burden every form factor with the cost of the most advanced systems,” he said.</p><p><strong>Software ecosystem</strong></p><p>Universal Robotics has a partner ecosystem where third parties can make hardware peripherals (like grippers, sanders, glue dispensers, welders, cameras, etc) and write software plugins for its cobots.</p><p>“No company in the world can solve everything on its own,” Beck said. “Robotics is both the science and the art of integration – you need a lot of systems and technologies playing well together, and that needs good silicon infrastructure, good technologies at every level, and good interfaces for integration.”</p><p>Last year, Universal Robotics introduced AI deployment infrastructure for AI companies who want to deploy their models on Universal Robotics’ hardware, a move that was welcomed by many software-centric startups, Beck said.</p><p>“[Startups] created great software products that they wanted to take out to customers, but they realized they need to come up with computing infrastructure, and support it,” Beck said. “We’ll have an Arm-based Nvidia-accelerated computer to run the AI models, and we offer long-term industrial product life cycle management and support – so startups can focus on being startups.”</p><p>This allows startups to focus on doing what they do best without worrying about infrastructure and the software ecosystem. This ecosystem currently has more than 500 certified cobot-compatible products designed by more than 380 third parties, Beck said.</p><p>Ecosystem is essential if we want to avoid what happened with smartphone software, Williamson said.</p><p>“The reason this excites Arm is we’re all about building these ecosystems and seeing where we can unlock potential through software commonality and ecosystems,” he said. “[Consistent with the smartphone analogy, Arm] enabling different performance levels and price points and capabilities and devices… allowed software developers to then target those multiple performance and price points in the industry.”</p><p>The same underlying architecture can enable both high-end and more power- and cost-efficient devices to exist in a consistent software ecosystem, Williamson said.</p><p>“Our goal is to ensure the software ecosystem allows for that flexibility, for customization and tuning, whether that be Cortex-A Linux-based platforms and virtualization spanning different silicon vendors’ platforms to allow models to be deployed for different computing environments, or whether it’s closer to camera operation or low power sensing, having that common software developer flow and tools in the cloud that are already tuned and optimized for Arm allows that portability,” he said.  </p><p><strong>Chip customers</strong></p><p>Arm’s customers in robotics today include chipmakers like NXP, Infineon, Qualcomm and Nvidia. Does Williamson see a future where, perhaps driven by the desire for custom innovation at the silicon level, big robotics companies make their own chips?</p><p>“The economics for vertical integration in the robotics sector today isn’t there,” he said.</p><p>Some chipmakers with automotive offerings are trying to redeploy their investment into the robotics space, as there is some crossover in requirements, but in general, there is such a breadth of potential form factors that a range of platforms will be required, Williamson said. The trend is towards higher-performance silicon in robots as use cases evolve and volumes grow, he added. </p><p>Arm provides compute subsystem (CSS) designs which integrate multiple Arm cores with interconnect in a ready-made subsystems for particular verticals, including the data center, mobile devices and automotive today. Is a CSS for robotics coming?</p><p>“We’re always looking at market demands,” Williamson said. “At the moment [in robotics] there is a breadth of performance and form factor, but if that tends to standardize, and if we see clear demands that we think need to be met at the system architecture level, we’d be keen to provide those for the industry.”</p><p><strong>Humanoid potential</strong></p><p>Both experts agreed that the humanoid form factor could develop into one of the most exciting forms of robotics, albeit a little further into the future.</p><p>“I’m super excited about [humanoid robots] because they’re driving innovation, but I think we’re going to see a lot more form factors become more relevant in the short term, and solve real world commercial problems in the short term, which are going to be as important as the long-term opportunity that a humanoid form factor could provide,” Williamson said.</p><p>“[Humanoids] generate a whole new wave of innovation within robotics, which is very welcome after maybe a decade of a bit of an innovation drought within robotics, so I’m really excited about that,” Beck said. “There’s no doubt there’s going to be a market for humanoids. Is it going to displace everything? I’m not so sure, but it’s exciting to follow.”</p> </div>",
            "pub_date": "2025-10-28 06:30:42",
            "link": "https://www.eetimes.com/physical-ai-needs-an-ecosystem/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "L&T Semiconductor, Hon Young Partner for 650V to 3300V SiC Wafer Development",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p><em>The alliance targets growing SiC demand in electric vehicles and energy systems</em> </p><p><a href=\"https://www.ltsct.com/\" rel=\"noreferrer noopener\" target=\"_blank\">L&amp;T Semiconductor Technologies Ltd </a>(LTSCT) has formed a long-term partnership with Hon Young Semiconductor (HYS) to jointly develop high-voltage silicon carbide (SiC) wafers, ranging from 650V to 3300V, for automotive, industrial, and energy applications. The wafers will be manufactured at HYS’s fabrication facilities in Taiwan. </p><p>Dr. Sandeep Kumar, CEO of LTSCT, revealed that the company considered multiple wafer manufacturers but ultimately selected HYS. “HYS’s expertise is concentrated on SiC wafer fabrication. They were selected as our preferred partner for opportunities requiring a certain readiness level, pricing structure, and supply chain resiliency,” he told EE Times in a set of written responses. </p>\n<p>The partnership focuses on developing SiC wafers in the 650V to 3300V range that serve as the base for high-voltage power devices such as SiC MOSFETs and Schottky barrier diodes (SBDs). “These components are key enablers of high-efficiency power conversion and are increasingly replacing conventional silicon-based devices like IGBTs and MOSFETs in next-generation systems due to lower switching losses, better thermal performance, and higher system efficiency,” Kumar explained. </p>\n\n<p>On the targeted focus on the 650V to 3300V range, Kumar added, “This range is particularly relevant to automotive, industrial, and energy sectors, where SiC devices improve energy efficiency.” </p>\n<p>LTSCT said the strongest demand for high-voltage power devices is coming from electric vehicles (EVs), renewable energy systems, and data centers. In EVs, SiC devices are used in onboard chargers, traction inverters, and DC fast-charging systems. In renewable energy, they enhance efficiency in solar inverters and wind power converters. Industrial applications include motor drives, power supplies, and high-voltage converters for automation and heavy machinery. </p><p>Beyond <a href=\"https://www.eetimes.com/tag/sic-mosfet/\" rel=\"noreferrer noopener\" target=\"_blank\">SiC MOSFETs</a> and Schottky barrier diodes, LTSCT works with other foundry partners for additional power and analog semiconductor devices. </p><p>The partnership is expected to improve supply reliability and competitive pricing for LTSCT’s global customers. “Customers will benefit from stable supply and pricing that enables access to reliable, energy-efficient products,” Kumar said. </p><p>LTSCT is also investing strategically in SiC research and product development to drive innovation in high-performance power solutions. Early success metrics include moving smoothly from prototyping to customer validation, followed by scaling production for mass manufacturing. The partnership also opens opportunities for intellectual property creation and joint development of next-generation power solutions, Kumar revealed. </p><p>“As a fabless company, our strength in design and innovation allows us to generate IP through strategic partnerships. Working on high-voltage semiconductor chip design with HYS’s SiC platforms creates opportunities for developing patentable technologies,” Kumar explained. </p><p>The initiative aligns with LTSCT’s roadmap for automotive and industrial power technologies. Using HYS’s fabrication capabilities, the company aims to accelerate product development, maintain quality and reliability standards, and scale production to meet global demand. </p><figure class=\"wp-block-image size-full\"><img alt=\"\" class=\"wp-image-1444703\" data-recalc-dims=\"1\" decoding=\"async\" height=\"1\" src=\"https://www.eetimes.com/wp-content/uploads/image_5cda78.png?resize=1%2C1\" width=\"1\"/></figure> </div>",
            "pub_date": "2025-10-24 15:10:30",
            "link": "https://www.eetimes.com/lt-semiconductor-hon-young-partner-for-650v-to-3300v-sic-wafer-development/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "NextSilicon Details Runtime-Reconfigurable Architecture",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>HPC silicon startup NextSilicon has unveiled some details of its runtime-reconfigurable hardware architecture and results for some popular HPC benchmarks which the company said shows its chip can outperform CPUs and GPUs on the same code. The company also showed off a test chip for a 10-wide RISC-V CPU it is developing as a host CPU for its next generation of accelerators.</p><p>Scientific computing and HPC customers are struggling with rigid CPU and GPU architectures, said NextSilicon CEO Elad Raz.</p><p>“This has become a multi-hundred-billion-dollar problem,” Raz said. “Massive code rewrites, nightmare porting scenarios, skyrocketing energy costs, and smaller performance gain – these have all become the norm.”</p>\n<p>NextSilicon wants to replace CPUs and GPUs in supercomputers with its dataflow chip, which is reconfigurable during runtime to mitigate code bottlenecks.</p>\n\n<p>“We saw that for compute-intensive applications, a small portion of the code runs the majority of the time,” Raz said. “We decided to use that to our advantage. […] We developed a smart software algorithm that continuously monitors your application. It identifies precisely which code path runs the most, and it reconfigures the chip to accelerate exactly those code paths. And we do this all during runtime in nanoseconds.”</p>\n<p>Raz, a software engineer by training, told EE Times in an earlier interview that the idea to start a company began 7 years ago with his patent “Runtime Optimization for Reconfigurable Hardware.”</p><p>“The origin of the idea came from looking at high-level synthesis and why it doesn’t work well,” he said. “In high-level synthesis, you take C code and you get an RTL netlist which is synthesized on the FPGA, but FPGAs are lookup-table-oriented, single-bit, so while you can emulate and run hardware code on an FPGA, they aren’t suitable for software.”</p><p>Maverick’s hardware is a reconfigurable fabric of ALUs which can be reconfigured quickly while code is running (FPGAs need a reboot cycle). An algorithm studies the code for bottlenecks, and configures the hardware appropriately while the program is running. The result is a dataflow processor that can reconfigure itself during operation as the demands of the workload change.</p><p>“Our intelligent compute architecture is based on the Pareto principle, the 80-20 rule, but in massive parallel code, it’s even more extreme,” Raz said. “[In parallel code,] 1% of the code runs 99% of the time. Our sophisticated software algorithm understands what’s happening in the application, and identifies those critical hot paths in real time. It reconfigures the chip to accelerate exactly those parts that matter… This asymmetric approach means we are not wasting silicon and power on code that rarely runs.”</p><p><strong>Dataflow processing</strong></p><p>Dataflow architectures are often used in AI accelerators to try to avoid the memory bottleneck and other complexities of modern von Neumann CPUs and GPUs, like branching, out of order operation and traffic management. This allows more of the silicon area to be dedicated to compute for computation-heavy workloads.</p><p>For dataflow architectures like NextSilicon’s, most of the area is taken up by compute blocks (ALUs), in contrast to CPUs where the ALU area is around 2%. This means more computation per clock cycle (and per Watt), provided you can ensure the data is in the right place at the right time.</p><p>In NextSilicon’s architecture, the compute blocks are connected to a memory bus to receive data, which is temporarily stored in what the company calls the reservation station (RS). A dispatch unit determines when it’s time to trigger the compute block. (The RS and dispatcher are analogous to registers in a CPU.) Memory entry points (MEPs) handle memory access operations, generating memory access requests on the memory bus, then directs its completion response to the RS. An MMU and TLB cache translate virtual memory addresses where required.</p><p>NextSilicon’s compiler maps operations to ALUs, generating configurations for hardware elements and dataflow. These configurations are replicated as needed across the compute block for parallel code. The ALUs are reconfigured for, say, a particular activation function in AI, and reconfigured again quickly for the next layer of vector or matrix processing. NextSilicon refers to its configured compute blocks as “software-defined”.</p><p><strong>Any code</strong></p><p>Critically, said Ilan Tayari, co-founder and VP Architecture at NextSilicon, the stack can run <em>any</em> code out of the box, whether that’s code written for CPUs or GPUs, or AI models.</p><p>“The best hardware in the world is useless if you can’t program it,” Tayari said. “Historically, this has killed the adoption of dataflow architectures because it required users to write applications using domain-specific languages. But we’ve solved it.”</p><p>Whether it’s C++, Fortran, Python, CUDA, ROCm, OneAPI, or even AI frameworks, NextSilicon’s compiler splits the code into the part that will run on the host CPU, and lowers the other part to an intermediate representation for the reconfigurable hardware.</p><p>“It’s not limited to what exists today,” Tayari said. “For AI researchers, this method opens up new exciting options. You get acceleration no matter what your model uses… exotic activation functions, complex numbers, or novel mathematical operations: everything is accelerated out of the box.”</p><p>At runtime, live on-chip telemetry continuously optimizes the application. For example, if compute sub-blocks communicate frequently, the grid is reconfigured to move them closer together. If there is a bottleneck, they are duplicated for parallelism. This happens automatically, without input from the developer.  </p><p><strong>Production silicon</strong></p><p>NextSilicon’s chip, Maverick2 (actually the first generation of its production silicon) is built on TSMC 5nm. There are two versions. A single compute die version with up to 96 GB of HBM3e is air-cooled with a 400-W TDP. A dual-die version with up to 192 GB HBM3e needs liquid cooling for its 750-W TDP.</p><p>The company has been testing both the single- and dual-die versions of Maverick2 with HPC benchmarks.</p><p>Stream, which measures how fast the system can move data between processor and memory, ran at 5.2 TB/s bandwidth.</p><p>GUPS (giga updates per second) tests the memory fabric, including latency, bandwidth, IO contentions, and cache behaviour. Maverick2 achieved 32.6 GUPS at 460 W.</p><p>HPCG is designed to resemble common HPC workloads, to test the balance of memory access, communication and computation. Maverick2 achieved 600 GFLOPS at 600 W, which the company said outperforms CPUs and GPUs since they typically have inefficient memory handling and can’t take advantage of sparsity; these things are dynamically optimized for in real time in Maverick2.</p><p>PageRank, the algorithm originally developed by Google that powers big parts of internet infrastructure today, requires good performance for graph algorithms, which use irregular memory access patterns. NextSilicon said Maverick2 achieved 40 gigapages/s, 10x better than GPUs at half the power for small graphs, and can run graphs bigger than 25 GB that comparable GPUs failed to run entirely.  </p><p>“[These results were] achieved using existing, unmodified application code,” said Eyal Nagar, co-founder and vice president of R&amp;D at NextSilicon. “Our competitors require specialized teams to modify code, BIOS, firmware, operating system, parameters, just to hit their published benchmarks. NextSilicon delivers superior results using the software you’re already running. This is the promise of a true drop-in replacement, kept.”</p><p><strong>RISC-V CPU</strong></p><p>Raz also unveiled test silicon for a big RISC-V CPU the company is working on as a host CPU for its next-gen accelerator Maverick3. The 10-wide CPU, Arbel, is an evolution of smaller RISC-V cores on Maverick2, which handle serial code.</p><p>“Those cores performed so well that we started thinking about what could be possible if we tested it with a dedicated chip,” Raz said. “The result is Arbel, a new enterprise-grade RISC-V performance core.”</p><p>This core will be comparable to Intel’s Lion Cove or AMD Zen 5 in terms of performance, Raz said.</p><p>“We are seeing tremendous customer interest in Arbel,” Raz said. “It opened our eyes to the same opportunity that AMD and Nvidia have recognized – the power of vertical integration between CPU and accelerator technologies. When you control both the general-purpose computing and the specialized acceleration, you can optimize the entire stack in ways that simply aren’t possible [otherwise].”</p><p>NextSilicon’s flagship runtime-reconfigurable accelerator, Maverick2, is deployed at “dozens” of customers today, according to the company. This includes a three-year collaboration with Sandia National Labs where it is deployed as part of the Vanguard 2 supercomputer. NextSilicon’s next generation accelerator, Maverick3, will offer reduced precision support for AI workloads and is expected to be available in 2027.</p><p></p> </div>",
            "pub_date": "2025-10-23 01:35:23",
            "link": "https://www.eetimes.com/nextsilicon-details-runtime-reconfigurable-architecture/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Silicon Valley Events Highlight Rapid Innovation from Power to Edge AI",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>Last week I had the chance of attending three key events in Silicon Valley – OCP Summit, 2025, Synaptics’ annual Tech Day, and Infineon Technologies’ annual OktoberTech event. What struck me and other people I spoke to is the rapid pace of innovation happening to meet the rapid growth of AI – from the data centers to the edge.</p><p>However, that’s quite a generic statement that I have to explain further what I mean. It always bothers many of us in the world of media that we hear AI being used so freely as a term that it’s often difficult to understand what exactly is being referred to. In fact, many startups often say they have to use ‘AI’ in their pitch decks to ensure big investment rounds, because investors like to see they are getting on the AI train.</p><p>So the areas I am referring to when I talk about the rapid pace of innovation to meet the rapid growth of AI, and through the lenses of the events last week, it’ about dealing with the massive amounts of power needed for AI data centers, it’s about breaking down workloads, it’s about the cooling methods, it’s about innovations in areas like advanced packaging to enable efficient use of power, it’s about moving more to AI to the edge and not needing to make a connection via the cloud to make the thing intelligent.</p>\n<p>To capture some of the mood of these three events, I caught up with the Majeed Ahmad, Editor-in-Chief of EDN magazine, and Jim McGregor, Principal Analyst at Tirias Research, to get their takes on the various events last week which they also attended.</p>\n\n<p>You can watch the video conversation below, where you’ll also see one aspect I haven’t mentioned – a humanoid robot called Ameca at Infineon’s OktoberTech event. In the video, around the 15-minute mark, you’ll see the robot summarizing the keynotes from the morning’s event, which ‘she’ was listening to intently all through the morning at the side of the stage. She listened to Infineon’s Adam White, Nvidia’s Deepu Talla, Addverb Technologies’ Tappan Pattnayak, and Quantinuum’s Rajeed Hizra and summarizes this in the video. You can see the delays as ‘she’ processes the answers to the host’s question by communicating with the cloud to deliver the answers.</p>\n<p>Watch the video here:</p><figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"></figure><p>In the video, Jim McGregor talks about the significance of the huge attendance at OCP Summit, his key takeaways, and the fact that Nvidia was everywhere, as well as the Arm Total Design news and liquid cooling and 800V DC announcements.</p><p>Majeed Kamran highlights how everyone at OCP was worried about power, and how AI data centers will be able to get the levels of power needed as AI demand grows exponentially in the data center.</p><p>Another key theme we all discuss is the notion of collaboration within the ecosystems and the need for open and/or modular standards. Jim McGregor notes that everyone finally realizes they all need to work together as an ecosystem. “Everyone won’t be the best at everything.” And Majeed Ahmad added that there’s a realization that the challenges ahead are huge – for example co-packaged optics, which has a long way to go and needs collaboration as no single company can do everything.</p><p>In relation to Synaptics Tech Day, we discuss how edge AI has been talked about for many years, and how fragmentation (in terms of different architectures etc.) has meant  there has been no killer application so far. We also discuss the impact of <a href=\"https://www.eetimes.com/google-open-sources-npu-ip-synaptics-implements-it/\" rel=\"noreferrer noopener\" target=\"_blank\">Synaptics new processor</a> for edge AI launched in collaboration with Google. Majeed Ahmad notes how Synaptics is trying to revive the fortunes of IoT by linking it now with edge ai, so turning the page on IoT. Jim McGregor notes how IoT has now become, with the possibility of ‘open’ edge AI, the ‘intelligence of things’.</p><p>Finally the video sees a discussion on Infineon’s OktoberTech event, with humanoid robotics, edge AI, quantum and power being the dominant themes. Majeed Ahmad notes how AI is the predominant theme everywhere, and how Infineon has been able to successfully link the needs of the various strands of AI to its power products. Jim McGregor talks about how everything is about power, not just for data centers, but everything, from the grid all the way to chip level. Majeed Ahmad also highlights the talk from Quantinuum, in which the CEO said quantum is already happening now, and that engineers shouldn’t be complacent.</p><h3 class=\"wp-block-heading\"><strong>Silicon Valley thriving better than ever</strong></h3><p>Having spent a week in Silicon Valley, we also reflect in the video conversation how innovation is thriving in Silicon Valley, and how industry is coming together to address the challenges. In fact Majeed Ahmad goes as far as saying that over the many years that he’s been coming to Silicon Valley, he’d never felt the energy he felt on this visit last week.</p><p>I would second that – and this is echoed by an <a href=\"https://www.youtube.com/watch?v=brjL6iyoEhI\" rel=\"noreferrer noopener\" target=\"_blank\">A16z podcast with Reid Hoffman</a>, founder of LinkedIn, published this week, in which he says, “Silicon Valley is one of the most amazing places in the world. There’s a network of intense co-opetition and learning, invention, building new things, which is just great.” He noted in the podcast that Silicon Valley also has its’ blind spots. He said that the belief in the last few years that everything should be done in software, everything should be done in bits, is a warning, since the AI revolution may come not from what we already know, but from within the Silicon Valley blind spots.</p><p>Well we saw it last week, we will see plenty of activity in how we deal with power, how we enable the ‘golden age of robotics’ as Nvidia put it, and how things will become intelligent and quantum will provide new directions.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><p><em>See also</em>:</p><p><a href=\"https://www.youtube.com/watch?v=efCXLR2Jb0s\" rel=\"noreferrer noopener\" target=\"_blank\"><em>Rahul Patel, Synaptics on Strategy for Edge AI Growth</em></a></p><p><a href=\"https://www.youtube.com/watch?v=FdaO_R63LuU\" rel=\"noreferrer noopener\" target=\"_blank\"><em>OCP Summit 2025: Ganesh Srinivasan, TE Connectivity on Scaling Up and Scaling Out AI Data Centers</em></a></p><p><a href=\"https://www.youtube.com/watch?v=Uj3FA0A6q34\" rel=\"noreferrer noopener\" target=\"_blank\"><em>OCP Summit 2025: Chris Suchoski, Texas Instruments on the Path to 800V DC in AI Data Centers</em></a></p><p><a href=\"https://www.youtube.com/watch?v=exYRXTBKRYw\" rel=\"noreferrer noopener\" target=\"_blank\"><em>OCP Summit 2025: Eddie Ramirez, Arm on Leveraging Chiplets For AI Infra</em></a></p><p></p> </div>",
            "pub_date": "2025-10-22 07:10:24",
            "link": "https://www.eetimes.com/silicon-valley-events-highlight-rapid-innovation-from-power-to-edge-ai/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "How EdgeQ Is Building India’s First Unified 5G + AI SoC",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>When Vinay Ravuri founded EdgeQ in 2018, he dreamt of challenging the status quo in wireless infrastructure. A veteran from Qualcomm, Ravuri saw that the global wireless chip market was dominated by a handful of players, leaving little room for innovation. “I looked around and saw that when it came to wireless, there was one dominant company and then a few that I could count on my fingers,” he said. That scarcity, he realized, stemmed from the complicated web of information theory, algorithms, and chip design in wireless signal processing that had been traditionally confined within proprietary silos. </p><p>Ravuri recognized that RISC-V, the open-standard instruction set architecture, could catalyze change much like Linux had for software. “RISC-V opened up hardware development. We were among the first to recognize that this could reduce cost and time to market. Today, almost all machine learning companies use RISC-V,” he said. That insight became the foundation for EdgeQ, a startup now building a programmable chip platform that unites 5G and artificial intelligence (AI) workloads on a single System-on-Chip (SoC).</p><p>Headquartered in Silicon Valley, EdgeQ’s journey began with a simple hiring philosophy: recruit the best talent wherever they are. “Honestly, there was no grand plan to start the company this way or to hire people in specific regions. The idea was simple—hire the best people wherever they are,” Ravuri explained. That approach led to a strong engineering presence in Bengaluru, where many team members had previously worked at Qualcomm or Intel. COVID-19 later reinforced the model, enabling remote recruitment across India, the US, and Europe. Today, EdgeQ employs engineers in Bangalore, Pune, Noida, and Hyderabad, among other global locations. </p>\n<p>Hariprasad Gangadharan, VP and Head of Silicon Engineering, and Country Head India, echoed Ravuri’s focus on talent and technological depth. “Many of us made a conscious decision to stay in India or come back to India and drive deep technology from here, at a time when entrepreneurship wasn’t common,” he said. Having worked at multiple wireless chip startups, including successful exits to Cypress and Broadcom, Gangadharan brought both experience and ambition to EdgeQ. </p>\n\n<h3 class=\"wp-block-heading\"><strong>The technology innovation</strong> </h3><p>EdgeQ’s core innovation lies in its SoC, which integrates the entire 5G stack along with AI acceleration on a single chip. “We’re one of the very few startups globally, and the only one from India, building a full 5G SoC from scratch,” Gangadharan said. The chip combines Layer 1, 2, and 3 software stacks with digital signal processing. The complete solution is accompanied by a reference platform with system-level validation and board-level design support for customers. </p>\n<p>Traditionally, telecom infrastructure relies on separate chips for 4G, 5G, and AI workloads, each with its own memory, power management, and peripherals. EdgeQ’s architecture unifies these functions using a pool of configurable multipliers that can perform either 5G baseband computations or AI matrix multiplications. “The same multipliers that perform channel estimation and equalization in 5G can also perform neural-network computations,” Gangadharan explained. The approach uses a fundamental mathematical commonality between AI and communication signal processing. </p><p>Ravuri described the technical flexibility in simpler terms. “Our chip architecture allows it to handle both 5G and AI computations. The same matrix mathematics used in 5G, based on complex numbers, can also be used for AI by modifying the elements of the matrix. By zeroing out the imaginary part, we turn it into a dot-product operation, which is what most AI computations rely on,” he elucidated. This digital, fully software-configurable architecture allows the chip to adapt to indoor or outdoor small cells, distributed units, or even satellites. </p><p>The SoC includes multiple hardware components designed for flexibility and efficiency. “We have Tensor Execution Units (TXU) for all mathematical operations, Forward Error Correction blocks for encoding/decoding per 3GPP standards, security and crypto accelerators, and custom instruction sets for packet processing and real-time scheduling,” Gangadharan said. The chip also integrates ARM CPUs and RISC-V cores for software programmability. </p><p>This entire integration delivers significant cost and power advantages as per Gangadharan. “Traditional solutions use at least three chips, one each for 4G, 5G, and the MAC layer. Our single-chip approach eliminates this redundancy, drastically reducing both power consumption and the bill of materials,” he said. The team has applied mobile SoC design techniques to base-station chips, achieving roughly one-third the power of comparable solutions, he revealed. </p><h3 class=\"wp-block-heading\"><strong>From Prototype to production</strong> </h3><p>EdgeQ has built its SoC with manufacturability and scalability in mind. Verification was among the most complex challenges, but Gangadharan proudly informs that the company delivered silicon with zero metal-mask fixes post-tape-out. “We achieved this through extensive modeling, emulation, and software-hardware co-verification. We created system-level models in SystemC, stress-tested them with real traffic patterns, and used FPGA-based emulation to validate throughput and latency,” he explained. </p><p>Once the chip is ready, customers start with evaluation kits, a board the size of a laptop containing the chip and base software. “They test it in their labs, sometimes connecting hundreds of phones to measure power and performance. This evaluation phase takes three to six months,” Ravuri said. System development by the customer then takes about 12 months, with support and firmware updates provided by EdgeQ for the chip’s typical five- to seven-year lifecycle. </p><p>Customers can also integrate their own Layer 2/3 software on the ARM subsystem running Linux, using standard APIs and following Small Cell Forum FAPI (Functional Application Platform Interface) specifications. This allows telecom operators and original equipment manufacturers to customize solutions while relying on EdgeQ for Layer 1 processing. </p><h3 class=\"wp-block-heading\"><strong>Business model and market focus</strong> </h3><p>EdgeQ primarily targets high-volume segments within wireless infrastructure, including indoor and outdoor small cells, open RAN deployments, and satellites. “Indoor private 5G networks are expected to grow faster than cell towers. There are about one million towers worldwide, but far more potential indoor locations, similar to Wi-Fi access points,” Ravuri said. </p><p>The company monetizes through chip sales bundled with base software, with additional licenses for features such as AI functionality or simultaneous 4G + 5G operation. “AI is an add-on feature. Customers who want AI capability pay extra for that functionality,” Ravuri explained. Maintenance fees cover updates and bug fixes as long as the chip remains in service. </p><p>EdgeQ’s customers include many high-end global original equipment makers and device makers, which then supply wireless infrastructure to telecom operators globally. The company also partners with cloud and software firms such as Radisys (now part of Jio) and supports deployments on AWS, Google Cloud, and other platforms. </p><p>The startup is backed by global investors such as Threshold Ventures, that has previously funded Tesla and SpaceX. EdgeQ has raised a total of $126M over 3 funding rounds — one seed and two early-stage rounds. EdgeQ’s largest funding round so far was a Series B round for $75M in Apr 2023. Its advisory board includes former Qualcomm executives Paul Jacobs and Matt Grob. Ravuri said that the startup leverages a full COT (Customer-Owned Tooling) model while focusing on chip design and architecture. The company collaborates with Synopsys, Cadence, TSMC for foundry services, OSAT partners for packaging and test, and PCB partners.  </p><p>The startup has already received recognition for its technology and has been part of EE Times’ list of 100 Silicon startups to watch out for three years in a row. </p><p><strong>Challenges and Growth Outlook</strong> </p><p>Despite its technological achievements, EdgeQ faces challenges typical of a disruptor in a market dominated by large incumbents. “The <a href=\"https://www.eetimes.com/5g-adoption-grows-but-lte-remains-strong/\" rel=\"noreferrer noopener\" target=\"_blank\">5G market</a> itself did not grow as fast as expected. It was meant to transform automation but has mainly been about phones so far,” Ravuri said. Yet, private 5G networks and industrial automation are gaining traction, and geopolitics is driving local manufacturing initiatives, particularly in India, creating new opportunities. </p><p>Looking ahead, EdgeQ plans to extend its unified 5G + AI platform toward 6G, advanced AI integration, and scalable multi-chip configurations. “Technically, our current chip could run 6G software in the future, but performance improvements will make new hardware more appealing, just like people upgrade their phones even when the old ones still work,” Ravuri said. Gangadharan added that India is now the company’s hub for design, verification, and software, with plans to scale production and AI capabilities on the same chip. </p><p>EdgeQ represents a rare convergence of deep engineering expertise, visionary leadership, and India-based research and development. By combining 5G infrastructure and AI acceleration in a single, programmable chip, the company is not only challenging global incumbents but also establishing India as a hub for advanced wireless and AI semiconductor design. As Ravuri put it: “Every disruptive technology takes longer than expected, about 10 years, but once it takes off, it grows quickly. We are at that turning point now.”</p><p><em>Further Reading: </em></p><ul class=\"wp-block-list\"><li><a href=\"https://www.eetimes.com/nokia-6g-horizon-ai-integration-and-beyond/\" rel=\"noreferrer noopener\" target=\"_blank\"><em>Nokia 6G Horizon, AI, Integration, and Beyond</em></a> </li>\n<li><a href=\"https://www.eetimes.com/europe-6g-future/\" rel=\"noreferrer noopener\" target=\"_blank\"><em>Europe 6G Future Races Against Time</em></a><em></em> </li>\n<li><a href=\"https://www.eetimes.com/podcasts/revolutionizing-wireless-ais-role-in-shaping-5g-and-beyond/\" rel=\"noreferrer noopener\" target=\"_blank\"><em>Revolutionizing Wireless: AI’s Role in Shaping 5G and Beyond</em></a><em></em> </li></ul><p></p> </div>",
            "pub_date": "2025-10-22 05:00:34",
            "link": "https://www.eetimes.com/how-edgeq-is-building-indias-first-unified-5g-ai-soc/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Broadcom Expands AI Ethernet Offerings",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>Broadcom Inc. continues to advance its Co-Packaged Optics (CPO) offerings to meet the demands of artificial intelligence (AI) scale out but also sees Ethernet technologies as complementary and just as critical. </p><p>In the first half of October, the company shipped its Tomahawk 6 – Davisson (TH6-Davisson) CPO Ethernet switch as well as its Thor Ultra, what Broadcom claims is the industry’s first 800G AI Ethernet Network Interface Card (NIC). Both are aimed speeding up and scaling AI networking. </p><figure class=\"wp-block-image size-full\"><img alt=\"\" class=\"wp-image-1444391\" data-recalc-dims=\"1\" decoding=\"async\" fetchpriority=\"high\" height=\"437\" src=\"https://www.eetimes.com/wp-content/uploads/Broadcom-AInetworking-image1.png?resize=640%2C437\" width=\"640\"/><figcaption class=\"wp-element-caption\"><em>TH6-Davisson, Broadcom’s third-generation CPO Ethernet switch, delivers 102.4 Terabits per second of optically enabled switching capacity. (Source: Broadcom)</em></figcaption></figure><p>In a briefing with EE Times, Manish Mehta, VP of marketing and operations for Broadcom’s optical systems division, said the TH6-Davisson, Broadcom’s third-generation CPO Ethernet switch, doubles the bandwidth of any CPO switch available today, delivering 102.4 Terabits per second of optically enabled switching capacity. </p>\n<p>But performance is not the only valuable metric AI data centers – the TH6-Davisson also makes advances in power efficiency and traffic stability, which Mehta said is necessary to support AI cluster scale-up and scale-out. “The optical interconnect space is exploding for AI networks,” he said. “It’s probably had an order of magnitude growth in volumes compared to when devices were just needed in the front end of the cloud network.” </p>\n\n<p>The primary reason for the massive uptick in AI networking is that back-end GPUs use about 10 times the optical bandwidth scale-out as the front-end CPUs, Mehta said, which saw their own uptick in adoption begin in 2010 due to increasing use of cloud networking. “The first layer tends to be a copper link from the CPU server to the top of rack switch and then you go to optical connectivity to the leaf or spine layers,” he said. </p>\n<p>AI networking involves connecting a lot of GPUs to a switch or network of switches and scale out networking demands reach that can only be achieved with optical, Mehta said. </p><p>When Broadcom introduced its CPO portfolio in 2021, he said the company was focused on reducing power consumption and only beginning to densify silicon photonics such that high-density optical engines could be placed on a common substrate with an ASIC. “We were focused on front-end compute.” Mehta said. </p><p>Earlier this year, Broadcom released <a href=\"https://www.eetimes.com/ai-clusters-spur-optical-connectivity/\" rel=\"noreferrer noopener\" target=\"_blank\">its third-generation 200G/lane CPO product line</a>. The company’s optical systems division has been built in part through acquisition, including the fiber optic product division of Avago, which was building optical transceivers based on multimode lasers. Broadcom has been investing in its CPO platform for the past fiver years and leveraged several decades of laser technology development. </p><p>The migration to AI networking and high volume of volume requirements of optics are driving a high velocity of advancement of CPO technology, he said. “We’re see that the power consumption continues to go up if you use traditional optics. CPO can help.” </p><p>Mehta cited data presented by Facebook parent company Meta last week that it had achieved more than one million cumulative 400 Gb/s equivalent port device hours without any data drops using Broadcom’s second generation Tomahawk 5 Bailley, released in March 2024. He said this indicates its readiness for large-scale deployment in data centers, while also noting a 65% reduction in optics power consumption. “There’s a significant improvement in link performance. If you can reduce that link flap frequency, end users can improve GPU utilization for monetization.” </p><p>TH6-Davisson migrates lane speed from 100GB per lane to 200GB per lane, Mehta said, while switch bandwidth grows from 51.2T to 102.4T. “We’re already in development on Gen 4, which will increase the line rate to 400GB per lane.”  </p><p>He said Broadcom is using the same platform for the backend manufacturing as previous generations – Tomahawk 6 has 512 logical ports, allowing to talk to 512 endpoints to provide the connectivity demanded by customers, and improve link performance and GPU cluster efficiency.   </p><p>Mehta said Broadcom’s focus on advancing CPO is delivering Ethernet-based CPO solutions and leveraging Ethernet for AI because it’s open, scalable, and power efficient. </p><p>Separate but related was Broadcom’s announcement of Thor Ultra, now sampling, the company’s 800G AI Ethernet NIC, which can interconnect hundreds of thousands of XPUs. The new NIC adopts the Ultra Ethernet Consortium (UEC) specification to provide the ability to scale AI workloads using high performance advanced RDMA capabilities. </p><p>In a briefing with EE Times, Hasan Siraj, Broadcom’s head of software products and ecosystem, said traditional RDMA lacks multipathing, out-of-order packet delivery, selective retransmit and scalable congestion control. By using the UEC specification, Thor Ultra supports packet-level multipathing for efficient load balancing and out-of-order packet delivery directly to XPU memory for maximizing fabric utilization. </p><figure class=\"wp-block-image size-full\"><img alt=\"\" class=\"wp-image-1444393\" data-recalc-dims=\"1\" decoding=\"async\" height=\"210\" src=\"https://www.eetimes.com/wp-content/uploads/Broadcom-AInetworking-image2.png?resize=640%2C210\" width=\"640\"/><figcaption class=\"wp-element-caption\"><em>Facebook parent company Meta has achieved more than one million cumulative 400 Gb/s equivalent port device hours without any data drops using Broadcom’s second generation Tomahawk 5 Bailley (Source: Broadcom) </em></figcaption></figure><p>Siraj said the thought process when the UEC was formed was that AI clusters become bigger, RDMA needed to be modernized. “It cannot do out-of-order packet delivery. If packets arrive out of order at the destination, they are dropped.” </p><p>The lack of a selective retransmit capabilities and challenges tuning the protocol for congestion control further drove the need to enhance RDMA to allow to support AI scale out effectively, Siraj said.  </p><p>Using UEC also enables to Thor Ultra to support selective retransmission for efficient data transfer and provide programmable receiver-based and sender-based congestion control algorithms. And by delivering these advanced RDMA capabilities in an open ecosystem, customers can connect with any XPUs, optics, or switches they want, Siraj said, and reduce their dependency on proprietary solutions. “We wanted to make sure the existing switches that are out there that are the equivalent of Tomahawk 5 can be supported with this NIC and the future generations like Tomahawk 6. </p><p>He said there are 2x400GB variations of Ethernet NICs available, but this is the first that can support a single 800GB flow. </p><p>Siraj said Thor Ultra is a critical piece of the puzzle as Broadcom builds out its AI infrastructure portfolio and address the limits of traditional RDMA, which is the baseline protocol used in AI and high performance computing environments. “It was not built to get the best performance at scale.” </p><p></p> </div>",
            "pub_date": "2025-10-20 23:05:31",
            "link": "https://www.eetimes.com/broadcom-expands-ai-ethernet-offerings/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Athos Spins Out Of Mercedes With Chiplet Concept for AVs",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>After five years of technology incubation, Athos Silicon has officially spun out of Mercedes-Benz. The startup was founded in March 2025 based on a research project underway at Mercedes since 2020. Mercedes has made a strategic investment in Athos, including “substantial” IP, according to the company, and the OEM and its spinout will continue to collaborate on Athos’ mSoC reference design, Polaris.</p><p>As far back as 2020, the Athos team had been looking at how to address functional safety for autonomy since silicon on the market wasn’t sufficient, Athos CEO Charnjiv Bangar told EE Times.</p><p>“That led us on this journey of fundamentally looking at what is needed for highly reliable and safe compute,” he said. “We started with the problem of autonomy, and with problem statements from [Mercedes’] functional safety teams and application teams in Germany, and built the architecture from the ground up.”</p>\n<p>That architecture – the mSoC – was developed for functional safety first and foremost. Realizing its potential applications were broader than automotive, Mercedes decided to spin out Bangar’s team as Athos at the end of 2024. The complex IP took a while to tease out, Bangar said, but today Athos has the agreements in place to sell its technology to anyone – whether that’s Mercedes’ competitors in the automotive arena, or to companies building planes, drones, robots or any other autonomy application.</p>\n\n<p>“[Mercedes has] a strategic interest in seeing Athos succeed,” Bangar said. “Not only are they an investor bringing in the IP, but the motivation for them was to see the IP commercialized.”</p>\n<p>Mercedes is making certain assets available to Athos, including a test car which Athos is integrating its prototypes into so it can validate its technology stack.</p><h3 class=\"wp-block-heading\"><strong>Reference design</strong></h3><p>Polaris, the company’s first-generation mSoC, will be able to drop into new Mercedes vehicles. It includes three compute die, a cache in the center (DreamBig Semiconductor’s chiplet hub) and a full-fledged NPU from an undisclosed supplier that will support all ML operators. All these chiplets are from third-party suppliers (none are made in-house).</p><p>Compute die are coming from a supplier who already makes automotive-grade SoC chips – this is the first time this particular supplier is offering chiplets, Athos CTO François Piednoël told EE Times.</p><p>“We use a full system-on-chip [chiplet], and that system has a memory controller, GPU, CPU and everything – we do not intend to do much disaggregation,” he said, noting that specially-designed disaggregated GPU and CPU chiplets would require a lot of investment since it would require many tape-outs, which would be expensive. The company has instead chosen to disaggregate at the system level, he said.</p><p>Athos’ team, after leaving Mercedes, came up with a voting mechanism which works with a minimum of three chiplets to guarantee safety. Three chiplets means there’s no single point of failure; each chiplet monitors the other two for software issues like memory overflows, and hardware issues like radiation-driven bit-flips (critical in aerospace applications, for example).  </p><p>Chiplets’ scalability advantages will apply going forward. Using a complete SoC chiplet means adding more chiplets would bump up all of Polaris’ computing performance parameters. This can be achieved in a few months versus years for new silicon. (The voting system requires odd numbers of compute chiplets so the voting never ends in a tie; a next-generation version of Polaris will use 7).</p><p>Piednoël said that a future level 4 robotaxi will need a backup for its backup to avoid having to stop the ride if something goes wrong (since the passenger would be unable to take over).</p><p>“We can enable very easily, just as a matter of cost, a backup for the backup,” Piednoël said. “That means we can do non-interrupted level four. If you were to try to do this with multiple chips on a main board, you would end up with a board that probably needs four or six chips minimum.”</p><p>Enabled by chiplets, this is part of how the company wants to enable higher performance at lower cost points, he said.</p><h3 class=\"wp-block-heading\"><strong>Software complexity</strong></h3><p>Software for functional safety applications is notoriously complex.</p><p>“[Polaris] has a scheduler which allows us to remove most of the modern threading that used to be the difficult part to certify,” Piednoël said. “Every time you create a thread, you need to make sure that the programmer of that thread didn’t do anything that can cause problems for the other threads – you need exhaustive validation to do that into a level three stack, [since] that’s about 1,200 programs competing with each other.”</p><p>Athos’ AMBA-compatible chiplet-based approach simplifies the software stack tremendously using hardware scheduling.</p><p>Customers will use Polaris with their own autonomy software stacks for varied types of system. A future family of Polaris SKUs could offer different CPU/NPU performance levels for different applications, Bangar said. Athos’ hardware is agnostic to customers’ upper levels of software, since Athos’ stack has an orchestration layer which directs the workload across Polaris’ different chiplets, he added.</p><p>“The challenge we see when it comes to chiplet technology is that you have UCIe, which has standardized the physical interconnect electrically, but nobody’s working on the interoperability problem,” Bangar said. “How do you talk to chiplets from different vendors in a standardized way? That’s the layer of software that Athos will bring, and that abstracts the OEM from having to worry about that at the hardware firmware layer.”</p><p>Initial Polaris SKUs will be designed for robust level 3 highway driving. Piednoël’s vision is that the car will monitor the driver and be able to take over if they become distracted, with a seamless transition. “We believe there will be no buttons,” he said.</p><h3 class=\"wp-block-heading\"><strong>Highway roadmap</strong></h3><p>Polaris uses third-party silicon, but custom silicon for future generations is under development by Athos’ small but experienced team today. This is enabled by advances in AI-enabled EDA tools which are making the design process more efficient, Piednoël said.</p><p>“From an architectural point of view, I have put together this chiplet pretty quickly on the architecture side,” he said. “In the past, you would have needed a team of five or ten people – I’m almost done with the thing in a few months by myself.”</p><p>For Polaris, Athos’ partners are currently bringing up the individual chiplets. They will then be sent for assembly into the final system-on-package. Bangar expects to have Polaris engineering samples back around the middle of 2026, but the company already has some development silicon which it is using to test its software, as well as a software emulator of Polaris it is making available to customers, including Mercedes.</p><ul class=\"wp-block-list\"><li>For more detail on Polaris and Athos’ safety concepts, view François Piednoël’s keynote presentation at EE Times’ virtual conference “The Future Of Chiplets” which you can catch on-demand <a href=\"https://chiplets.eetimes.com/\" rel=\"noreferrer noopener\" target=\"_blank\">here</a>.  </li></ul><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><p><em>See also:</em></p><p><em><a href=\"https://www.eetimes.com/a-guide-to-building-chiplets-today-while-shaping-tomorrows-standards/\" rel=\"noreferrer noopener\" target=\"_blank\">A Guide to Building Chiplets Today While Shaping Tomorrow’s Standards </a></em></p><p><em><a href=\"https://www.eetimes.com/chiplets-consolidation-wave-is-just-beginning/\" rel=\"noreferrer noopener\" target=\"_blank\">Chiplets Consolidation Wave is Just Beginning</a></em></p><p><em><a href=\"https://www.eetimes.com/the-chiplet-economy-three-pillars-for-semiconductor-success/\" rel=\"noreferrer noopener\" target=\"_blank\">The Chiplet Economy: Three Pillars for Semiconductor Success</a></em></p><p><em><a href=\"https://www.eetimes.com/driving-toward-the-goal-of-making-automotive-chiplets-viable/\" rel=\"noreferrer noopener\" target=\"_blank\">Driving Toward the Goal of Making Automotive Chiplets Viable</a></em></p><p></p> </div>",
            "pub_date": "2025-10-18 06:55:29",
            "link": "https://www.eetimes.com/athos-spins-out-of-mercedes-with-chiplet-concept-for-avs/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Texas Instruments On Its Path to 95% In-House Manufacturing by 2030",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p><em>An exclusive interview with TI’s EMEA President on growing internal capacity with in-house fabs and design agility, plus its plans in India.</em> </p><p>On its mission to increase self-reliance in semiconductor production, Texas Instruments (TI) is has set a target to grow its internal manufacturing capacity to more than 95% by 2030. Stefan Bruder, President, Texas Instruments Europe, Middle-East, Africa and India (EMEA), said the company is investing in facilities across multiple regions to deliver geopolitically dependable capacity. </p><p>“You need to be flexible, as the future is uncertain,” Bruder told <em>EE Times</em> in an exclusive interview. “That is why we are investing in our own manufacturing, aiming to grow our internal manufacturing to more than 95% by 2030. We have manufacturing sites in different global locations, and we run them with dual-flow capabilities. This allows us to switch production between locations to ensure continuity and adapt in a world that is ever-changing.” </p>\n<p>In June 2025, <a href=\"https://www.eetimes.com/texas-instruments-updates-60-billion-u-s-fab-investment-plans/\" rel=\"noreferrer noopener\" target=\"_blank\">TI updated its long term plans</a> which equate to more than $60 billion in seven fabs being invested since 2021 across three U.S. mega-sites in Texas and Utah, including up to $40 billion at its new 300mm site in Sherman, Texas. The company says the combined projects could support over 60,000 jobs. TI currently operates 300mm fabs in Richardson, Texas, and Lehi, Utah, among 15 manufacturing sites globally. Customers include Apple, Ford, Medtronic, NVIDIA, and SpaceX. </p>\n\n<p>The expansion comes as TI looks to strengthen its global position against growing competition from Chinese manufacturers of foundational lower-end chips, while also meeting rising demand for semiconductors that power vehicles, smartphones, and data centers. TI’s approach extends beyond factories to include raw material procurement. The company tries to source materials locally in the regions where it manufactures, which Bruder said helps improve supply resilience and reduce dependency on other areas. </p>\n<h3 class=\"wp-block-heading\">The AI strategy </h3><p>Bruder said TI’s work in artificial intelligence spans both large data centers and edge devices. “AI data centers are highly power-intensive. Our gallium nitride products increase power efficiency, lowering data center energy use,” he explained. </p><p>Gallium nitride technology is also gaining traction, with compact chargers for phones and portable devices among the first applications globally. “Customers are looking closely at gallium nitride for the efficiency and smaller form factor it provides,” Bruder said. </p><p>At the same time, TI is embedding AI capabilities directly into devices. “For example, our C2000 family of real-time microcontrollers has embedded neural network capabilities, suited for detecting sparks in solar inverters—making it easier for device makers to integrate AI at the edge.” Many of these innovations, he confirmed, are being developed at TI in India, one of the company’s largest R&amp;D hubs, in collaboration with TI’s global engineering, design and manufacturing teams. </p><h3 class=\"wp-block-heading\">Engaging with the Indian market </h3><p>India is central to this strategy, both as a design hub and a growing customer base. Bruder noted that about 20% of the world’s engineering population is based in India. “This is one of the reasons we started our engagement here 40 years ago. We want to leverage this by doing product design and also engaging with customers.” </p><p>That engagement extends to talent development programs. TI runs the Women in Semiconductors and Hardware Program (WiSH) for second-year female engineering students, which concluded its fourth edition in 2025 with over 1,500 registrations and 190 participants. The program combined three weeks of online mentoring with a final week at TI’s Bengaluru campus, including lab visits, projects, and sessions with company engineers. Bruder said the program is part of TI’s effort to draw young engineers into the semiconductor field, whether in product design or sales roles. </p><p>India, he added, is not only a source of talent but also an expanding market. “India is one of the largest economies worldwide and has been growing fast. We see multinational corporations investing in India and leveraging its design and engineering capabilities, as well as many local companies that are very active. Those are the customers we are engaged with.” </p><h3 class=\"wp-block-heading\">Innovation for local needs </h3><p>Bruder said Indian customers are asking for more system-level solutions, which TI supports through reference designs and demos. He also pointed out that India often leads in innovations driven by local needs. “It is hot here, so efficient HVAC systems are in demand. Pairing them with solar panels offers a locally relevant solution given the abundant sunlight. This is innovation based on local needs, and it is done sustainably. That is where India is leading.” </p><p>The automotive sector is another focus, particularly electric two- and three-wheelers. “A key requirement is affordability,” Bruder said. “Our chips help reduce system costs. We have demos showing instrument clusters for both electric and combustion two-wheelers, with seamless smartphone connectivity,” he added, referencing TI’s showcase at electronica India 2025. </p><h3 class=\"wp-block-heading\">Addressing cost concerns </h3><p>TI also highlighted renewable energy at electronica, displaying its C2000 microcontrollers for solar adoption through efficient power conversion. For India’s cost-sensitive market, Bruder pointed to TI’s new F28E12 series of C2000 microcontrollers, which start at 49 cents per unit for 1,000-unit orders. </p><figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"></figure><p>Beyond product development, TI has expanded how customers can buy its chips. Components can be purchased directly from TI, including through TI.com in local currency, or through TI’s authorized distributors. “Customers can order as little as one unit—there is no minimum order quantity,” Bruder said. </p><p>Close interaction with customers, including those in India, along with engineers’ proximity to local challenges, feeds back into product design. “This way, we release products designed for the local market and help India lead in innovation,” Bruder said. </p><p>Bruder said TI’s priority is to be easy to work with, from providing design information and direct engagement with teams to flexible procurement. The goal, he concluded, is to support customers building efficient and affordable systems. </p><p><em>See also:</em> </p><p><a href=\"https://www.eetimes.com/texas-instruments-updates-60-billion-u-s-fab-investment-plans/\" rel=\"noreferrer noopener\" target=\"_blank\"><em>Texas Instruments Updates $60 Billion U.S. Fab Investment Plans</em></a><em></em> </p><p><a href=\"https://www.eetimes.com/ti-begins-gan-production-in-japan-to-boost-internal-capacity/\" rel=\"noreferrer noopener\" target=\"_blank\"><em>TI Begins GaN Production in Japan to Boost Internal Capacity</em></a><em></em> </p><p><a href=\"https://www.eetimes.com/peering-into-the-pepper-flake-sized-mcu/\" rel=\"noreferrer noopener\" target=\"_blank\"><em>Peering into the Pepper Flake-Sized MCU</em></a><em></em> </p><p><a href=\"https://www.eetimes.com/electronica-2024-tis-amichai-ron-on-edge-ai/\" rel=\"noreferrer noopener\" target=\"_blank\"><em>Electronica 2024: TI’s Amichai Ron on Edge AI</em></a><em></em> </p> </div>",
            "pub_date": "2025-10-16 09:05:34",
            "link": "https://www.eetimes.com/texas-instruments-on-its-path-to-95-in-house-manufacturing-by-2030/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Google Open-Sources NPU IP, Synaptics Implements It",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>Google Research has open-sourced its <a href=\"https://www.eetimes.com/podcasts/what-the-google-and-synaptics-collaboration-means-for-edge-ai/\">Coral NPU IP (previously codenamed Kelvin)</a>, which it is giving to the industry in a bid to accelerate edge AI implementations by reducing fragmentation and improving security. Synaptics is the first to implement this NPU in silicon as part of its Astra SL2610 series of IoT device SoCs.</p><p>Google has been working on a new approach to enable the edge AI ecosystem to grow, Billy Rutledge, director of edge AI research at Google, told EE Times.</p><p>“We’re trying to affect nascent wearable-class SoCs before they become mature to try to avoid the things that we’ve seen in the mobile phone SoC industry,” Rutledge said. “There’s a lot of fragmentation in the world of mobile phones. Can we prevent that from happening in wearables?”</p>\n<p>Google had built a mini-TPU ASIC for edge AI marketed under the Coral brand as far back as 2017; this recent work on the Coral NPU builds on learnings from that project, though the IP itself isn’t based on the architecture of the earlier Coral ASIC. Rutledge said the team identified the key roadblocks for edge AI being sufficient performance for sophisticated models within tight power budgets, cumbersome toolchains not helped by ecosystem fragmentation, and security. Google wants to address all of these challenges at the ecosystem level by offering an easy-to-use standards-based platform as a next generation to the original Coral.  </p>\n\n<p>“This time around, we’re taking an open standards approach, an IP based approach,” Rutledge said. “Instead of offering a proprietary ASIC from Google, we’re going up one level higher and offering IP for other silicon companies to take for free.”</p>\n<p>The Coral NPU today is a 4-way superscalar 32-bit RISC-V CPU. A vector engine is under development as part of the Coral NPU platform, and a matrix engine, which will also come later.</p><p>RISC-V, being open, modular and extensible fit the bill perfectly, Rutledge said.</p><p>“We’ve chosen RISC-V because of its rapid adoption and interest in the ecosystem today, but also it’s modular and flexible, easily extendable by others, and there’s no licensing fees or royalties required,” Rutledge said. “It gives us a good basis to build open-source hardware designs and be able to share them broadly with others.”</p><p>Google imagines most companies will start with the small, lightweight CPU as a consistent front-end to other execution units on chip. A compiler and software stack that can lower models from any ML framework onto the CPU has also been open-sourced.</p><p>“We’re creating this open-standards-based pipeline from the ML frameworks all the way down to the NPU front end and trying to encourage consideration and adoption, and then specialization of that for different industry segments,” Rutledge said.</p><p>The move to RISC-V is intended to reduce fragmentation in software stacks, thereby accelerating edge AI deployments, Rutledge said. If Google expects implementations of its NPU to be customized by silicon vendors, how can it ensure compatibility?</p><p>“Following the specs and compliance, if we’re able to keep those policies and checks in place, then our tools should work properly,” Rutledge said, noting that math extensions are probably going to be common to most compute-heavy designs, limiting fragmentation.</p><p><strong>Synaptics silicon</strong></p><p>Synaptics has the first production deployment of the Coral NPU; the company has incorporated it into the Astra series of AI-enabled IoT SoCs, which range from application processor level to microcontroller level parts. The first SKUs to become available are the SL2610 family of five chips for applications ranging from smart appliances to retail point of sales terminals and drones. All parts in this family have dual Arm Cortex-A55 cores implemented in 12 nm. Some parts in the SL2610 family have the NPU subsystem, some don’t.</p><p>“We feel this is going to be a very disruptive product for the spaces that it is positioned for – it’s primarily for IoT,” Nebu Philips, senior director of strategy and business development at Synaptics told EE Times. “A lot of the IoT silicon out there is repurposed from other large primary markets from the large semi players. [Their] primary market is automotive or industrial and that die will always have some IP that is very specific for their primary markets.”</p><p>Synaptics has dubbed its combination of AI hardware and software Torq. The hardware block includes the Coral NPU (implemented as a tightly-coupled CPU optimized for Synaptics’ PPA requirements), plus Synaptics’ home-grown AI accelerator (the T1, a fixed-function 1 TOPS (INT8) accelerator for transformer and CNN operations). The Coral NPU will be used to accelerate scalar operations without having to go across the AXI bus to the A55s. On the software side, there is an open-source toolchain which includes Google’s MLIR/IREE compiler and runtime.</p><p>While many models and frameworks are becoming standard, there is still a lot of fragmentation on the compiler side, Philips said, noting that many silicon vendors have acquired software toolchain providers for this reason (Qualcomm/Edge Impulse, ST/Cartesiam, Infineon/Imagimob, etc).</p><p>“All these are nicely packaged into pretty good user experiences, but it’s very tightly coupled with their silicon portfolio,” Philips said. “[The ecosystem] is beginning to show some signs of lock-in because the tooling is very closely tied to the silicon. Along with Google, we want to get ahead of that [by] going completely open source.”</p><p>Synaptics’ toolchain is based on IREE, an MLIR-based compiler and runtime project that started at Google Research. The companies partnered to support multiple ML frameworks at the front end and Synaptics Torq at the lower levels. (A “vanilla” version will be hosted on the Coral NPU site, though today it’s fundamentally the same as the Synaptics version since Synaptics has the only implementation of the NPU in silicon, Rutledge said).</p><p><strong>Coral roadmap</strong></p><p>Google Research is working with Verisilicon to productize and support the Coral NPU; Verisilicon has performed extensive design verification testing on the IP for different process nodes, Rutledge said, as well as building test silicon and overall ensuring quality. Verisilicon will make the IP available to companies on Google’s behalf (and support it, for a fee), but working with Verisilicon is optional if companies don’t need that support, he said.</p><p>On Coral’s roadmap are a new version of the vector core, which will support LLM inference, and a matrix engine, once the appropriate RISC-V profile is ratified. CHERI support is also on the roadmap.</p><p>As part of Google, the Coral NPU team are working behind the scenes with other parts of the business on tiny model development, Rutledge said.</p><p>“We can optimize the Coral NPU further to support new architectures coming from Google, which is something we’re pretty excited about,” he said. “[Google] is the thought leader in this space for new architectures, having created both MobileNet and the transformer. We want to make sure that the Coral NPU is co-designed and optimized for the Google architectures as they come out.”</p> </div>",
            "pub_date": "2025-10-15 22:20:26",
            "link": "https://www.eetimes.com/google-open-sources-npu-ip-synaptics-implements-it/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "AMD Raises GPU Bar with Landmark OpenAI Deal",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>OpenAI, which is all over the artificial intelligence (AI) landscape, has <a href=\"https://openai.com/index/openai-amd-strategic-partnership/\" rel=\"noreferrer noopener\" target=\"_blank\">inked a strategic deal with AMD</a> that is expected to bring the GPU supplier more than $100 billion over a five-year period. Moreover, this multibillion-dollar partnership will allow OpenAI to acquire a 10% stake in AMD.</p><p>OpenAI is committed to purchasing 6 gigawatts of AMD GPUs, starting with the supply of 1 gigawatt worth of MI450 GPUs in the second half of 2026. OpenAI will buy 6 gigawatts of AMD GPUs either directly from AMD or through its cloud partners.</p><p>AMD claims its MI450 series will outperform Nvidia’s comparable offerings such as Rubin CPX through hardware and software improvements. “AMD GPUs, such as the MI355X, at the right price and on the right workload, already show a perf/$ benefit over the Nvidia B200,” said SemiAnalysis analyst Jordan Nano.</p>\n<p>It’s worth noting that AMD’s collaboration with OpenAI began with the supply of MI300X GPUs in 2023 and continued with the MI350X series. MI450 will be AMD’s sixth data center GPU with AI capabilities and fourth “real” AI GPU following the <a href=\"https://www.eetimes.com/can-amds-mi300x-take-on-nvidias-h100/\" rel=\"noreferrer noopener\" target=\"_blank\">MI300X</a>, MI325X, and MI355X, which are all currently shipping and available for rent at prominent neoclouds.</p>\n\n<p>In fact, OpenAI had already been onstage at AMD’s Advancing AI event in June 2025, talking about the company’s use of AMD GPUs. That’s also when AMD Chair and CEO Lisa Su called OpenAI a “very early design partner” for AMD’s MI450 GPUs.</p>\n<h3 class=\"wp-block-heading\"><strong>AMD’s upside</strong></h3><p>The arrangement between AMD and OpenAI is a marvel of creative financing. AMD chief Su called it pretty innovative and added, “I wouldn’t say it came lightly.”</p><p>In this “multi-year, multi-generation” partnership, AMD will issue OpenAI a warrant to purchase as many as 160 million shares at an exercise price of $0.01 over time, provided that OpenAI meets GPU deployment milestones. These shares represent approximately 10% of AMD. The deal also stipulates that this arrangement will only be exercised if AMD’s share price increases due to this groundbreaking partnership.</p><p>Industry watchers call it a breakthrough for AMD. It makes AMD a strategic supplier to a leading AI company, which could entice more Tier 1 AI firms. On the other hand, OpenAI is able to lock up sufficient data center capacity to build next-generation AI systems. “It’s hard to overstate how difficult it has become to get enough computing power,” said OpenAI co-founder and CEO Sam Altman.</p><p>“This is a major win for AMD and shows that it has been putting the right strategy in place to take advantage of the AI mega-wave with its advanced GPUs,” said Jack Gold, President and Principal Analyst at J. Gold Associates, LLC. “It’s also an indication that OpenAI recognizes a need to diversify its processor supplies, as it continues to expand its data centers.”</p><p>Especially when most advanced Nvidia GPUs are on allocation with buildouts outpacing supplies. “By solidifying AMD chip supplies through this commitment and investment, OpenAI can continue its massive buildout campaign,” Gold added.</p><h3 class=\"wp-block-heading\"><strong>AMD versus Nvidia</strong></h3><p>Another upside for AMD in this landmark deal is that it enables the company to challenge Nvidia’s dominance in the AI compute market. “It’s an endorsement that AMD has become competitive with Nvidia’s processing power, as well as enhancing its software strategy to compete, much like it did in the data center market earlier against Intel,” Gold said.</p><p>According to Mizuho Securities, Nvidia has captured more than 70% of the market for AI chips. Here, while AMD’s processors are widely used in gaming, PCs, and traditional data center servers, they haven’t made a lot of inroads in pricier compute chips for advanced AI systems. In fact, AMD and other Nvidia rivals have sought to offer more affordable AI chip alternatives as secondary suppliers.</p><p>This arrangement between AMD and the maker of ChatGPT could potentially disrupt Nvidia’s AI dominance. “It’s highly likely that other major AI players will follow suit and deploy AMD-powered data center chips,” said Gold. “Ultimately, this is a huge win for AMD and its ability to become a true competitor to Nvidia in AI and the first choice in AI processors.”</p><p>It’s important to note that OpenAI’s tie-up with AMD is definitive, unlike its $100 billion deal with Nvidia announced last month, in which Nvidia would become a “preferred strategic compute and networking partner” for building new AI data centers. The Nvidia deal isn’t yet completed; although the two companies have signed a letter of intent, they have yet to disclose details about the regulatory filing.</p><p>SemiAnalysis’s Nano acknowledged that the structure of the deals is very different. He also quoted OpenAI chief Altman, “We plan to increase our Nvidia purchasing over time. The world needs much more compute.”</p><h3 class=\"wp-block-heading\"><strong>Exciting times in AI</strong></h3><p>Forrest Norrod, executive VP at AMD, calls this deal transformative, not just for AMD, but for the dynamics of the industry. AMD, striving to establish itself as a credible alternative to Nvidia’s AI hardware and software, has finally scored a significant win. No wonder Lisa Su calls it a major reflection point for AMD.</p><p>AMD generated $5 billion from its Instinct GPUs in 2024, compared to Nvidia’s $102.2 billion from data center compute products. That could change with AMD’s compute commitment with OpenAI; it could eventually narrow Nvidia’s AI lead and bring hyperscalers like Meta and Microsoft to AMD’s door.</p><p>According to Gold, in two to three years, AI will shift its focus to inference as the primary workload, rather than relying on large-scale data center expansions used for model training. “However, in the meantime, there is a massive market for GPUs, and AMD is now able to capture a significant piece of this revenue.” He also points out that it’s a major win for TSMC, as it fabricates chips for both AMD and Nvidia. “TSMC profits when either AMD or Nvidia powers new data centers.”</p><p>Exciting times in the world’s most ambitious AI buildout and advancement of the entire AI ecosystem.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/> </div>",
            "pub_date": "2025-10-15 19:02:01",
            "link": "https://www.eetimes.com/amd-raises-gpu-bar-with-landmark-openai-deal/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Qualcomm To Acquire Arduino",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>Qualcomm Technologies <a href=\"https://www.qualcomm.com/news/releases/2025/10/qualcomm-to-acquire-arduino-accelerating-developers--access-to-i\" rel=\"noreferrer noopener\" target=\"_blank\">announced it is acquiring Arduino</a>, a move aimed at democratizing access to edge computing and AI for developers worldwide.</p><p>The news follows earlier acquisitions by Qualcomm of Edge Impulse and Foundries.io and will give Qualcomm access to over 33 million active users in the Arduino community.</p><p>While no details of the transaction were provided at the time of writing, Qualcomm stated that Arduino will retain its independent brand, tools, and mission, while continuing to support a wide range of microcontrollers and microprocessors from multiple semiconductor suppliers.</p>\n<p>Qualcomm emphasized that Arduino’s simplicity, affordability, and community would help developer productivity across industries, with Arduino preserving its open approach and community spirit while unlocking a full-stack platform for modern development, starting with the introduction of a new single-board computer, Arduino UNO Q.</p>\n\n<p>The new Arduino UNO Q, highlighted as part of the announcement, features a “dual brain” architecture incorporating the Qualcomm Dragonwing QRB2210 processor running a full Linux environment.</p>\n<p>The single-board computer is aimed at developers creating AI-powered vision and sound solutions that react to their environment, ranging from sophisticated smart home solutions to industrial automation systems. Qualcomm said UNO Q is designed to become ‘the go‑to tool for every developer.’</p><p>Qualcomm said that Arduino is also launching App Lab, a new, integrated development environment built to unify the Arduino development journey across Real-time OS, Linux, Python and AI flows to make development faster and easier.</p><p>As an open-source platform designed to help developers rapidly ideate, prototype, and scale AI-powered solutions to production, App Lab is said to integrate seamlessly with the Edge Impulse platform. Qualcomm stated that this helps streamline and accelerate the process of building, fine-tuning, and optimizing AI models using real-world data for a wide range of capabilities, including object/human detection, anomaly detection, image classification, ambient sound recognition, and keyword spotting.</p><h3 class=\"wp-block-heading\">Giving Arduino users a path towards commercialization</h3><p>In a prepared statement, Nakul Duggal, group general manager for automotive, industrial, and embedded IoT, for Qualcomm Technologies, said, “With our acquisitions of Foundries.io, Edge Impulse, and now Arduino, we are accelerating our vision to democratize access to our leading‑edge AI and computing products for the global developer community. Arduino has built a vibrant global community of developers and creators. By combining their open-source ethos with Qualcomm Technologies’ portfolio of leading-edge products and technologies, we’re helping enable millions of developers to create intelligent solutions faster and more efficiently – including a path towards global commercialization by leveraging the scale of our ecosystem.”</p><p>And with his excitement showing in his statement, Fabio Violante, CEO of Arduino, added, “Joining forces with Qualcomm Technologies allows us to supercharge our commitment to accessibility and innovation. The launch of UNO Q is just the beginning— we’re excited to empower our global community with powerful tools that make AI development intuitive, scalable, and open to everyone.”</p><p>The closing of the transaction is subject to regulatory approval and other customary closing conditions.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/> </div>",
            "pub_date": "2025-10-15 19:02:00",
            "link": "https://www.eetimes.com/qualcomm-to-acquire-arduino/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Emerson Leverages AI to Address Complexity in Test and Measurement",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>MADRID, Spain. At Publitek Connect 2025, Emerson Test &amp; Measurement, the business unit formerly known as National Instruments (NI), is announcing the availability of its <a href=\"https://www.emerson.com/en-us/news/2025/07-emerson-expands-ai-capabilities-into-test-and-measurement-software-portfolio\" rel=\"noreferrer noopener\" target=\"_blank\">specialized artificial intelligence platform, Nigel</a>. This development marks a significant strategic investment in software and data capabilities, designed to evolve the company’s established integrated platform of modular hardware and open software solutions.</p><p>The introduction of Nigel, described by the company as an AI Advisor, reflects an industry shift toward utilizing sophisticated computing power to manage increasingly complex systems under test.</p><p>National Instruments, which Emerson acquired two years ago but maintains the NI brand as an independent operating company, has a nearly 50-year history of leveraging technological innovations, such as the personal computer and graphical programming (LabVIEW, founded circa 1986), to advance test methodologies.</p>\n<h3 class=\"wp-block-heading\">Focus on platform investment</h3><p>The launch of Nigel is part of a broader commitment to the integrated platform, which supports 35,000 customers worldwide across critical growth sectors, including semiconductor, aerospace/defense, and transportation. Rudy Sengupta, vice president and general manager of Test and Analytics Software at Emerson Test &amp; Measurement, highlighted the holistic nature of this technological pivot. “AI, we believe, will permeate through our whole portfolio: hardware, software, and data,” Sengupta noted, emphasizing that the platform is evolving by leveraging this new capability.</p>\n\n<p>Current investments have spanned both hardware and software. On the hardware side, the company continues to develop its modular PXI instrument platform, including new RF products such as a 4 GHz+ VST Bandwidth offering, which allows customers testing next-generation standards like 6G to integrate new capabilities without discarding existing, high-cost capital equipment.</p>\n<p>The data acquisition portfolio has also expanded to include products like FieldDAQ, which is IP67 rated for use in rugged, high-shock, and vibration environments.</p><p>On the software front, the platform includes LabVIEW, the graphical programming environment, and the <a href=\"https://www.ni.com/es-es/shop/product/labview--suite.html?srsltid=AfmBOoqrAtemxFQik4FyDveKpi-Cd0FOCYs8t4PxV7_bL7udFThVroc8\" rel=\"noreferrer noopener\" target=\"_blank\">LabVIEW Plus Suite</a>, which assists test engineers with comprehensive workflow tasks, from planning requirements to data analysis and reporting. </p><p>Central to the data strategy is SystemLink, an enterprise-level data platform that helps customers track asset utilization and health, often across thousands of test benches globally, while also aggregating siloed engineering data.</p><h3 class=\"wp-block-heading\">Nigel: domain-specific advisor</h3><p>The Nigel AI Advisor is currently available in flagship products like LabVIEW and TestStand. The adoption rate for the software release featuring Nigel has reportedly been the fastest of any software released by the company in the last decade.</p><p>Nigel is differentiated from general-purpose AI assistants by its deep specialization in test and measurement. The tool is specifically trained on T&amp;M knowledge, hardware and APIs, test methodologies, and internal IP.</p><p>Sengupta described the AI’s immediate function: “Think of Nigel like an intern or an advisor that sits next to you,” helping users, particularly newer engineers, analyze complex code, identify required hardware, and find advanced intellectual property (IP) within a company’s systems.</p><p>The AI performs a context-aware search, meaning it can read and understand the function of code rather than just searching file names.</p><p>The goal of this capability is to expedite typical engineering workflows. For instance, in a semiconductor manufacturing process, the ability to optimize test sequences using AI insight could reduce time and cost by identifying faulty parts earlier in a 30-step final test phase.</p><h3 class=\"wp-block-heading\">Data security and business model</h3><p>Addressing industry concerns regarding proprietary data, Emerson maintains that no customer data is used in the training of the Nigel AI model. The company ensures that data remains within the customer’s secure environment, whether on a cloud or on-premise server.</p><p>The technical architecture of Nigel leverages a standard large language model (LLM), such as OpenAI’s model, but utilizes a technique called retrieval augmented generation (RAG). This process incorporates a filter of supplemental content—Emerson’s proprietary expertise, knowledge on test methodologies, and IP—to provide domain-specific answers without modifying the foundational LLM.</p><p>Regarding the business model, the company has elected to include the current advisory version of Nigel with existing subscription or service plans at no additional charge. Sengupta suggested that advanced data and analytics features delivered through Nigel in the future might introduce a tiered model, such as a “Nigel Pro” upgrade option.</p><h3 class=\"wp-block-heading\">Strategic roadmap for abstraction</h3><p>The development of Nigel is a necessary response to the increasing difficulty of testing modern devices. Products across industries, from automotive systems (now incorporating complex computing and battery management systems) to sophisticated aerospace jets and heterogeneous semiconductor chips, are exhibiting higher degrees of complexity.</p><p>The company views the AI roadmap in three phases: moving from an Advisor (current stage) to an Author, and eventually to an Agent. As an Author, Nigel could deliver code generation capabilities, with projections for these features to materialize in 2026 and 2027. As an Agent, the AI would autonomously complete aspects of the test workflow, such as keying in specifications from documentation, thereby improving efficiency for the test engineer.</p><p>This strategy aims to shift the current user-orchestrated workflow to one that the user and AI jointly orchestrate.</p><p>The company’s emphasis on integrating customer feedback and leveraging technology to meet market needs underscores the importance of the Nigel initiative. The core strategic motivation, according to Sengupta, remains efficiency: “The end goal is to make the customer more efficient. It’s to take the workflow that they already have and get through that task faster with lower cost.”</p><p>The integrated approach, combining compute-accelerated hardware with an AI-accelerated workflow, is central to Emerson’s effort to abstract the rising complexity faced by its global customer base.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><p></p> </div>",
            "pub_date": "2025-10-15 18:56:50",
            "link": "https://www.eetimes.com/emerson-leverages-ai-to-address-complexity-in-test-and-measurement/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "GenAI Can Raise Level Of Abstraction For XMOS Tools",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>XMOS is working to enable generative AI-based natural language tools for the configuration of its Xcore devices, a property uniquely enabled by the Xcore’s properties, XMOS CEO Mark Lippett told EE Times.</p><p>Raising the level of abstraction of its toolflow could transform XMOS’ customer experience, Lippett said.</p><p>“[It’s about] how can we make a transformational change to how our customers get to market?,” he said. “How can we not just accelerate that, but also broaden it so that we can engage more customers without wrapping them with field application engineers? Customers want to be able to succeed without having to pick up the phone to us.”  </p>\n<p>Enter XMOS’ <a href=\"https://www.xmos.com/gensoc\" rel=\"noreferrer noopener\" target=\"_blank\">generative SoC concept</a>, which is enabled by the same configurability the Xcore has offered over the last 10 years (more than 35 million Xcore-based processors have been shipped to date).</p>\n\n<p>“The Xcore is a parallel array of processors that is capable of doing many special things, most of which are rooted in its hard real-time behavior,” Lippett said. “The thesis of XMOS is that we can empower software engineers with off-the-shelf silicon platforms to do things that hardware engineers could only have done, shrinking design cycle times from years down to months.”</p>\n<p>The Xcore’s main application so far has been audio processor applications, but the processor can be used for DSP, AI and other workloads (it is a general purpose programmable computing platform, said Lippett). Its core capabilities are well-known and well-validated, he said.</p><p>As a proprietary core, the Xcore has its own toolset developed in-house at XMOS, including a complete embedded software development flow. The advent of generative AI has added a new dimension to what is possible; the company is working on development tools that enable access to the Xcore’s hardware properties via generative AI prompts.</p><p>“We’re not too far away from being able to demonstrate the ability to describe a system in natural language, [and] to translate that down into a design that will operate on the Xcore platform functionally, but also, critically, from a timing perspective,” Lippett said. “Those are core attributes of the platform itself, to such an extent that you could deploy it [more quickly].”</p><p>Lippett estimates this will bring the development time for Xcore-based systems from months to days. Simple examples can be produced in a matter of minutes, he said.</p><p>“That doesn’t just transcend the way that Xcore was designed, the way our customers have engaged with Xcore, but it transcends the way that customers engage with other competitive platforms as well,” he said.</p><p>In the Generative SoC concept, instead of deciding manually how to map an application onto a DSP, AI accelerator and I/O primitives in hardware, XMOS’ tools will be able to do this. Since the Xcore’s architecture enables static verification of both functionality and timing, it can also be tested by higher-level tools.</p><p>“[The Xcore] has an array of processors that can be partitioned from one another to the extent that they don’t interfere with each other’s function, but also, timing, which means you can compose systems in a way that you can’t with time-sliced approaches, which generally speaking, rely on empirical methods to make sure they will work,” Lippett said.</p><p>XMOS will work on implementing a generative layer on top if its existing tool chain, for its DSP use cases first, based on the company’s experience in this area. The company will internally generate its own training data, Lippett said, using the system-level design work the company has been doing over the last 10 years or so.</p><p>XMOS plans to demonstrate DSP solutions with its generative tool before the end of 2025.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><p></p> </div>",
            "pub_date": "2025-10-15 18:56:49",
            "link": "https://www.eetimes.com/genai-can-raise-level-of-abstraction-for-xmos-tools/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Intel’s Confidence Shows As It Readies New Processors on 18A",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>Intel has today announced its latest generation of client and server processors, Panther Lake and Clearwater Forest, and has said these have been in production and ready for rollout in volume early next year on its latest 18A process technology at its Fab 52 state-of-the-art manufacturing facility in Chandler, Arizona. </p><p>The big news is that this is a story of massive disaggregation, use of gate-all-around (GAA) and back-side power delivery technologies, utilizing Intel’s advanced packaging technologies, with the manufacturing ready 18A technology. Intel called it a move from system <em>on</em> chip to a system <em>of</em> chips using its RibbonFET and PowerVia technologies manufactured in 18A, as the two slides below illustrate.</p><p>The products announced are the Intel Core Ultra series 3 processors (code-named Panther Lake) the Intel Xeon 6+ (code-named Clearwater Forest), both built on Intel 18A. Intel said both Panther Lake and Clearwater Forest, as well as multiple generations of products built on Intel 18A, are being manufactured at Fab 52, Intel’s new, state-of-the-art factory in Chandler, Arizona.</p>\n<p>Details were revealed to media and analysts at the Intel Tech Tour event in Arizona a couple of weeks ago, where we were also shown into the facilities at Fab 52 at the 700-acre plus Ocotillo site, which is also the home of Fab 12, 32, 42, plus the future Fab 62. The Fab 52 facility is fully dedicated to Intel 18A manufacturing, and Intel emphasized the fact that 18A is the world’s most advanced process technology researched, developed, and manufactured on American soil.</p>\n\n<p>CEO Lip-Bu Tan didn’t attend the Intel Tech Tour, but in a prepared statement said, “Our next-gen compute platforms, combined with our leading-edge process technology, manufacturing and advanced packaging capabilities, are catalysts for innovation across our business as we build a new Intel. The United States has always been home to Intel’s most advanced R&amp;D, product design and manufacturing – and we are proud to build on this legacy as we expand our domestic operations and bring new innovations to the market.”</p>\n<p>However, Sachin Katti, Intel’s senior VP, chief technology and AI officer, spoke at the event and commented, “We are building a new Intel, with products manufactured here in the U.S. to empower global customers around the world.”</p><p>Katti was clearly briefed by his CEO as he said that the company will do this by being engineering-led, have an innovation mindset, be disciplined in execution, and be customer and work-load centric. And he added that the partnership with Nvidia is part of an open strategy, ensuring that Intel is partnering with the entire ecosystem.</p><p>Kevin O’Buckley, SVP and GM, Intel Foundry, added in his speech that Panther Lake and Clearwater Forest embrace a system of chips. “We’re moving from the homogenous system on chip (SoC), to systems of chips, or chiplets. These are the new SoCs.” However, such systems of chips, he said, require significant advanced packaging. He also added that Intel was the first in the industry delivering gate-all-around transistors and backside power delivery.</p><p>We have put together a series of video interviews to give you a sense of the mood at Intel, what we learned from the Intel Tech Tour, the focus on edge computing with Panther Lake, and the opportunities for Clearwater Forest in data centers as well as beyond this in telecommunications and storage. Watch the videos below:</p><h3 class=\"wp-block-heading\"><strong>An analyst perspective: it’s looking good for Intel</strong></h3><p>In this video, EE Times’ editor-in-chief Nitin Dahad chats to Jim McGregor, Principal Analyst at Tirias Research, to explore the impact of the new announcements plus the mood at Intel. Watch the video below:</p><figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"></figure><p><em>Nitin Dahad talks to Jim Jim McGregor, Principal Analyst at Tirias Research.</em></p><h3 class=\"wp-block-heading\"><strong>The opportunity for Panther Lake at the edge</strong></h3><p>In this video interview, EE Times spoke to Mike Masci, vice president and general manager of edge product management at Intel, to discuss what Panther Lake means for edge computing, the opportunities for the new processor at the edge, use cases, and some of the key trends in edge computing applications. Watch the video below:</p><figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"></figure><p><em>Nitin Dahad chats with Mike Masci, vice president and general manager of edge product management at Intel.</em></p><h3 class=\"wp-block-heading\"><strong>Clearwater Forest for the data center and beyond</strong></h3><p>In this video interview, EE Times spoke to Keyur Halari, director of Xeon product line and systems, network and edge group, to talk about the innovations in Clearwater Forest, target markets beyond just data centers, and upgrade paths for existing customers. We also asked why it is named 6+ rather than the next number in the series, despite the innovations in the product. Watch the video below:</p><figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"></figure><p><em>Nitin Dahad speaks to Keyur Halari, director of Xeon product line and systems, network and edge group, Intel.</em></p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><h3 class=\"wp-block-heading\"><strong>And – from the news release</strong></h3><p>If you want to dig deeper into the announcements, here’s an edited version of what <a href=\"https://www.intc.com/news-events/press-releases/detail/1752/intel-unveils-panther-lake-architecture-first-ai-pc\" rel=\"noreferrer noopener\" target=\"_blank\">Intel’s news release</a> says about Panther Lake, Clearwater Forest, and 18A:</p><h3 class=\"wp-block-heading\"><strong>Panther Lake: scalable AI PC performance built on 18A </strong></h3><p>Aimed at a broad spectrum of consumer and commercial AI PCs, gaming devices and edge solutions, Intel Core Ultra series 3 processors are the first client chips built on Intel 18A, introducing a scalable, multi-chiplet architecture that offers partners ‘unprecedented’ flexibility across form factors, segments and price points.</p><p>Highlights include:</p><ul class=\"wp-block-list\"><li>Lunar Lake-level power efficiency and Arrow Lake-class performance.</li>\n<li>Up to 16 new performance-cores (P-cores) and efficient-cores (E-cores) delivering more than 50% faster CPU performance vs. previous generation.</li>\n<li>New Intel Arc GPU with up to 12 X<sup>e</sup> cores delivering more than 50% faster graphics performance vs. previous generation.</li>\n<li>Balanced XPU design for next-level AI acceleration with up to 180 Platform TOPS (trillions of operations per second).</li></ul><p>Beyond the PC, Panther Lake will extend to edge applications including robotics. A new <a href=\"https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Simplify-Physical-AI-Deployment-with-Intel-Robotics-AI-Suite/post/1719666\" rel=\"noreferrer noopener\" target=\"_blank\">Intel Robotics AI software suite</a> and reference board enables customers with sophisticated AI capabilities to rapidly innovate and develop cost-effective robots using Panther Lake for both controls and AI perception. </p><p>Panther Lake will begin ramping high-volume production this year, with the first SKU slated to ship before the end of the year and broad market availability starting January 2026.</p><h3 class=\"wp-block-heading\"><strong>Clearwater Forest: efficiency and scale for the modern data center</strong></h3><p>Clearwater Forest is Intel’s next generation E-core processor. Branded Intel Xeon 6+, it is the most efficient server processor the company has ever created and is built on Intel 18A. Intel plans to launch Xeon 6+ in the first half of 2026.</p><p>Highlights include:</p><ul class=\"wp-block-list\"><li>Up to 288 E-cores.</li>\n<li>17% Instructions Per Cycle (IPC) uplift over prior generation.</li>\n<li>Considerable gains in density, throughput and power efficiency.</li></ul><p>Tailored for hyperscale data centers, cloud providers, and telcos, Clearwater Forest ‘enables organizations to scale workloads, reduce energy costs, and power more intelligent services.’</p><h3 class=\"wp-block-heading\"><strong>Intel 18A: U.S. technology setting new industry standards</strong></h3><p>Intel 18A is the first 2-nanometer class node developed and manufactured in the United States, delivering up to 15% better performance per watt and 30% improved chip density compared to Intel 3. The node was developed, qualified for manufacturing and began early production at the company’s Oregon location and is now ramping toward high-volume production in Arizona.</p><p>Key innovations on Intel 18A include:</p><ul class=\"wp-block-list\"><li>RibbonFET: Intel’s first new transistor architecture in over a decade, enabling greater scaling and more efficient switching for improved performance and energy efficiency.</li>\n<li>PowerVia: A groundbreaking backside power delivery system, enhancing power flow and signal delivery.</li></ul><p>Additionally, Foveros, Intel’s advanced packaging and 3D chip stacking technology, enables the stacking and integration of multiple chiplets into advanced SoC designs, delivering flexibility, scalability and performance at the system level.</p><p>Intel 18A forms the foundation for at least three upcoming generations of Intel’s client and server products.</p><p>More details are on <a href=\"https://newsroom.intel.com/client-computing/intel-unveils-panther-lake-architecture-first-ai-pc-platform-built-on-18a\" rel=\"noreferrer noopener\" target=\"_blank\">Intel’s web site</a>.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><p><em>See also</em>:</p><p><em><a href=\"https://www.eetimes.com/intel-foundry-we-are-listening-and-learning-from-our-customers/\" rel=\"noreferrer noopener\" target=\"_blank\">Intel Foundry: We Are Listening and Learning from Our Customers</a></em></p><p><em><a href=\"https://www.eetimes.com/intel-18a-advanced-packaging-is-key-to-tech-leadership/\" rel=\"noreferrer noopener\" target=\"_blank\">Intel 18A Advanced Packaging is Key to Tech Leadership</a></em></p><p><em><a href=\"https://www.eetimes.com/intel-facing-another-crossroads-18-a-or-14a-process-node/\" rel=\"noreferrer noopener\" target=\"_blank\">Intel facing another crossroads: 18A or 14A process node</a></em></p><p><em><a href=\"https://www.eetimes.com/what-does-softbanks-investment-in-intel-stand-for/\" rel=\"noreferrer noopener\" target=\"_blank\">What Does SoftBank’s Investment in Intel Stand For?</a></em></p> </div>",
            "pub_date": "2025-10-15 18:50:27",
            "link": "https://www.eetimes.com/intels-confidence-shows-as-it-readies-new-processors-on-18a/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Cinch Details CIN:APSE Interconnect Evolution for Harsh Environments",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>MADRID, Spain. Cinch Connectivity Solutions, a subsidiary of Bel Fuse Inc., used the Plublitek Connect 2025 event to detail the core features and development trajectory of its <a href=\"https://www.cinch.com/company/brands/cinapse\">CIN:APSE interconnect</a> technology, a system designed for high-reliability applications. </p><p>Jerry Metcalf, business development manager at Cinch Connectivity Solutions EMEA, discussed how the solderless, compression-based technology addresses critical design issues faced by electrical and mechanical engineers.</p><h3 class=\"wp-block-heading\">Core technology and robust design</h3><p>CIN:APSE is primarily a solderless interconnect device that relies on compression technology. The system utilizes a small, randomly wound wire button, typically made of gold-plated molybdenum wire, held within a specially shaped insulator housing. The connection requires compression to ensure a proper fit and continuity.</p>\n<p>Metcalf emphasized the design philosophy of the contact technology, noting that its effectiveness stems from its straightforward construction. “Its genius really is in its simplicity,” he stated, adding that the design lacks “convoluted complex, spring systems or anything like that.”</p>\n\n<p>The technology can withstand extreme conditions, including high vibration, high shock, and wide temperature ranges, spanning from desert launches to the cold of space. </p>\n<p>The maturity of CIN:APSE is shown by its NASA technology <a href=\"https://www.nasa.gov/directorates/somd/space-communications-navigation-program/technology-readiness-levels/\" rel=\"noreferrer noopener\" target=\"_blank\">readiness level 9 status</a>; “It’s in space, it’s on Mars. It’s in satellites around you. It’s in all sorts of systems around us,” Metcalf said. This proven reliability is essential for mission-critical uses, as he explained: “When it has to work, it must work. You can’t go up and change a device on a satellite if something’s stopped working.”</p><p>Due to the internal structure of the wire bundle contact, which provides multiple points of contact, Cinch asserts that they “don’t tend to see any real compromise in terms of resistance in the contacts” compared to traditional methods.</p><h3 class=\"wp-block-heading\">Application and configuration versatility</h3><p>The solderless nature of CIN:APSE avoids the restrictions associated with processing solder, such as cracking solder joint faults, and allows for simplified rework and removal of parts. This ease of assembly allows designers to easily disassemble, replace a component, and then reassemble without risking damage.</p><p>CIN:APSE supports high-density arrays, sometimes featuring 4,000 or more contact points. The device is highly customizable as a low-profile bridge between PCBs. Substrates can be designed with a low profile, typically down to about 0.7 millimeters, making it the slimmest interposer available in the market.</p><p>The technology allows for significant design flexibility, including the ability to build a variety of heights within a single device using spacers or plungers. Furthermore, engineers can mix different arrays—such as RF, power, and signal channels—within a single connector to save space and weight. Applications range from use as a socket for Land Grid Array (LGA) chips to connecting mother and daughter boards in mezzanine, right-angled, or co-planar arrangements.</p><h3 class=\"wp-block-heading\">CIN:APSE future development</h3><p>Historically, the American market for CIN:APSE leaned toward the IT industry, specifically the server farm and cloud markets, although in Europe, it focused more on defense and space applications. Metcalf noted a need to increase market visibility for the technology. Although engineers often recognize the applicability of the technology quickly, they frequently state, “I’ve never heard of it,” Metcalf noted.</p><p>Kelly Wigginton, senior director of marketing for Bel Fuse Inc., acknowledged that historically, Bel’s engineering focus meant that “when you don’t have that marketing component and go to market strategy and strong sales capability, then you’re just going to have more organic growth.” </p><p>Wigginton emphasized the current push to address this gap, stating, “There is awareness about making people realize that this [technology] is available and accessible.”</p><p>Recent adoption has been strong in the growing space market, particularly for low Earth orbit observation systems monitoring climate change, wildfires, and ocean movements.</p><p>Looking ahead, Cinch is developing new technology platforms to push the limits of CIN:APSE. Metcalf detailed three areas of focus: higher density formats, higher temperature tolerance, and enhanced speed performance. </p><p>The evolving requirements of the defense and space industries drive these new advancements. “Now we’re thinking about how we make the small form factors, how we make the nano versions, the high density versions, and the high temperature versions that apply to both defense and space,” Metcalf explained.</p><p>For extreme environments, Cinch is working on versions with temperature tolerances ranging from -200°C to 500°C. The company is also investigating higher speed performance, currently capable of handling up to 50 GHz, to determine “how far beyond that can we go.”</p><h3 class=\"wp-block-heading\">Mechanical limits and optical technology<strong> </strong></h3><p>Cinch is focusing on determining whether the remaining limitations are material or mechanical in nature, as “we’re really starting to push the boundaries of the technology now,” Metcalf concluded.</p><p>The company places a significant emphasis on optical technology. This intense focus aligns with the major industry trend of photonics, which is considered one of the key buzzwords and trends in the semiconductor sector, driven by the demand for faster speeds both inside systems and externally. </p><p>To advance this capability, Cinch maintains an R&amp;D research center down in Melbourne, Florida, where a talented team works on the next generation of their transceiver, specifically concentrating on the active opto-electronic side of things. </p><p>Furthermore, Cinch maintains a long history of innovation in passive optical components, as Jerry Metcalf noted regarding their UK operations: “In Chelmsford again, Cinch has been an innovator in expanded beam technology. We lead the field in that technology.” </p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><p></p> </div>",
            "pub_date": "2025-10-15 18:50:26",
            "link": "https://www.eetimes.com/cinch-details-cinapse-interconnect-evolution-for-harsh-environments/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Optics Shine Light On Brain Functions",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p><strong>PALO ALTO, Calif.:</strong> At the SPRC Symposium, the annual showcase of the Stanford Photonics Research Center, academics from Stanford, Strathclyde, and Glasgow gave attendees a glimpse of the future of optoelectronic devices for medical and other use cases.</p><p>Karl Deisseroth, known as the father of optogenetics, opened the show with a keynote presentation in which he showcased recent work in the field.</p><p>Optogenetics relies on opsins – light-sensitive proteins that respond to certain frequencies of light. Proteins with this property are still being discovered; the most promising come from ocean algae.</p>\n<p>The opsins used in optogenetics (channel rhodopsins), when activated by light, open pores that allow hundreds of ions to pass in and out of cells per incident photon, changing the cell’s electrical properties. Genetically engineering cells to add channel rhodopsins means some types of cells, including neurons, can be activated or deactivated by applying light at the right frequency and intensity.</p>\n\n<p>“We can bring [channel rhodopsins] to neuroscience and use them to turn neurons on or off using the fact that light was being transduced into electricity with a single gene, and that makes it very portable and targetable,” Deisseroth said. “You can deliver a single gene to cells of interest using genetic tricks, and we found that we could also use various optical strategies, fiber optics, and spatial light modulators to get all kinds of precise light targeting to the brains of freely moving, behaving fish, mice, rats, monkeys, and even people.”</p>\n<p>Modifications to naturally-occurring channel rhodopsins can drive the activation of individual neurons hundreds of times per second with blue light, and some versions switch on or off using different colours of light. Current techniques allow spots of light to be guided to single cells; up to 1000 individual cells can be turned on or off in a live mouse brain with millisecond precision, Deisseroth said.</p><p>ChRmine (“Carmine”) is particularly promising, given that it requires very little light to activate and can switch quickly. Its sensitivity helps avoid heating target cells and allows cells to be targeted non-invasively through the skull in some cases.</p><p>Initial work with ChRmine in mice produced interesting results. Researchers identified brain regions that naturally responded to visual stimuli (vertical or horizontal bars of light) and then optogenetically activated those cells, causing the mice to behave as if they were seeing the pattern.</p><p>“We played in activity to small numbers of the cells that naturally responded to vertical bars of light,” Deisseroth said. “The mouse behaved as if it was seeing something that wasn’t there. We were providing, effectively, a perception for the animal, and everything that we could record in the brain was behaving as if it was seeing something that wasn’t there.”</p><p>Recent work from Deisseroth’s group (<a href=\"https://www.science.org/stoken/author-tokens/ST-2646/full\" rel=\"noreferrer noopener\" target=\"_blank\">published in Science</a>) drew parallels between the “intrinsic time scale” (the autocorrelation function of activity in different parts of the brain) of mouse brains and human brains.</p><p>The work found that <a href=\"https://med.stanford.edu/news/all-news/2025/05/emotions-eye-puff.html\" rel=\"noreferrer noopener\" target=\"_blank\">emotional response triggered by an adverse event was consistent across humans and mice</a>, and by extension, all mammals. (In this case, the adverse event was the unpleasant experience of a puff of air directed at the open eye – Deissesroth said it was tricky to come up with an event that would apply to both humans and mice without being painful).</p><p>“It seems that modulating local temporal dynamics of neural activity may inhibit brain-wide activity coupling and the generation of unified integrated affective (or emotional) states in a way that is beautifully conserved across mouse and human,” Deisseroth said. “Anything that’s conserved over tens of millions of years [of evolution] is, I think, likely to be important.”</p><p>Deisseroth wants to use the precision of optogenetic methods to test the causal role of this principle in behaving mice (mice that can move freely and behave naturally, without needing to keep their head still to apply optical signals). To do this, Deisseroth’s lab at Stanford built a mini-optoencephalograph – a mouse-head-mounted device weighing less than 5g.</p><p>This device is designed to be worn by mice with genetically engineered neurons that include channel rhodopsins (to activate or deactivate neurons with incident light) and calcium indicators (which fluoresce when cells are activated, for detection). Work by other teams at Stanford has led to voltage-based alternatives to calcium indicators, which lack resolution, but are much faster (the example Deisseroth showed was 80 Hz). The head-mounted device can detect both.</p><p>“We’ve been able to shrink down its size and weight so it’s bearable by a mouse running around,” Deisseroth said. “It integrates both imaging and optogenetic control in a steerable, point-and-shoot way. So now we can record what’s going on and come in and modulate it precisely in a freely moving animal.”</p><p>Using Carmine, which activates with red light, means imaging can be done with blue light, so the two don’t interfere; this allows imaging and control to be done simultaneously.</p><p>Work like this could offer insight into neuropsychiatric disorders that could potentially lead to new types of treatments.</p><p><strong>Brain-Optics Interfaces</strong></p><p>Keith Mathieson, professor of neurophotonics at the University of Strathclyde in Scotland, has been working on brain-optics interfaces. Earlier work by other research teams has shown that an array of 100 electrodes is sufficient to record and stimulate neuron activity in non-human primates, with companies including Neuralink commercialising brain-electrical interfaces.</p><p>Deisseroth’s team’s work on understanding the brain has been instrumental in designing new brain interfaces, Mathieson said.</p><p>“This is actually a big problem in neuroprosthesis,” Mathieson said. “You can read out the signals and imagine a brain-machine interface there, but if you want to write information into the brain, then you really need to know how to do that, which neurons to target, [and] what the spatial-temporal activation of those targets should be, so that you can reproduce some useful code to interface with the brain. Optogenetics can obviously play a very important part in that.”</p><p>Mathieson’s group has developed systems for optogenetics research. Recent (unpublished/unreviewed) work with the Institut Pasteur produced an electronic implant designed to be installed on the surface of a mouse brain in a way that allows the mice to move around freely so their behaviour can be studied. This cortical implant is a 10×10 GaN-on-sapphire microLED array, 2x2mm in size, thinned to 150 µm. It’s flip-chip bonded to a flexible PCB as a connector and encapsulated in silicone.</p><p>“One of the big challenges when you’re developing electronics like this to be implanted in the brain is the packaging,” he said. “[In other words,] how you develop a system that’s minimally invasive, but also can withstand what is a pretty harsh environment in the brain in terms of a conductive solution in a warm environment that tends to degrade pretty quickly.”</p><p>The device is well-tolerated by mice, though it requires part of the skull to be removed, Mathieson said, and it’s stable for around a month in vivo. LEDs usually begin to fail within 30 days.</p><p>Other challenges in building a brain implant like this include minimising the current required to get light as deep into the brain as possible to prevent heating. The light distribution from the LEDs needs to be optimised; they are separated from the target by the thickness of the (thinned) wafer, which serves as a barrier to prevent device degradation. The sweet spot with today’s devices is around 10 mW/mm2 of power at 3 mA, which can drive enough brightness to reach cortical layers 2/3.</p><p>Construction and testing of this device have enabled the researchers to experiment with the spatial resolution of the LED array and its brightness. Future devices may use TSV “collimators” on an interposer (to avoid diffusion of the light and hopefully increase spatial resolution), with the hope of eventually building a multi-chip device that could wrap around the brain, Mathieson said.</p><p><strong>Brain Scanning</strong></p><p>Outside of optogenetics, there are other, less invasive ways we can use optics to study the brain.</p><p>Functional MRI scanning (measuring changes in blood flow and oxygenation in the brain with an MRI machine) has been used to detect brain activity over the last 10-20 years. While fMRI can be used to decode what a person is looking at and even reconstruct the semantics of what they are thinking, the machines required are very large and expensive. </p><p>“What if we could all just have a little wearable hat that we can wear day long and have access to this kind of technology?” asked Daniele Faccio, professor of quantum technologies at the University of Glasgow. “I believe photonics is the solution to this. There are several companies out there selling wearable photonic technologies for brain reading.”</p><p>Such technologies use a technique called functional infrared spectroscopy, in which two wavelengths of light are directed through the scalp. Oxygenated and deoxygenated blood absorb or reflect these frequencies, and with careful detection, this technique can be used to determine the absolute concentrations of oxygenated and deoxygenated blood, which can be used to deduce brain activity.</p><p>Light sources and sensors are mounted on a cap that covers the scalp. Sensors detect a banana-shaped trajectory of light through the brain, and complex AI algorithms are used to reconstruct brain activity from these “bananas”. Limitations in spatial resolution and depth can be compensated for in the time domain, Faccio said.</p><p>Faccio’s group has been using a sensor cap commercialised by Kernel, which places multiple optical modules around the skull. There are three sources and six detectors in each module.</p><p>“[The Kernel module] creates a very complicated and interesting network of connections between the various inputs and outputs across the brain,” Faccio said. “The issue, then, or the computational challenge, is to resolve this fairly ill-posed problem […], to disentangle the bananas actually to reconstruct brain activity.”</p><p>In recent, unpublished work, Faccio’s team showed images of different objects to test subjects wearing the Kernel cap. AI was able to tell which image the subject was looking at with 50-60% accuracy. This is a good base for further work, Faccio said.</p><p>Missing pieces include aligning training across multiple people (currently, neither optical nor MRI methods can do this). The aim is to get beyond classification to image reconstruction and language decoding.</p><p>“What we do need is even lighter, and I’d argue, even more wearable devices, especially when it comes down to weight and heat,” Faccio said. “We’re also very interested in trying to see if we can achieve faster sampling rates. At the moment, the Kernel device operates at about three samples per second. We think we can hit a kilo sample per second.”</p><p>While 1000 samples per second temporal resolution isn’t required for measuring oxygenated blood levels (this level takes several seconds to change after a neuron fires), faster sampling can look at other brain signals, some of which have unclear sources; faster responses may be down to neurons contracting as they fire, or changing concentrations of calcium, sodium or potassium which change the refractive index around the neurons.</p><p>Faccio’s group is spinning out a company, Brain Dynamics, to take this work forwards.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><ul class=\"wp-block-list\"><li><em>This is the second in a series of reports from SPRC Symposium. You can read the first report here: <a href=\"https://www.eetimes.com/can-tfln-make-photonic-compute-competitive/\">Can TFLN Make Photonic Compute Competitive</a>?</em></li></ul><p></p> </div>",
            "pub_date": "2025-10-15 18:47:53",
            "link": "https://www.eetimes.com/optics-shine-light-on-brain-functions/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Arizona Benefits from TSMC, Intel as SEMICON West Debuts in Phoenix",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>Having just spent over a week in Arizona for Intel Tech Tour 2025 and SEMICON West 2025, it’s clear that Arizona officials are very excited, with over $210 billion of investments in chip manufacturing facilities in the area since 2020. And on top of that, to see a successful debut for SEMICON West 2025 in Phoenix, where the organizers, SEMI, said they saw significantly increased attendance on the first day, up from 11,400 on day one in San Francisco in 2024, to over 20,000 on day one in Phoenix last week.</p><p>It’s not difficult to see why the numbers are up for the trade show – which covers the whole chip manufacturing ecosystem, from lithography to materials and gases, plus more. For example, in the extensive networking opportunities at the event, I talked to people involved in areas spanning from buildings and construction and facilities management, to bonding adhesives.</p><p>It’s impressive to think the semiconductor industry, that produces the tiniest of features on silicon in small packages, involves so many materials, equipment, buildings and people. No wonder countries are finding the whole rebalancing of the supply chain that is happening in the semiconductor world right now quite frustrating and complex. Walking the show floor, talking to people, it’s apparent that no country can become completely self-sufficient.</p>\n<p>However, the ambition is there to bring more capabilities onshore, and that must please both the politicians as well as the population who voted on the basis of bringing more jobs back to the nation. For example, Rose Castanares, president of TSMC Arizona, said in her talk on the Monday of the show, “Our own technicians are 90% native from Arizona.”</p>\n\n<p>She highlighted just how good Arizona was, from enticing TSMC to locate there, to the ecosystem and pipeline. On this she said, “Our largest customer asked us to be here [in the U.S.]. so we explored the states. We also needed talent pipelines. And the clencher was that Arizona really wanted us to be here. Arizona chose semiconductors and Arizona chose us.”</p>\n<p>While this soundbite would please the Arizona officials attending, more seriously she added that having everyone in the locality (suppliers and talent), is a force multiplier. Speaking on the same panel David McCann, senior VP and chief of staff at Amkor Technology, who just broke ground at its semiconductor advanced packaging and test campus in Arizona as part of a total $7 billion investment, said, “Our customers see the availability of the whole supply chain here as helpful to them.” He added, “We expect to have around 3,000 staff here in Arizona once fully built out, with around a third being graduates, a third being technician level, and a third being school leavers.”</p><p>And the fact that you did have so many people in the ecosystem in Arizona already serving the big headline manufacturing capacity being added, as well as many others that don’t get mentioned (watch out for a future story on some of the players we spoke to at SEMICON West), was a key contributor to the excitement and buzz in Phoenix at SEMICON West 2025.</p><p>We caught up with Ajit Manocha, president and CEO of SEMI, the trade organization that organizes SEMICON West and other SEMICON trade shows around the world, to get his view on the success of the show, as well as other key topics challenging the industry – including talent pipeline and more.</p><p>Watch the interview here:</p><figure class=\"wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"></figure><p>We’ll have more stories and interviews from SEMICON West 2025 soon.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><p><em>See also</em>:</p><p><em><a href=\"https://www.eetimes.com/intels-confidence-shows-as-it-readies-new-processors-on-18a/\" rel=\"noreferrer noopener\" target=\"_blank\">Intel’s Confidence Shows As It Readies New Processors on 18A</a></em></p><p><em><a href=\"https://www.eetimes.com/nvidia-amd-ramp-at-tsmc-arizona-as-u-s-tariffs-loom/\" rel=\"noreferrer noopener\" target=\"_blank\">Nvidia, AMD Ramp at TSMC Arizona as U.S. Tariffs Loom </a></em></p><p><em><a href=\"https://www.eetimes.com/semicon-india-2025-pm-modi-maps-vision-to-target-global-chip-industry/\">Semicon India 2025: PM Modi Maps Vision To Target Global Chip Industry</a></em></p><p><em><a href=\"https://www.eetimes.com/advanced-chip-packaging-tools-are-new-battleground-in-india/\" rel=\"noreferrer noopener\" target=\"_blank\">Advanced Chip Packaging Tools Are New Battleground in India</a></em></p><p><em><a href=\"https://www.eetimes.com/semicon-india-2024-pm-modi-pulls-out-all-the-stops-for-local-chip-growth/\" rel=\"noreferrer noopener\" target=\"_blank\">Semicon India 2024: PM Modi Pulls Out All the Stops for Local Chip Growth</a></em></p> </div>",
            "pub_date": "2025-10-15 18:47:44",
            "link": "https://www.eetimes.com/arizona-benefits-from-tsmc-intel-as-semicon-west-debuts-in-phoenix/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "Diraq CEO Aims for Company’s First Quantum Computer by 2029",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>Three-year-old <a href=\"https://diraq.com/\" rel=\"noreferrer noopener\" target=\"_blank\">Diraq</a> aims to make its first quantum computers with the help of European R&amp;D organization imec by 2029, company CEO Andrew Dzurak told EE Times in an exclusive interview. Diraq won’t be the first to make a “useful” quantum computer, but the startup will make the most affordable ones, he said. </p><p>Diraq will drive down production costs through massive integration of qubits and classical silicon on a single chip, said Dzurak, who has been working on quantum tech for more than 25 years as a professor at the University of New South Wales in Australia. </p><p>In September, European R&amp;D organization <a href=\"https://www.imec-int.com/en\" rel=\"noreferrer noopener\" target=\"_blank\">imec</a> and Diraq produced silicon quantum-dot qubits that consistently showed error rates surpassing the values needed for quantum error correction. The results were reported in <em>Nature</em>.  </p>\n<p>“For at least the next three to five years at the minimum, the actual volume of chips that we require for our technology is really quite small because Diraq technology allows ultimately millions of qubits on a single chip,” Dzurak told EE Times. “We’re working on all of our processing on single chips. We actually can do low-volume fabrication of devices at imec for quite some time.” </p>\n\n<p>The first half of 2029 is Diraq’s target delivery date for its first quantum computing system that will combine quantum chips and classical chips in a single cryogenic unit.   </p>\n<p>Diraq qubits operate “happily” at 1 kelvin, a roughly 100-fold higher temperature compared with typical qubits that only operate close to absolute zero, Dzurak said.  </p><p>He notes the higher temperature operation of Diraq qubits aids the cooling power of cryogenic systems by a factor of 1 million, allowing Diraq to package classical chips adjacent to quantum chips.  </p><p>“It’s a really crucial aspect of our technology,” Dzurak said.  He declined to give details on the interconnect material between the qubits and classical chips. </p><p>“What you want is to have a good electrical connection but a lousy thermal connection,” he said. “You don’t want the classical chip heating up the quantum chip. Semiconducting materials are one example of what could be used.” </p><p>Diraq devices consistently achieved over 99% fidelity in operations involving two qubits made with imec’s spin-qubit tech platform. It’s a crucial step in Diraq’s roadmap toward “utility”, the point where a quantum computer is worth more than it costs. </p><p>Imec called the development with Diraq an important first, moving quantum chips from research labs to production-scale facilities. </p><p>“For the first time, silicon MOS based quantum-dot spin-qubit devices realized with industrial manufacturing techniques perform as well as academic hero devices,” Kristiaan De Greve, program director for quantum computing at imec, said in a statement. “This shows that imec’s 300 mm process flow for MOS based quantum-dot structures enables a low-noise qubit environment, resulting in high fidelity values for a set of critical qubit operations. The methods used and insights gained from it also show us that there is further room for fidelity improvement.” </p><h3 class=\"wp-block-heading\">GlobalFoundries partnership </h3><p>Diraq will rely on imec as its chip supplier for the near term, but the company has been working with GlobalFoundries as a potential second source.  </p><p>“We’ve currently got chips we are waiting to get back from them in the not too distant future, so GF is an important foundry partner,” Dzurak said. “One of the reasons we work with GF is their technology called 22FDX that is known to work very well at cryogenic temperatures.” </p><h3 class=\"wp-block-heading\">Integration and packaging are what Dzurak calls “crucial.” </h3><p>“We have an R&amp;D packaging partner, and we are in discussion with commercial packaging partners,” Dzurak said. “For the next three to five years because the volume is low, R&amp;D packaging is sufficient for our purposes for quite some time. Because we are packaging together chips that need cryogenic temperatures, we need interconnects that operate well at those temperatures, and so there’s a limited number of organizations that are developing that technology. Imec is one of them.” </p><h3 class=\"wp-block-heading\">Customers </h3><p>Dzurak is talking to potential customers he describes primarily as supercomputing facilities and government laboratories in the U.S., Europe, Australia and Japan with sales starting around 2031 and taking off around 2033.  </p><p>“We don’t say we’re going to have the first useful quantum computer,” Dzurak notes. “There will be a few around by the time we bring out our first product in 2029. We believe that we will outperform our competitors on cost.  That’s really the differentiator: driving the cost down with massive integration on the chip.” </p><p>A quantum computing facility based on Diraq hardware would likely be housed in a space measured in tens of square feet, Dzurak added.  </p><h3 class=\"wp-block-heading\">Raising capital </h3><p>Dzurak says he has been spending more time in the U.S. to raise capital. </p><p>In September, PsiQuantum, led by Dzurak’s schoolmate Pete Shadbolt, raised $1 billion in funding to build the world’s first commercially useful, fault-tolerant quantum computers. Investors included BlackRock, Temasek and Baillie Gifford in the fundraising round valuing the quantum computing company at $7 billion. New investors included NVentures, Nvidia’s venture capital arm. </p><p>Private capital, venture capital and sovereign wealth funds are also looking to invest in quantum startups, Dzurak says. </p><p>“Some of the capital raises have been eye-watering,” he notes. “For Diraq, that’s good news. We’ve raised around $60 million. Capital is located here in the U.S., and you know I’m having all of those conversations continually. That’s a big part of the life of a CEO.”</p> </div>",
            "pub_date": "2025-10-15 18:47:32",
            "link": "https://www.eetimes.com/diraq-ceo-aims-for-companys-first-quantum-computer-by-2029/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        },
        {
            "title": "ASIC Wars and Channel Density Redefine Oscilloscope Mid-Range",
            "description": "<div class=\"articleBody\">\n\n\t\t\t\t\t//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?&gt;\n\t\t\t\t\t<!-- EET_Top_Leaderboard -->\n<p>The $3.74 billion <a href=\"https://www.mordorintelligence.com/industry-reports/oscilloscope-market\" rel=\"noreferrer noopener\" target=\"_blank\">global oscilloscope market</a>, a cornerstone of electronics development, is currently undergoing a period of significant technological transformation, driven primarily by the strategic deployment of proprietary silicon and a surging demand for higher channel density.</p><p>This competitive battle is forcing traditional incumbents—led by Tektronix, Keysight Technologies, Rohde &amp; Schwarz (R&amp;S), and Teledyne LeCroy—to redefine performance in the crucial mid-range segment, where devices must now offer both superior measurement precision and system-level debugging capabilities.</p><p>This technological pressure comes from two directions: the relentless need for higher-frequency tools to support 5G/6G and high-speed data validation, and the aggressive push by Asian challengers, such as Rigol and Siglent, which have successfully “commoditized features previously exclusive to high-end equipment.”</p>\n<h3 class=\"wp-block-heading\">Market built on precision and heritage</h3><p>The history of the oscilloscope market is rooted in legacy brands. The modern landscape, for instance, still reflects the lineage of Hewlett-Packard (HP), whose test and measurement division eventually transformed into <a href=\"https://www.keysight.com/us/en/home.html\">Keysight Technologies</a>.</p>\n\n<p>In contrast, <a href=\"https://www.rohde-schwarz.com/\" rel=\"noreferrer noopener\" target=\"_blank\">Rohde &amp; Schwarz</a>, a privately-owned company older than 90 years, focuses heavily on R&amp;D, typically investing 15% to 20% of its revenue into technology development.</p>\n<p>Historically, 8-bit analog-to-digital converters (ADCs) were the industry standard. However, the market has undergone a “resolution revolution,” migrating toward 10-bit and 12-bit architectures to improve vertical resolution for complex tasks like power integrity analysis.</p><p>A second inflection point has been the expansion of channel count. The complexity of modern systems—such as three-phase power analysis or motor drive testing—has established 6- and 8-channel instruments as the new standard in the mid-range segment. The requirement is fueling a new competitive sub-class, necessitating instruments that can analyze more analog signals simultaneously.</p><h3 class=\"wp-block-heading\">R&amp;S MXO 3 challenges the 3000 class</h3><p>Rohde &amp; Schwarz is challenging the established norms in the high-volume “3000 class”—a segment typically defined by cost optimization and a maximum bandwidth of 1 GHz. The company is launching the new R&amp;S MXO 3 Series today, as the replacement for its predecessor, the RTM 3000. The MXO 3 is a direct attempt to bring the performance of higher-end platforms (such as the MXO 4 and MXO 5, introduced in 2022 and 2024, respectively) into a more economical package.</p><p>The most bold competitive move is the pricing of the 8-channel configuration. According to Ernst Flemming, director of product management Oscilloscopes at R&amp;S, “historically, the entry pricing for an 8-channel oscilloscope in the European market starts typically at around €25,000 [to] €28,000, if not more.” The new 8-channel base model (100 MHz) of the MXO 3 is being offered starting at €12,500.</p><p>The 4-channel base model starts at €5,350. Noha Ibrahim, product manager for the MXO 3, characterized this pricing as enabling an investment that “sometimes you don’t need to think twice to make.”</p><h3 class=\"wp-block-heading\">Performance through proprietary silicon</h3><p>The competitiveness of the MXO 3 derives from its proprietary, in-house built MXO-EP ASIC. This custom chip, the same featured in the higher-segment MXO 4 and MXO 5, provides the foundation for the scope’s claimed advantages in speed and precision.</p><figure class=\"wp-block-image size-full is-resized\"><img alt=\"\" class=\"wp-image-1444124\" data-recalc-dims=\"1\" decoding=\"async\" height=\"378\" src=\"https://www.eetimes.com/wp-content/uploads/Screenshot-2025-10-12-at-12.42.15.png?resize=640%2C378\" style=\"width:750px\" width=\"640\"/><figcaption class=\"wp-element-caption\"><em>Source: R&amp;S</em></figcaption></figure><p>In terms of speed, the MXO 3 achieves an impressive maximum acquisition rate of 4.5 million waveforms per second (wfms/s), which Flemming says is “at least three times or four times faster than any other scope in the segment.” This speed allows the MXO 3 to capture up to 99% of real-time signal activity. For comparison, R&amp;S notes that the Tektronix MDO 3 captures only 1.4% of the signal at a 20-nanoseconds/division setting.</p><p>The ASIC also enables hardware acceleration for complex functions like spectrum analysis, achieving 50,000 FFTs per second, a thousand times faster than its RTM 3000 predecessor. This performance also translates to precision. The MXO 3 offers 12-bit hardware vertical resolution at all sample rates, capable of reaching up to 18 bits in its High Definition mode.</p><p>Crucially, the hardware architecture maintains accuracy when performing complex calculations. According to Flemming, unlike some competitors in the 3000 class that may take a “reduced set of samples” when measuring math or FFTs to retain speed, the MXO 3’s ASIC ensures that “all acquired samples” are considered for math and spectrum analysis. Failing to consider all acquired samples can result in a “false representation of these measurements.”</p><h3 class=\"wp-block-heading\">Long-term platform commitment</h3><p>Beyond raw specifications, R&amp;S focuses on delivering long-term value through its platform approach. The MXO 3 features a compact design, weighing only 4 kg, and includes an 11.6-inch full HD screen. It also includes a built-in VESA mount, allowing it to mount and lift on a test bench.</p><p>Significantly, the instrument’s bandwidth is a software option, allowing users to “upgrade later on by a software license” without needing to send the instrument in for service.</p><p>R&amp;S emphasizes that its platform provides continuous development. The company releases three to four firmware upgrades for the whole platform per year, which are made available to users “for free”. Ms. Ibrahim highlighted this as a key differentiator, noting that some competitors may require users to “pay some maintenance fees to get these free upgrades.” This commitment, Flemming pointed out, reflects R&amp;S’s “strategic long-term commitment to the oscilloscope market.”</p><h3 class=\"wp-block-heading\">Future developments</h3><p>Looking ahead, the technological arms race defined by proprietary silicon will continue to drive rapid market evolution. Established incumbents will pivot further toward advanced software ecosystems and platform-based services, with the business model evolving toward subscription services as a key source of differentiation. </p><p>Simultaneously, the leading Chinese challengers will persist in their upward migration, with expectations that their next-generation products will standardize 12-bit ADCs and push bandwidths into the competitive 2-8 GHz range. </p><p>The intense collision of forces, centered on the lucrative mid-range segment, is forecast to generate sustained downward pressure on prices for comparable hardware performance, ensuring that the dynamic is “overwhelmingly positive” for the end-user with increasingly capable measurement tools at more accessible price points.</p><hr class=\"wp-block-separator has-alpha-channel-opacity\"/><p></p> </div>",
            "pub_date": "2025-10-15 18:47:29",
            "link": "https://www.eetimes.com/asic-wars-and-channel-density-redefine-oscilloscope-mid-range/",
            "source": "investors",
            "kind": 1,
            "language": "en-US"
        }
    ]
}