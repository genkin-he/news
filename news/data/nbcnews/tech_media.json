{
    "data": [
        {
            "title": "The AI child exploitation crisis is here",
            "description": "<div>The rapid advancement of artificial intelligence has made it easier than ever for bad actors to create child sexual abuse material, leaving prosecutors and lawmakers struggling to keep up. Despite efforts by tech companies, law enforcement and activists, offenders consistently exploit system loopholes, open-source AI models and ready-made sexual exploitation platforms to generate imagery of both identifiable and nonexistent children, according to experts and law enforcement officials who spoke with NBC News. Between January and September of 2025, NCMEC’s CyberTipline — the official online sexual exploitation tip line in the U.S. — received over a million reports related to generative AI, according to Fallon McNulty, the executive director of the center’s exploited children division. “We often see bad actors at the forefront of leaning into those types of advancements in order to sexually exploit children online,” McNulty said. “The almost indistinguishable nature of the content that is being generated makes it extremely difficult for victim identification efforts.” Law enforcement officials have found that child sexual abuse material (CSAM) created with generative AI can take on many forms. Sometimes people photograph children in public settings or use already-public photographs, and then use AI systems to turn them into CSAM. Other times, people create entirely new sexually explicit material that involves no real child or recognizable face and is completely AI-generated. The material is becoming more realistic and harder to differentiate from real images, posing new issues for prosecutors and law enforcement. Michael Prado, the deputy assistant director of Homeland Security Investigations’ Cyber Crimes Center (C3), said that in the first six months of 2025 alone, reports of child exploitation and generative AI increased by over 600% compared to 2023 and 2024 combined. “What has, quite frankly, taken us by surprise is how rapidly it has spread,” Prado said. Now, it’s not uncommon to find AI-generated CSAM mixed in with troves of “traditional” CSAM featuring real children, according to Prado. “Collectors of this type of material, sometimes they don’t really differentiate. They’re just looking to increase their collections,” he said. “They’re looking to satisfy their perverse sexual interest in children and will use any means to accomplish that.” Though widely available generative AI is a relatively recent phenomenon, the issue is already appearing in dozens of CSAM prosecutions across the U.S. But the number of cases is a tiny fraction of the number of reports made about CSAM created with AI. NBC News identified 36 state and federal criminal court cases brought within the last three years related to or mentioning AI-generated CSAM, spanning 22 states. In several cases NBC News reviewed, defendants were allegedly caught with thousands of AI-generated CSAM images. While over half of the cases NBC News reviewed are still active, all closed cases have resulted in guilty verdicts. The cases appear to represent only a small part of the problem, but Prado said it’s hard for reports to match prosecutable cases. “The fact of the matter is, half a million reports just in the first six months of the calendar year, that’s not going to result in 500,000 investigations, or certainly not 500,000 arrests,” Prado said. “Let’s say multiple reports pertain to one individual, so it’s hard to track just exactly how prevalent it is amongst the general population.” How people are creating more AI-generated CSAM Creators of AI-generated CSAM use a constellation of apps and platforms to generate abusive material, outpacing enforcement efforts. While public attention has been focused on companies racing to create more powerful models, many smaller companies and websites have sprung up that offer similar features. A review of the legal cases highlights how these smaller platforms can fly under the radar of law enforcement.NBC News found five criminal cases that involved defendants allegedly using small AI platforms like Bashable.art, undress.ai and Faceswapper.AI — which seemingly have less robust platform moderation or were expressly built for making explicit content — to create nude imagery of children. None of the platforms were mentioned as defendants in the cases, and none responded to requests for comment. An Idaho man allegedly generated over a thousand images of “Apparent Child Pornography” using Bashable.art, according to a federal complaint. Investigators had found that the man was a registered sex offender and had previously been arrested 21 years earlier for sexually abusing a 13-year-old girl. Using the platform’s “unrestricted mode,” he allegedly prompted the program to create nude images of children under 13, including requests for images of a “large group of girls who are age 11 years old taking a shower” and a “10 year old little nude girl.” The case is still active. Bashable.art restricts explicit content but gives registered users access to its “unrestricted” mode, which “removes any filters on prompts and models, and allows viewing other shared unrestricted generations,” according to its website. The platform’s website also says it monitors content created in unrestricted mode and may suspend users and report them to NCMEC. While the defendant who allegedly used Bashable.art is not accused of generating images of known victims, the defendants in cases involving undress.ai and Faceswapper.AI are.  Platforms like undress.ai are part of a network of “nudify” generators designed solely to create explicit deepfakes using images of real people. In a federal criminal case, a defendant allegedly used the website DeepSukebe, described as an “AI-Leveraged Nudifier” that generates deepfake nude images of women from a clothed photograph, according to a motion to suppress evidence. DeepSukebe did not respond to a request for comment. According to a Justice Department press release, the man used AI to “digitally alter clothed images of minors making them sexually explicit,” including images of “from a school dance and a photo commemorating the first day of school.” The man, who had also possessed videos and images of children that he secretly recorded, was sentenced to 40 years in prison.Open-source AI models present particularly difficult issues in the effort to fight CSAM, allowing anyone to download, copy, modify and operate them. Stability AI, a company behind the widely used open-source image model Stable Diffusion, was allegedly used by a Wisconsin man to create CSAM, according to a federal court brief. Law enforcement alleged that the man had used Stable Diffusion as well as “special add-ons created by other Stable Diffusion users that specialized in producing genitalia,” which allowed him to “generate photo-realistic images of minors,” according to the brief in the ongoing case. A lawyer representing the man declined to comment. In response to a request for comment on the case, a Stability AI spokesperson said to NBC News that it “is deeply committed to preventing the misuse of AI and has always prohibited the use of our image models and tools for unlawful activity, including all attempts to edit or create CSAM.”Riana Pfefferkorn, a policy fellow at the Stanford University’s Institute for Human-Centered Artificial Intelligence, said the use of open-source platforms has made it difficult for authorities to crack down on AI-generated CSAM. “When you have an identifiable entity that has a U.S. presence and has a corporate office, you can pin them down,” Pfefferkorn said. But open-source models like Stable Diffusion 1.5, she said, can “float around out there and can keep being trained up locally.” Larger companies face an uphill battle given their number of users. Major tech companies have submitted thousands of reports of users potentially using their services to create CSAM, according to a report from NCMEC’s CyberTipline — a reporting mechanism where electronic service providers can flag potential CSAM to the center. Platforms are legally mandated to report potential CSAM. At least one major player appears to be exacerbating the issue. In January, Elon Musk’s X faced global backlash after an update to its AI tool Grok allowed users to create and post nonconsensual deepfakes. The U.K.-based Internet Watch Foundation told NBC News that dark web users were sharing “criminal imagery” of minor girls allegedly created with Grok. Musk later responded in an X post by saying that he was “not aware of any naked underage images generated by Grok,” and that Grok “will refuse to produce anything illegal, as the operating principle for Grok is to obey the laws of any given country or state.” Real versus AI  Increasingly real-looking AI-generated content has introduced an issue for CSAM investigators: differentiating between imagery of real people versus fully virtual material. “The advances in generative AI year over year have made these images become extremely photorealistic,” McNulty said. “I think that is certainly a fear, that law enforcement may be spending days looking for someone who doesn’t exist.” The distinction can drastically change how a case is prosecuted.In one ongoing federal case, a man who is accused of using AI to generate explicit imagery of children with no known victim has pending charges related to federal obscenity laws, rather than federal CSAM laws, since the allegedly generated children do not physically exist. Pfefferkorn said she has reviewed over 60 state and federal AI-related CSAM cases and found that obscenity charges have largely been used in cases that do not feature real children. However, she said that most people found to possess AI-generated CSAM possess real CSAM as well. “You can nail them to the wall for that.” When real victims have been involved, though, prosecutors have argued that charges shouldn’t be adjusted because of the use of AI. In an Arkansas case, a defendant tried to dismiss charges against him by alleging that the photos of children that were put onto the bodies of adults engaging in sexual activity were computer-generated. The prosecuting attorney for the case said in response that altered images, “would still run afoul of Arkansas’ law prohibiting the production and promotion of sexually explicit conduct involving a child,” according to a brief in the case. The man was found guilty. For NCMEC, the source of the image is secondary to its impact. “We at NCMEC consider all of those images the same. We still consider them to be a harm, whether they’re fully AI-generated content or whether they are taken by an offender with access to the child,” said Kathryn Rifenbark, the director of the CyberTipline at NCMEC. “To the victim, the harm is going to be the same,” she added. “They’re still going to have that impact of that nude picture, whether AI or not, distributed of them online. And since it’s hard for professionals to tell the difference, it’s certainly hard for members of the public to be able to tell the difference, which is why victims are going to be impacted equally.” The legal landscape While broader AI regulation remains politically divisive, lawmakers across the aisle are attempting to address AI-generated CSAM, though approaches have varied by state. According to the watchdog group Public Citizen, 45 states have enacted laws pertaining to intimate AI deepfakes, many of which focus specifically on minors. A deepfake is an AI-generate image, video or audio recording depicting a real person, typically for malicious purposes, and is difficult to distinguish from the real thing. Missouri and New Mexico haven’t passed any such laws yet, and several other states have pending bills. “My sense is there’s a general interest in passing this type of legislation,” said Ilana Beller, an organizing manager at Public Citizen who created the tracker. “And in states where it hasn’t happened yet, it is not a function of a lack of political will or interest so much as a function of logistics and broader politics.” She noted that some state laws are specifically tailored toward minors, others toward nonconsensual deepfakes generally, and others outline specific requirements for AI companies. Beller said states have been proactive about passing legislation on AI-generated CSAM, but that targeting AI companies can be a “trickier area to legislate in” because it can mean that the smaller, unregulated open-source AI platforms are let off the hook. Four states have passed or introduced legislation that specifically targets platforms, but all already have pre-existing legislation that covers AI deepfakes generally. NBC News identified five cases pertaining to AI-generated CSAM in Missouri, Alaska and Ohio that have no specific legal framework to combat the issue. Still, two of the cases, both of which involved known victims, resulted in guilty verdicts related to the possession of child pornography. The other cases are ongoing. Federal efforts to address AI-generated CSAM are continuing. In May 2025, President Donald Trump signed the TAKE IT DOWN Act, which made the creation of nonconsensual deepfakes a federal crime and requires platforms to take down imagery 48 hours after it is reported. On Dec. 16, the Enhancing Necessary Federal Offenses Regarding Child Exploitation (ENFORCE) Act passed the Senate unanimously. It would allow for the creators and distributors of AI-generated CSAM to be prosecuted to the same degree as those who create other forms of CSAM. The legislation is now waiting to be reviewed by the House of Representatives. Beller said the federal law is a step in the right direction, but that state legislation is crucial for civil cases, and for handling a rising caseload. Beller pointed to a New Hampshire law that both prohibits “certain uses of deepfakes” and creates “a private claim of action.” “It is really important that state and local prosecutors are empowered to address these issues in the courts as well,” Beller said. “The number of cases related to nonconsensual, intimate deepfakes would just be too much for only federal prosecutors. They would only be able to get to a small fraction of the total number of cases.” Prado said that if the technology continues to evolve at the pace that it has, it will continue to be difficult for lawmakers to find the right approach. “What I see is the states and the federal government really taking action in response to this problem,” he said. “But as we are well aware, the state legislatures and Congress, there’s often a lag between laws, because it does take time to formulate laws and get them on the books and get people trained to enforce them. It’s hard to keep up with the rapidly evolving nature of technology and generative AI.”</div>",
            "link": "https://www.nbcnews.com/tech/security/ai-child-exploitation-crisis-rcna259409",
            "pub_date": "2026-02-28 20:01:38",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Trump bans Anthropic from government use",
            "description": "<div>After months of increasingly heated rhetoric between the Defense Department and leading AI company Anthropic over the military’s use of its systems, President Donald Trump announced Friday afternoon that he was banning federal agencies from using Anthropic’s services.  “I am directing EVERY Federal Agency in the United States Government to IMMEDIATELY CEASE all use of Anthropic’s technology. We don’t need it, we don’t want it, and will not do business with them again!” Trump wrote in a post on Truth Social. Anthropic did not immediately reply to a request for comment. The company, led by CEO Dario Amodei, has made clear in months of contract negotiations with the Pentagon that it will not allow its AI systems to be harnessed for domestic surveillance or direct use in lethal autonomous weapons.  The Pentagon has maintained that it must be allowed to employ its AI systems for “any lawful use,” which may violate Anthropic’s red lines. “I believe deeply in the existential importance of using AI to defend the United States and other democracies,” Amodei wrote in a statement Thursday night, but “using these systems for mass domestic surveillance is incompatible with democratic values.” Amodei added that “today, frontier AI systems are simply not reliable enough to power fully autonomous weapons.” In a series of tweets late Thursday night, Undersecretary of Defense Emil Michael wrote on X that Amodei “is a liar and has a God-complex. He wants nothing more than to try to personally control the US Military and is ok putting our nation’s safety at risk.” Earlier Thursday, Pentagon Chief Spokesperson Sean Parnell wrote on X that the Pentagon’s desire to use Anthropic’s model for all lawful purposes “is a simple, common-sense request that will prevent Anthropic from jeopardizing critical military operations.” Anthropic currently has a contract worth up to $200 million with the Pentagon to “advance responsible AI in defense operations” and works with data analytics company Palantir to provide its AI services on classified defense and intelligence networks. Throughout Friday, a growing chorus of lawmakers had called on the parties to deescalate their feud and come to an amicable solution, contrasting with the relative silence from Anthropic and the Pentagon in the hours before the deadline. In a letter to Defense Secretary Pete Hegseth made public Friday afternoon, Sens. Ed Markey, D-Mass., and Chris Van Hollen, D-Md., said the Pentagon’s “threats to punish an American AI company for refusing to surrender basic safeguards on the use of its AI model represent a chilling abuse of government power.” Rep. George Whitesides, D-Ca., told Hegseth he was “concerned that your threats to compel changes to safety policies on an accelerated timeline could push the Department toward broader deployment without sufficient guardrails” in a letter released Friday morning. Unlike many major defense technologies, today’s leading AI systems have been developed primarily in the private sector, by companies like Anthropic, OpenAI, and Google. The increasing capabilities of these systems have forced the Pentagon to bargain with Anthropic over its usage policies or opt for a less proven services. Until this week, Anthropic was the only leading AI company that had been cleared to offer services on classified networks. In a memo sent to employees Thursday evening and viewed by NBC News, OpenAI CEO Sam Altman said that his company would largely follow Anthropic’s approach if it were in the same position with the Pentagon. “We have long believed that AI should not be used for mass surveillance or autonomous lethal weapons, and that humans should remain in the loop for high-stakes automated decisions. These are our main red lines,” he wrote. Altman added that “this is no longer just an issue between Anthropic and the DoW; this is an issue for the whole industry and it is important to clarify our stance.” It’s unclear how other leading AI companies would respond. Google, Meta and xAI did not respond to a request for comment.</div>",
            "link": "https://www.nbcnews.com/tech/tech-news/trump-bans-anthropic-government-use-rcna261055",
            "pub_date": "2026-02-28 05:35:52",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Ads funded by AI industry are flooding the 2026 election. They're about everything except AI.",
            "description": "<div>Competing super PACs backed by the AI industry, flush with tens of millions of dollars, are already pouring money into the 2026 midterms, starting with the year’s first primaries in Texas and North Carolina.  There’s just one thing missing from their ads so far: any reference to artificial intelligence. The groups are seeking to shape how AI models and companies are regulated nationwide, a debate that big players in AI see as existential for the future of the industry, the United States and the world. But instead of the actual policy reason they are taking sides in these primaries, the groups are leaning into red meat or progressive messaging on other hot-button issues. It’s a tactic also used by groups in other areas. But the early AI-backed spending on ads about Immigration and Customs Enforcement, President Donald Trump, health care and more is especially notable because of the dramatic scale of change AI titans expect their product to bring to the American workforce and society. The forces behind the super PACs are also funding large nonprofits that could spend big to shift public opinion on AI moving forward. For now, though, their big issue is not figuring in their big political campaigns. So far, two rival umbrella organizations have dominated the AI spending in congressional races. Leading the Future — which has received significant funding from OpenAI co-founder Greg Brockman and his wife, Anna Brockman, as well as venture capitalists Marc Andreessen and Benjamin Horowitz — is one major super PAC pushing a national framework for AI and criticizing the prospect of different state regulations governing the industry. Leading the Future had $39 million banked away at the end of last year and is wading into races via a pair of connected groups, one associated with each party: Think Big, which backs Democrats, and American Mission, which supports Republicans.  Public First, another super PAC, is seeking to counter Leading the Future and its network. The group has received at least $20 million from the AI company Anthropic and has called for more significant regulation on AI.  It also has two affiliated super PACs: Jobs and Democracy PAC backing Democrats and Defending Our Values backing Republicans. Brad Carson, the former congressman and Defense Department official who helps lead Public First, told NBC News in a statement that while the public recognizes the importance of the issue, “we know AI isn’t the first thing on every voter’s mind when they go to the polls.”  “They’re worried about cost of living, about corruption, about whether the economy is working for regular people or just for tech billionaires. We believe those concerns are inseparable from AI,” added Carson, who served two terms in the House as a Democrat from Oklahoma. “We support candidates who understand what’s coming and who will fight for working families as these technologies roll out. AI is an issue right now, and you want leaders in place who’ve done the thinking on what the impact is going to look like.” A slice of New York City has emerged as the earliest big battleground for the two sides. The Democratic House primary aimed at succeeding retiring Rep. Jerry Nadler is extremely crowded, but the AI groups are focused on one candidate: state legislator Alex Bores, a proponent of AI safety regulation in New York and a former data scientist at Palantir Technologies who says he quit that job over his frustration with the company’s work with Immigration and Customs Enforcement.  Think Big, one of the Leading the Future affiliates, has spent more than $1.5 million attacking Bores, including by hammering him for Palantir’s work for ICE — even though Palantir co-founder Joe Lonsdale has supported the group and Bores has said he left the company due to his opposition to that work. “It’s black and white: Alex Bores’ tech company works for ICE,” a narrator says in a new Think Big digital ad in the race.  Jobs and Democracy PAC, the Public First affiliate, framed those attacks as cynical and profit-motivated in its own ads. “Right-wing billionaires think they can buy this congressional seat, the same ones who bankroll hate, fund lies and prop up ICE raids on our community. Their target: Assemblyman Alex Bores, because he’s the only one who stood up to them before,” says the narrator in a recent Jobs and Democracy PAC ad.  Think Big is also spending more than a million dollars each in support of two former Illinois members of Congress, Jesse Jackson Jr. and Melissa Bean, in their comeback bids. Ads in both those races tout Jackson and Bean’s accomplishments in Congress, including voting for the Affordable Care Act — long before the AI debate entered the halls of Congress. Both Illinois Democrats are scrapping in competitive primaries for open, deep blue, Chicagoland seats where the March primary will serve as the de-facto general election.  Then there’s Rep. Valerie Foushee’s bid for re-election in North Carolina’s Research Triangle, where Durham County Commissioner Nida Allam is attacking the incumbent from her left. Foushee is a member of the Bipartisan House Task Force on Artificial Intelligence, and there’s a major AI-related fight brewing in the district over a push for a new data center.  The incumbent has called for regulations around data centers to mitigate any energy or environmental impacts in the past, and she addressed the data center debate in a social media video this week, saying that while she doesn’t personally support the new data center, she trusts local officials to “make the right choice.” Allam has blasted the proposal, calling for a moratorium on data centers and attacking Foushee because an AI-linked group is spending on her behalf. Indeed, Public First is the only major AI group spending in this district, looking to boost Foushee with more than $1.6 million flooding into the race in its final weeks.  But while the data center debate is playing out in local politics, it’s absent from the pro-Foushee ad, which instead frames the incumbent as a progressive fighter on issues like immigration raids and holding Trump accountable. The AI spending has prompted a small counter from Justice Democrats, a progressive group supporting Allam, which dropped a new digital ad this week criticizing the incumbent for receiving support from the AI industry. The two sides are also boosting preferred candidates in other, less prominent races.  Lead the Future’s Republican group is spending $500,000 each on generic, biographical ads boosting Republican candidates Laurie Buckhout in a North Carolina swing district and Chris Gober — who was a legal adviser to Elon Musk’s political group in 2024 — in an open, safely Republican Texas district. The Public First network is also spending to boost two Republicans who are heavy favorites to win their primary elections in Texas without wading into AI in their ads: Army veteran Alex Mealer and Air Force veteran Carlos De La Cruz. The decision to de-emphasize the central issue motivating these groups is not a new strategy. Groups with political aims on Israel, cryptocurrency, the environment and more have long chosen to wage their political battles using other issues more salient to a primary or general electorate, separating the political goals of winning elections from the policy goals they hope the winner will support in office. Jesse Hunt, a spokesman with Leading the Future, told NBC News that the group is supporting and assessing candidates “who we believe are the most pro-innovation candidates when it comes to AI,” adding that it uses both a candidate questionnaire, as well as a review of public statements or relevant records, to help guide them. He added that the debate over AI is “still in its infancy” and he expects it to evolve over this and future election cycles. “The administration has been clear where they stand on this issue, but other members of Congress are starting to formulate their positions, what they want to see from these companies, how it stands to benefit their constituents, their states, and how it stands to benefit the U.S. economy,” he said. Hunt said the public conversation around AI policy “will see it mature over time as people become more knowledgeable.” He added: “Through that process, you’re going to see a greater conversation about the likely ways it will enhance a lot of people’s lives.” </div>",
            "link": "https://www.nbcnews.com/politics/2026-election/ads-ai-industry-are-flooding-2026-election-artificial-intelligence-rcna260782",
            "pub_date": "2026-02-28 01:40:49",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "AI chip made for Chinese company draws scrutiny over potential U.S. export violations",
            "description": "<div>Key components produced by a leading Taiwanese chipmaker were found in a powerful AI chip from a Chinese company, according to an initial report from a semiconductor research firm, which experts say could violate U.S. export controls. U.S. officials have been briefed on the research, according to a source familiar with the issue who requested anonymity to discuss private matters. The research details a breakdown of an AI chip produced for the Chinese company Enflame. After it took the Enflame processor apart, the research firm TechInsights said on its website, it found that key chip components had been manufactured by Taiwan Semiconductor Manufacturing Co. (TSMC). Over the past several years, the U.S. has imposed tight restrictions on what AI-related products can be sold to Chinese companies or companies operating in China, a move intended to maintain America’s lead in AI development. Chips provide the computing power necessary to create and run AI systems, and their limited supply has acted as a bottleneck for China’s AI industry. The trade rules, established by the Commerce Department, determine where and how a chip can be exported based on such metrics as the number of computing calculations it can perform per second and its computing power relative to its size. The Enflame S60 chip would be powerful enough to be subject to those export restrictions, TechInsights appeared to conclude, based on its initial analysis. The export control classification potentially makes the chip’s sale to Chinese AI companies illegal from late 2022 onward, experts say. According to several Chinese newspapers, at least one of which is state-affiliated, Enflame S60 chips were manufactured in 2024. TechInsights says it classifies export control details, also called ECCNs, based on information from manufacturers or third-party suppliers or on “information available at the time of analysis.” In the same disclaimer, TechInsights notes that its inferences and conclusions are not final regulatory decisions.  Shortly after NBC News sent its request for comment to TechInsights, the publicly viewable classification was changed to “TBD” before it was removed from the website entirely. “The technical analysis of the Enflame S60 are ongoing and not complete,” TechInsights spokesperson Graham Butler wrote in response to a request for comment. “We will temporarily place an ECCN based on the preliminary report findings. These are subject to change as reports are finalised.” TechInsights declined to provide the full report to NBC News. TSMC told NBC News that the classification originally applied by TechInsights was incorrect.  Since 2022, the U.S. has asserted that its export controls apply to certain AI-related “items made with the use of U.S. technology, software, or tools” destined for China, even if the items themselves are manufactured outside the U.S. That determination includes essentially all AI chips, given that American technology is used in the machines that manufacture chips.  Under the U.S. export rules, which have been expanded to apply to chips that are destined for countries beyond China to address smuggling fears, TSMC has been banned from shipping cutting-edge chips meant for AI purposes to Chinese companies without specific export licenses.  TSMC fabricates the majority of the world’s AI chips because of its expertise with the unique and intricate manufacturing techniques required to create semiconductor-dense chips.  “We have reviewed a certain third-party analysis report claiming a particular chip manufactured by TSMC is classified as a controlled AI chip,” a TSMC spokesperson wrote in reply to a request for comment about the new research regarding the Enflame S60.  “Based on its technical features and applications, this chip does not meet the criteria for classification as a controlled AI chip and the report has since been corrected, and the incorrect claim has been removed. TSMC is a law-abiding company, and we are committed to complying with all applicable rules and regulations, including applicable export controls.” Asked about TSMC’s claim that TechInsights’ classification for the Enflame chip was erroneous, a TechInsights spokesperson simply reiterated that the technical analysis was “ongoing and not complete.”  Enflame did not reply to requests for comment. TSMC has previously been accused of violating export controls related to manufacturing a chip found in AI processors from the leading Chinese tech company Huawei, according to Reuters.  In April, Reuters reported that the Commerce Department opened an investigation into TSMC over how its chips made for a company called Sophgo wound up in Huawei’s processors, which was already prohibited by export controls from receiving TSMC-made AI chips, and said TSMC could face a penalty of $1 billion or more. At the time, a TSMC spokesperson said that the company was committed to complying with the law and that it was cooperating with the Commerce Department.  When asked about the existence and status of the Huawei investigation, Commerce Department spokesperson Lauren Weber Holley wrote in an email: “The Department of Commerce does not comment on active enforcement matters or confirm or deny the existence of any pending investigations.” Asked about any potential investigation regarding the Enflame chip, Holley wrote: “The Department of Commerce does not comment on active enforcement matters or confirm or deny the existence of any pending investigations.”  Measures introduced in November 2023 further restricted the types of chips and countries to which semiconductor manufacturing companies using American technology can export chips meant for AI purposes. The Enflame S60 chip, manufactured by TSMC, is sufficiently powerful to make it subject to these tighter regulations, according to details previously viewable on TechInsight’s website. Jacob Feldgoise, a senior data research analyst at Georgetown University’s Center for Security and Emerging Technology who had access to separate data about the chip, said he was inclined to trust TechInsights’ initial determination that the chip is powerful enough to be subject to the rules. Because of intense global AI competition and fears that Chinese companies might divert advanced chips to the Chinese military, the Commerce Department rarely grants such licenses. According to a senior Commerce Department official, Enflame did not receive a license in 2023, 2024, or 2025. “If this is a large AI chip designed for data center use and produced at TSMC, that’s likely an export control violation similar to the Huawei-Sophgo incident,” said Lennart Heim, a leading semiconductor expert, specifying that he had not seen the chip and would need more details to reach a firm conclusion. The Commerce Department did not comment on the existence or status of the Huawei-related investigation or confirm whether that incident was an export control violation.  “Enflame is basically Tencent’s outsourced AI chip designer. It’s not some random independent startup — it’s closely tied to one of China’s largest tech companies,” Heim told NBC News in written comments. “This is probably a significantly smaller incident than Huawei-Sophgo in terms of how many chips were produced — but it shows the pattern of chips getting through TSMC that shouldn’t have.” According to the reports from Chinese newspapers, Enflame has deployed tens of thousands of S60 chips in data centers and similar infrastructure critical for AI development and usage. In recent months, the Trump administration has approved the sale of high-powered chips far exceeding the performance capabilities of the Enflame S60 chip to China. However, the State Department has reportedly slowed the approval process as it pushes for tighter rules on how the chips, like Nvidia’s H200 series, are used given national security considerations.</div>",
            "link": "https://www.nbcnews.com/tech/tech-news/ai-chip-tsmc-enflame-techinsights-rcna259342",
            "pub_date": "2026-02-27 23:25:53",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "South Korea set to finally get a fully functioning Google Maps",
            "description": "<div>SEOUL, South Korea — South Korea will soon no longer be one of the few countries where Google Maps doesn’t work properly, after its security-conscious government reversed a two-decade stance to approve the export of high-precision map data to overseas servers. The approval was made “on the condition that strict security requirements are met,” the Ministry of Land, Infrastructure and Transport said in a statement. Those conditions include blurring military and other sensitive security-related facilities, as well as restricting longitude and latitude coordinates for South Korean territory on products such as Google Maps and Google Earth, it said. The decision is expected to hurt Naver and Kakao — local internet giants that currently dominate the country’s market for digital map services. But it will appease Washington, which has urged Seoul to tackle what it says is discrimination against U.S. tech companies. “We welcome today’s decision and look forward to our ongoing collaboration with local officials to bring a fully functioning Google Maps to Korea,” Google Vice President Cris Turner said in a statement. South Korea, still technically at war with North Korea, had shot down Google’s previous bids in 2007 and 2016 to be allowed to export the data, citing the risks that information about sensitive military and security facilities could be exposed. Google Maps was not banned per se in South Korea and other countries have gone further. It is prohibited in mainland China, North Korea, Syria and Vietnam, for example. The data in question is 1:5000 scale data, where 1 centimeter on a map represents 50 meters in actual distance. Google has argued it needs to export the data to provide real-time navigation information worldwide. This includes people researching South Korean destinations from overseas. The conditions stipulate that Google must process map data on locally based servers and is only allowed to export data related to navigation and direction services that have been pre-approved by the government. The South Korean government also reserves the right to request revisions to maps, and Google must set up a security incident prevention framework to respond to emergency issues. Choi Jin-mu, a geography professor at Kyung Hee University, said the decision raised serious questions about market control and national security. “Google can now come in, slash usage fees, and take the market. If Naver and Kakao are weakened or pushed out and Google later raises prices, that becomes a monopoly. Then, even companies that rely on map services — logistics firms, for example — become dependent, and in the long run, even government GIS (geographic information) systems could end up dependent on Google or Apple. That’s the biggest concern.” Naver’s stock ended 2.3% down after the decision on Friday, though Kakao gained 1.5%. AMCHAM Chairman James Kim welcomed the decision, saying it “sends a positive signal about Korea’s commitment to innovation, open markets, and ensuring a level playing field for global companies operating in Korea.”</div>",
            "link": "https://www.nbcnews.com/world/asia/south-korea-set-finally-get-fully-functioning-google-maps-rcna260924",
            "pub_date": "2026-02-27 17:31:03",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Plaintiff in landmark social media trial testifies that apps affected her 'self-worth'",
            "description": "<div>LOS ANGELES — In a highly-anticipated testimony Thursday, the plaintiff in a landmark social media addiction trial said using apps like Instagram and YouTube when she was a child fueled her depression and anxiety, leading her to withdraw from her family. Now 20 years old, the plaintiff — identified in court documents by her initials, K.G.M. — detailed how her almost nonstop use of social media “really affected my self-worth.”  “They made me give up a lot of hobbies and old interests, and they prevented me from making friends ... [and] caused me to compare myself to other people,” she told jurors in Los Angeles County Superior Court.  “I just felt like I wanted to be on it all the time,” she said. “If I wasn’t on it, I was going to miss out on something.” K.G.M.’s trial is the first in a consolidated group of cases brought against Instagram, YouTube, TikTok and Snap by more than 1,600 plaintiffs, including over 350 families and over 250 school districts. The plaintiffs accuse the tech companies of knowingly designing addictive products harmful to young users’ mental health.  Historically, social media platforms have largely been shielded by Section 230, a provision added to the Communications Act of 1934 that says internet companies aren’t liable for the content users post. TikTok and Snap reached settlements with K.G.M. before the trial, but they remain defendants in a series of similar lawsuits expected to go to trial this year. K.G.M.’s bellwether case could set a legal precedent for whether social media platforms are liable for mental health issues in children. If the jury’s verdict favors K.G.M., the companies could face damages to be determined by the jury and forced to change the designs of their platforms. The verdict could also set the tone for whether they choose to fight or settle the oncoming cases. Meta, which owns Instagram and Facebook, and Google, which owns YouTube, have denied that their apps are purposefully harmful and addictive for young users.  In court, K.G.M. elaborated on the claims that the companies made deliberate design choices to make their platforms more addictive to children for purposes of profit. Her complaint highlights a variety of features that it argues the platforms use to “exploit children and adolescents,” including “an algorithmically-generated, endless feed to keep users scrolling,” rewards that encourage people to keep using the platform and “incessant” notifications, as well as “inadequate” measures for age verification and parental control. K.G.M. said she created her Instagram account at age 9, before the app asked new users to enter their birthdays. Before that, she said, she also lied about her age to create a YouTube account without having read through any of the platform’s legal fine print.  As she was growing up, K.G.M. told jurors, being without her phone often sent her “into a panic.” “Without it, I felt like a huge part of me was missing,” she said. “If I didn’t have it, I would be missing out on something. I couldn’t see who was liking my stuff.” Though she and her mother often argued about her phone use, she couldn’t put it down. She said she would get “a rush” every time she got a notification about her social media posts or profiles and subsequently felt so compelled to check them that she’d sneak away to the bathroom or put off sleep at night.  “When I got a bunch of likes, I was really happy,” she said. “If I didn’t get a lot of likes, I would feel I shouldn’t have posted it, I was ugly.” With YouTube, K.G.M. said, she started “at a young age” and “spent all my time on it.”  “I would watch it in class,” she said. “Any time I tried to set limits for myself, it just didn’t work, and I just couldn’t get off.” K.G.M. said that because of her social media use, she still struggles with body dysmorphia. She said she began experiencing it after she was exposed to social media filters, which often overlay effects like skin smoothing or makeup.  Meta introduced beauty filters to Instagram stories in 2017. In 2019, it significantly expanded its slate of augmented reality filters, allowing users to make and publish their own.  K.G.M. said she now tries to avoid filters “because I know I’ll feel worse if I use them.” Meta has pushed back against claims that the design of social media platforms is responsible for K.G.M.’s mental health challenges as a child, arguing in a brief filed Wednesday that she faced other issues at home that contributed to her mental state. Its filing pointed to “numerous examples of ‘emotional abuse and neglect by [Plaintiff’s] mother, including prolonged periods of the silent treatment, frequent name-calling (e.g., ‘dumb,’ ‘stupid’), and mocking of her voice,’ and ‘physical abuse, including hitting the plaintiff.’” (In court, K.G.M. disputed Meta’s argument that her family contributed to her mental health struggles, telling jurors she didn’t experience “abuse or neglect or anything like that.”) Pressed about social media addiction, Instagram head Adam Mosseri, who testified earlier this month, said, “I think it’s important to differentiate between clinical addiction and problematic use.” Mosseri also touched on filters, saying the platform eventually decided to prohibit “effects promoting plastic surgery.” YouTube’s vice president of engineering, Cristos Goodrow, who took the stand Monday and Tuesday, also emphasized that the video platform is “not designed to maximize time.”  On Wednesday, Victoria Burke, who was K.G.M.’s therapist when she was 13, testified that she doesn’t think social media was the sole driver of her former client’s mental health issues but that she does suspect it played a part. “I believe it was a contributing factor, not a causation factor,” she said.</div>",
            "link": "https://www.nbcnews.com/tech/tech-news/social-media-addiction-trial-plaintiff-testifies-depression-anxiety-rcna260851",
            "pub_date": "2026-02-27 08:51:26",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "AI giant Nvidia made $120 billion in profit last year. Investors are still spooked.",
            "description": "<div>Nvidia — the most valuable company in the world thanks to its place atop the AI food chain — generated a staggering $120 billion in profits last year. This includes an eye-popping $43 billion during the three-month period ending in January, one of the strongest quarters of any business ever recorded.  Investors didn’t seem to care.  Shares of Nvidia fell more than 5% Thursday, following the results. With a total market value of roughly $4.5 trillion, the company’s one-day loss amounted to roughly $256 billion worth of market capital.  Nvidia’s stock price decline is part of a broader phenomenon dubbed the  AI “scare trade,” that is percolating in certain corners of the stock market. This bearish play threatens the very driver that has powered broader, double-digit gains across the benchmark S&P 500 for the past two years.  And while the stock market might look broad, its gains are increasingly concentrated in just a handful of mega-cap names, including Nvidia. In other words, the entire market’s performance is heavily tied to the performance of these select companies.  Nvidia says its growth story is very much still intact.  “We have now seen the inflection of agentic AI and the usefulness of agents across the world and enterprises everywhere,” Nvidia CEO Jensen Huang said during the company’s quarterly earnings call Wednesday, referring to AI chatbots like OpenAI’s ChatGPT and Anthropic’s Claude.  “You’re seeing incredible compute demand because of it,” he said. “In this new world of AI, compute is revenues.”   Compute refers to the processing power that is needed to train and operate AI models. Nvidia’s chips, each of which is around 30 inches square, underpin the massive data centers needed to run AI chatbots and agents.  Nvidia’s dominance in the AI chip race also means that more companies than ever are dependent upon its products, at a time when AI is evolving faster than even its early adopters say they could have imagined. Over the past five years, this has sparked a massive investor rush to buy a piece of Nvidia, whose shares have surged nearly 1,300% since the start of 2021. Driven by a mixture of FOMO and faith in AI’s growth-at-any-cost business model, these investors and others like them have piled into virtually any company with even a tangential relationship to the AI industry.  All this time, Nvidia has led the charge. But so far this year, Nvidia’s share price is barely positive. Some firms, including HSBC, have argued that in order to justify another leg higher in the company’s stock price, Nvidia needs a “new narrative,” such as a meaningful expansion in AI demand or pricing power, to justify another leg higher in the stock. But more broadly, the AI scare trade visited upon Nvidia Thursday underscores a growing unease around the future of AI.  After a multi-year boom for public and private companies alike, AI is now facing tougher scrutiny. Questions persist around whether or not the AI boom is starting to look more like a bubble. Likewise, investors are uncertain whether AI can generate the kind of near-term returns necessary to justify the massive investments — and soaring share prices —  coursing through the tech world. “Artificial intelligence stands to become one of the most consequential technologies in generations, if not in the history of humankind, with enormous implications for the economy,” Moody’s economist Mark Zandi wrote in a new report Wednesday. “However, the specifics of how it will shape the future remain highly uncertain and are the subject of immense debate.” That debate includes growing concerns over how AI agents will affect vulnerable industries like cybersecurity and software — and potentially upend traditional business models that have worked for decades.  Shares of software companies like ServiceNow and Synopsis have fallen sharply amid those fears, declining roughly 20% and 15% over the past month, respectively. Salesforce is down nearly 25% this year. So far this year, companies in the software industry have been the largest drag on the S&P 500. AI has “started to call into question how exactly software companies are really going to compete and provide something superior in this environment,” Melissa Otto, head of Visible Alpha research at S&P Global, told NBC News. Nvidia’s Huang attempted to push back on this narrative in an interview with CNBC on Wednesday.  The “markets got it wrong” when it comes to the AI-driven panic around software, he said. Huang argued that AI will enhance productivity and expand what software can do, rather than kill the whole industry.  Huang’s attempt to assuage investor fears didn’t move the needle much, though. On Thursday, the tech-heavy Nasdaq fell nearly 1.5% on the back of Nvidia’s slide. Software giants like Synopsys dropped 5% while shares of Microsoft and Alphabet also traded lower.  Beyond software, investors are contending with other existential anxieties. Many of them were captured this week in an essay posted on Substack by a small research firm called Citrini Research. The post warned that AI adoption would lead to a stock market crash, a sharp pullback in consumer spending and widespread white-collar layoffs by 2028. The report painted a vivid picture of an economic doomsday scenario caused by AI, effectively animating investors’ vague, simmering fears. Payments giants like Mastercard and American Express were hit particularly hard after the post named them as potential casualties in a lower-spending, AI-disrupted economy. Shares of the two payment giants rebounded slightly on Thursday.  Many Wall Street analysts say it’s too soon to panic.  “While we take concerns about the AI trade and private markets and other matters seriously, we think it’s premature to assume that’s the kind of risk we face today,” Lori Calvasina, head of U.S. equity strategy research at RBC Capital Markets, wrote in a client note earlier this month.  Kristy Akullian, Blackrock’s head of iShares investment strategy for the Americas, added in a separate note Thursday that the recent sell-off “is predicated on still uncertain existential risk,” rather than any immediate changes to company earnings or business fundamentals.  Nonetheless, this existential risk is one that investors are taking more seriously now than they did six months ago. </div>",
            "link": "https://www.nbcnews.com/business/markets/nvidia-made-120-billion-profit-last-year-investors-are-worried-rcna260839",
            "pub_date": "2026-02-27 06:10:48",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "What the polls say about how Americans are using AI",
            "description": "<div>Americans are increasingly encountering and using artificial intelligence technologies like Anthropic’s Claude, OpenAI’s ChatGPT and Google’s Gemini. Most surveys now suggest more than two-thirds of the population is interacting with these technologies. However, a defining question is how far along we are in a transition from experimentation with AI to integration into daily personal and professional life. How many Americans are using it regularly? For what purpose? And what value are they extracting? One compelling way to track this adoption of AI is via Gallup polling, which found that 12% of Americans now report using AI daily at work. While the number seems modest, it represents a threefold increase in just over a year, from 4% in mid-2024. Employees in the knowledge-work sector are leading this change. In the technology industry, 31% of workers reported daily usage. That’s compared to 19% of finance industry workers and 16% of employees in the professional service industry. Gallup also identified the ability to work remotely as an indicator of AI adoption. Daily usage among remote-capable jobs sits at 19%, more than double the 7% daily adoption rate in roles requiring an on-site or physical presence. For historical perspective, Pew Research Center tracked internet adoption in the early aughts. In 2000, 14% went online daily for job-related research. In 2001, the number jumped up to 19%. While AI usage by white-collar workers is more pronounced than among blue-collar workers, pessimism regarding AI’s impact on the labor market cuts across the typical dividing line for social and cultural issues.  An Economist/YouGov poll found 63% of American adults thought that advances in AI would lead to an overall decrease in jobs. There was little difference in opinion based on education: 67% of those with college degrees and 61% of those without degrees shared this concern. This pessimism does appear to be greater than concerns about computers in the workplace at the turn of the century. In 1999, a NPR/Kaiser/Harvard Technology survey found that 32% thought computers would lead to a decrease in jobs, while 43% thought they would lead to an increase and 23% thought computers wouldn’t make a difference. That means today’s anxiety about AI is nearly twice as high as the computer anxiety of the late ’90s, as overall attitudes toward AI seem to resemble the mix of skepticism and curiosity seen during the early rise of the internet. As use of AI spreads, another question is how people are actually using it. OpenAI released a study last year that estimated 30% of ChatGPT usage was work-related and 70% was personal. The study categorized ChatGPT usage into three main behaviors. There was “asking” (49%), or prompting the AI for advice or information on a specific topic. “Doing” (40%) involved practical tasks such as drafting text or writing code. And “expressing” — chatting or playing — accounted for 11% of usage. Writing and seeking guidance are among the leading practical uses of ChatGPT, according to OpenAI. When specifically looking at the workplace, the study found technical help and writing become even more dominant uses. Still, a majority of respondents (53%) in a recent Fox News poll said AI had not yet made a significant difference in their lives, while 26% said it had personally helped them and 20% believed it had caused them harm. With daily AI use in the workplace tripling in a year, it's possible those numbers could shift even more rapidly in the months ahead, as the technology moves from the periphery and toward the center of American life.</div>",
            "link": "https://www.nbcnews.com/politics/politics-news/polls-say-americans-are-using-ai-more-work-personal-rcna260422",
            "pub_date": "2026-02-27 01:41:26",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Instagram will alert parents to teens' repeated suicidal or self-harm searches",
            "description": "<div>Instagram will begin notifying parents when their teenagers repeatedly attempt to search for suicide or self-harm content, the company said Thursday. The new feature will alert parents enrolled in the parental supervision tool to repeated searches in a short time period and offer expert resources about how to talk to their teens about the issue. The function will start to roll out in the coming weeks in Australia, Canada, the United Kingdom and the United States, with more countries to come later this year.  “The vast majority of teens do not try to search for suicide and self-harm content on Instagram, and when they do, our policy is to block these searches, instead directing them to resources and helplines that can offer support,” the company said in its announcement. “These alerts are designed to make sure parents are aware if their teen is repeatedly trying to search for this content, and to give them the resources they need to support their teen.” The move comes as Instagram’s parent company, Meta, and other social media platforms face ongoing scrutiny over the safety of their products, particularly for young people.  In Los Angeles, a consolidated group of cases with more than 1,600 plaintiffs, including more than 350 families and over 250 school districts, are accusing Instagram, YouTube, TikTok and Snap of deliberately designing platforms to be addictive to young users. Last week, Meta CEO Mark Zuckerberg said in court that Instagram is meant to build “a community that is sustainable” and not designed to addict young users. TikTok and Snap settled ahead of the trial. Meta, and in particular Instagram, have taken some steps to address concerns around the use of its platforms by teens. In 2024, Instagram introduced accounts specifically for teens meant to restrict who can contact them. In October, the company said it would overhaul its approach to teens' accounts, limiting their access to certain content in an attempt to make the experience closer to viewing PG-13 movies.  Instagram already blocks content related to suicide or self-harm from reaching teens’ accounts. However, families of teens who died by suicide allege in their lawsuits that Instagram is responsible for multiple sextortion scams targeting teens, NBC News previously reported. Meta spokesperson Sophie Vogel told NBC News that teens can also talk to Instagram’s existing artificial intelligence tool later this year to seek support, and parents will also be notified of conversations related to suicide or self-harm. If you or someone you know is in crisis, call or text 988 or go to 988lifeline.org to reach the Suicide & Crisis Lifeline. You can also call the network, previously known as the National Suicide Prevention Lifeline, at 800-273-8255 or visit SpeakingOfSuicide.com/resources.</div>",
            "link": "https://www.nbcnews.com/tech/social-media/instagram-will-alert-parents-teens-repeated-suicidal-self-harm-searche-rcna260789",
            "pub_date": "2026-02-27 00:50:55",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Florida Gov. Ron DeSantis leans into AI skepticism, seeking a contrast with Vance",
            "description": "<div>Florida Gov. Ron DeSantis isn’t sold on the massive expansion of AI.  And that belief might be his way back to national political relevance. The Republican governor is appealing to a growing number of people who have concerns that AI’s rapid build-up, fueled in part by taxpayer dollars, could displace jobs, increase energy costs and hurt the environment. DeSantis’ positions stand in direct contrast to the embrace of the AI industry by President Donald Trump and the two likeliest potential candidates to snag his 2028 presidential endorsement: Vice President JD Vance and Secretary of State Marco Rubio. “We don’t want to see them building a massive data center and then sending you the bill,” DeSantis said this month when asked about AI companies. “Data centers take up the power equivalent of a half a million-person city. We feel very, very strongly about protecting the consumer.” For DeSantis, the embrace of AI skepticism is rooted in both personal policy preference and a 2028-focused political calculation as the term-limited governor plots out his political future, according to eight sources, most of whom have either worked in his administration or for his past campaigns at both the state and national levels. Many of them requested anonymity to speak candidly. “It’s kind of a no-brainer, right? You’ve got JD Vance and Marco Rubio, the top two contenders for 2028 big time in the pro-AI lane,” a longtime DeSantis adviser said. “The infrastructure is lining up behind JD and to some extent Marco. So, DeSantis’ challenge is to stay relevant.” Taryn Fenske, a DeSantis political aide, said the governor is a skeptic because of AI’s potential societal dangers. “The governor is an AI skeptic because chatbots are convincing children to commit suicide,” she said.  DeSantis also leads a state the AI industry is likely to target, making it a focus of the broader fight between AI skeptics and proponents.  NBC News reported this month that Leading the Future, a pro-AI super PAC, is spending $5 million on TV ads boosting Republican Rep. Byron Donalds' Florida gubernatorial campaign. Donalds is the only state-level politician the group has spent money to help, and officials there said it's an indication AI companies will look to expand their footprint there in the future. Recent polling suggests that AI could be a pertinent issue in the upcoming midterm elections and the 2028 presidential race. A poll conducted this month by The Economist and YouGov found that 63% of the U.S. citizens surveyed — including 60% who voted for Trump in 2024 — believe that advances in AI will reduce the number of jobs available in the country. A plurality of respondents, 33%, said that AI would have a “more negative than positive” impact on the U.S. economy. And a Morning Consult poll in November found that a 41% plurality of registered voters favored banning the construction of data centers near their homes; 36% opposed such a ban, while 22% said they didn’t know or had no opinion. DeSantis and the Trump administration have already been at odds on the issue.  Trump’s AI czar David Sacks and other administration allies have directly lobbied against DeSantis’ push to get Florida’s GOP-dominated Legislature to implement state-level regulations on AI and the massive data centers needed to accommodate the industry’s boom. Those bills remain alive, but as Florida’s legislative session comes to a close, their passage has become increasingly unlikely. “There are some people … who almost relish in the fact that they think this just displaces human beings and then, ultimately, you’re going to have AI run society, and that you’re not going to be able to control it,” DeSantis said at an AI roundtable earlier this month. “Count me out on that.” DeSantis and Trump themselves have publicly buried the hatchet, including golfing together this month, after a brutal 2024 GOP presidential primary. But as the jockeying begins to become the first post-Trump Republican nominee for president, it does not mean the notoriously politically sharp-elbowed DeSantis will not look to use the fight over AI regulation as a political cudgel against Trump allies such as Vance and Rubio. “You know the story of the scorpion and the frog? Ron DeSantis is looking for his moment to stab the White House on something, and that might very well be AI,” said a longtime DeSantis political adviser who currently represents AI industry clients.  “And you know why he’s going to do it?” the person added. “Because he’s Ron DeSantis. It’s what he does.” During his State of the Union address Tuesday night, Trump said his administration had struck a deal with major tech companies to require them to pay for more of the energy costs associated with building massive data centers.  “We’re telling the major tech companies that they have the obligation to provide for their own power needs — they can build their own power plants as part of their factory, so that no one’s prices will go up — and in many cases, prices of electricity will go down for the community,” the president said. For now, DeSantis’ AI strategy would likely to some degree center on Vance, who in most public polling has been the overwhelming leader for the 2028 Republican presidential nomination, even as Rubio has picked up momentum in recent weeks. Vance, with his background in Silicon Valley venture capital and relationships with leading Big Tech figures, is known as one of the Republican Party’s biggest AI champions.  At last year’s Artificial Intelligence Action Summit in Paris, the vice president warned that excessive regulation “could kill a transformative industry” and pledged the Trump  administration’s support for “pro-growth AI policies.”  At the same time, Vance has attempted to reassure those on both sides of the debate.  Last fall, in an interview with Fox News’ Sean Hannity, Vance compared a rise in AI-related jobs to the arrival decades ago of cash-dispensing ATMs. Human bank tellers, Vance noted, still exist. He also linked his pro-AI stance to his anti-immigration views. Using home construction as an example, Vance described robots as complementary to “blue-collar” workers and immigrant laborers as an outright threat to replace them on job sites. “No robot can replace a great blue-collar construction worker,” Vance said. “You see some of the houses, some of the things they do, the trim that they’re able to do. There’s an art there that I don’t think a robot is ever going to be able to replace.” Vance has more recently expressed concerns about AI, telling Fox News’ Martha MacCallum last week that he worries specifically about it being used to surveil Americans or to advance invasions of privacy and political bias. In that same interview, Vance also illustrated the differences between him and DeSantis when it comes to regulating AI. DeSantis has pushed for state-level regulation, while Vance, along with the Trump administration more broadly, has supported the industry-backed idea of Congress passing a national regulatory bill. “I think that eventually you’re going to have some standard applied, whether it’s a federal standard or whether it’s one state standard dominating,” Vance told MacCallum when pressed for his thoughts on regulation. “I think, frankly, the worst possible outcome would be to have far left California dominate the entire AI regulatory map.” Representatives for Vance did not return requests for comment. An official close to the DeSantis administration said that the governor sees an opportunity with the growing number of average people who feel elbowed out and negatively impacted by AI’s growth. “Look at all these many trillions of dollars being spent on AI and data centers. They have no clue how it will ever benefit them,” the person said. “This stuff is more for enterprises than the individual person, and they see it as something that will jack up costs and replace them.” “For DeSantis’, it’s a populist play,” the person added. “And that’s perfect for him.” The former official  said there are, to some extent, broader geopolitical concerns to consider as the U.S. engages in an “arms race” with China over AI expansion — and the almost inevitable change to the modern world that fight will usher in.  “The reality is this is an arms race, this is a cold war arms race against China dumping tons of money into AI,” the person said. “If you are on Team Trump, the only way you dig out of debt right now is to radically enhance productivity and grow production.” “International investors I talk to are perplexed on how anyone in the U.S. could be anti-AI right now,” the person said. “That just makes things easier for China.” Across the country, people are heavily pushing back on efforts to put massive data centers in their cities and towns. Most national polling is favorable to building data centers, but the numbers crater when people are asked if they would like one in their backyard. “Nationally, when you talk about data centers, it polls at roughly 60%,” said an adviser who works with pro-AI groups. “But when you say to people in Loudoun County, if they want one in Loudoun County, the numbers are really, really bad,” the person said, referring to the Virginia county. “Nationally it all sounds good and gravy,” the adviser added. “But when you get to the local stuff, you’re f-----.”</div>",
            "link": "https://www.nbcnews.com/politics/2028-election/florida-gov-ron-desantis-ai-skepticism-contrast-vance-rcna258824",
            "pub_date": "2026-02-26 18:05:58",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        }
    ]
}