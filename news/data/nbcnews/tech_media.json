{
    "data": [
        {
            "title": "Instagram will alert parents to teens' repeated suicidal or self-harm searches",
            "description": "<div>Instagram will begin notifying parents when their teenagers repeatedly attempt to search for suicide or self-harm content, the company said Thursday. The new feature will alert parents enrolled in the parental supervision tool to repeated searches in a short time period and offer expert resources about how to talk to their teens about the issue. The function will start to roll out in the coming weeks in Australia, Canada, the United Kingdom and the United States, with more countries to come later this year.  “The vast majority of teens do not try to search for suicide and self-harm content on Instagram, and when they do, our policy is to block these searches, instead directing them to resources and helplines that can offer support,” the company said in its announcement. “These alerts are designed to make sure parents are aware if their teen is repeatedly trying to search for this content, and to give them the resources they need to support their teen.” The move comes as Instagram’s parent company, Meta, and other social media platforms face ongoing scrutiny over the safety of their products, particularly for young people.  In Los Angeles, a consolidated group of cases with more than 1,600 plaintiffs, including more than 350 families and over 250 school districts, are accusing Instagram, YouTube, TikTok and Snap of deliberately designing platforms to be addictive to young users. Last week, Meta CEO Mark Zuckerberg said in court that Instagram is meant to build “a community that is sustainable” and not designed to addict young users. TikTok and Snap settled ahead of the trial. Meta, and in particular Instagram, have taken some steps to address concerns around the use of its platforms by teens. In 2024, Instagram introduced accounts specifically for teens meant to restrict who can contact them. In October, the company said it would overhaul its approach to teens' accounts, limiting their access to certain content in an attempt to make the experience closer to viewing PG-13 movies.  Instagram already blocks content related to suicide or self-harm from reaching teens’ accounts. However, families of teens who died by suicide allege in their lawsuits that Instagram is responsible for multiple sextortion scams targeting teens, NBC News previously reported. Meta spokesperson Sophie Vogel told NBC News that teens can also talk to Instagram’s existing artificial intelligence tool later this year to seek support, and parents will also be notified of conversations related to suicide or self-harm. If you or someone you know is in crisis, call or text 988 or go to 988lifeline.org to reach the Suicide & Crisis Lifeline. You can also call the network, previously known as the National Suicide Prevention Lifeline, at 800-273-8255 or visit SpeakingOfSuicide.com/resources.</div>",
            "link": "https://www.nbcnews.com/tech/social-media/instagram-will-alert-parents-teens-repeated-suicidal-self-harm-searche-rcna260789",
            "pub_date": "2026-02-27 00:50:55",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Florida Gov. Ron DeSantis leans into AI skepticism, seeking a contrast with Vance",
            "description": "<div>Florida Gov. Ron DeSantis isn’t sold on the massive expansion of AI.  And that belief might be his way back to national political relevance. The Republican governor is appealing to a growing number of people who have concerns that AI’s rapid build-up, fueled in part by taxpayer dollars, could displace jobs, increase energy costs and hurt the environment. DeSantis’ positions stand in direct contrast to the embrace of the AI industry by President Donald Trump and the two likeliest potential candidates to snag his 2028 presidential endorsement: Vice President JD Vance and Secretary of State Marco Rubio. “We don’t want to see them building a massive data center and then sending you the bill,” DeSantis said this month when asked about AI companies. “Data centers take up the power equivalent of a half a million-person city. We feel very, very strongly about protecting the consumer.” For DeSantis, the embrace of AI skepticism is rooted in both personal policy preference and a 2028-focused political calculation as the term-limited governor plots out his political future, according to eight sources, most of whom have either worked in his administration or for his past campaigns at both the state and national levels. Many of them requested anonymity to speak candidly. “It’s kind of a no-brainer, right? You’ve got JD Vance and Marco Rubio, the top two contenders for 2028 big time in the pro-AI lane,” a longtime DeSantis adviser said. “The infrastructure is lining up behind JD and to some extent Marco. So, DeSantis’ challenge is to stay relevant.” Taryn Fenske, a DeSantis political aide, said the governor is a skeptic because of AI’s potential societal dangers. “The governor is an AI skeptic because chatbots are convincing children to commit suicide,” she said.  DeSantis also leads a state the AI industry is likely to target, making it a focus of the broader fight between AI skeptics and proponents.  NBC News reported this month that Leading the Future, a pro-AI super PAC, is spending $5 million on TV ads boosting Republican Rep. Byron Donalds' Florida gubernatorial campaign. Donalds is the only state-level politician the group has spent money to help, and officials there said it's an indication AI companies will look to expand their footprint there in the future. Recent polling suggests that AI could be a pertinent issue in the upcoming midterm elections and the 2028 presidential race. A poll conducted this month by The Economist and YouGov found that 63% of the U.S. citizens surveyed — including 60% who voted for Trump in 2024 — believe that advances in AI will reduce the number of jobs available in the country. A plurality of respondents, 33%, said that AI would have a “more negative than positive” impact on the U.S. economy. And a Morning Consult poll in November found that a 41% plurality of registered voters favored banning the construction of data centers near their homes; 36% opposed such a ban, while 22% said they didn’t know or had no opinion. DeSantis and the Trump administration have already been at odds on the issue.  Trump’s AI czar David Sacks and other administration allies have directly lobbied against DeSantis’ push to get Florida’s GOP-dominated Legislature to implement state-level regulations on AI and the massive data centers needed to accommodate the industry’s boom. Those bills remain alive, but as Florida’s legislative session comes to a close, their passage has become increasingly unlikely. “There are some people … who almost relish in the fact that they think this just displaces human beings and then, ultimately, you’re going to have AI run society, and that you’re not going to be able to control it,” DeSantis said at an AI roundtable earlier this month. “Count me out on that.” DeSantis and Trump themselves have publicly buried the hatchet, including golfing together this month, after a brutal 2024 GOP presidential primary. But as the jockeying begins to become the first post-Trump Republican nominee for president, it does not mean the notoriously politically sharp-elbowed DeSantis will not look to use the fight over AI regulation as a political cudgel against Trump allies such as Vance and Rubio. “You know the story of the scorpion and the frog? Ron DeSantis is looking for his moment to stab the White House on something, and that might very well be AI,” said a longtime DeSantis political adviser who currently represents AI industry clients.  “And you know why he’s going to do it?” the person added. “Because he’s Ron DeSantis. It’s what he does.” During his State of the Union address Tuesday night, Trump said his administration had struck a deal with major tech companies to require them to pay for more of the energy costs associated with building massive data centers.  “We’re telling the major tech companies that they have the obligation to provide for their own power needs — they can build their own power plants as part of their factory, so that no one’s prices will go up — and in many cases, prices of electricity will go down for the community,” the president said. For now, DeSantis’ AI strategy would likely to some degree center on Vance, who in most public polling has been the overwhelming leader for the 2028 Republican presidential nomination, even as Rubio has picked up momentum in recent weeks. Vance, with his background in Silicon Valley venture capital and relationships with leading Big Tech figures, is known as one of the Republican Party’s biggest AI champions.  At last year’s Artificial Intelligence Action Summit in Paris, the vice president warned that excessive regulation “could kill a transformative industry” and pledged the Trump  administration’s support for “pro-growth AI policies.”  At the same time, Vance has attempted to reassure those on both sides of the debate.  Last fall, in an interview with Fox News’ Sean Hannity, Vance compared a rise in AI-related jobs to the arrival decades ago of cash-dispensing ATMs. Human bank tellers, Vance noted, still exist. He also linked his pro-AI stance to his anti-immigration views. Using home construction as an example, Vance described robots as complementary to “blue-collar” workers and immigrant laborers as an outright threat to replace them on job sites. “No robot can replace a great blue-collar construction worker,” Vance said. “You see some of the houses, some of the things they do, the trim that they’re able to do. There’s an art there that I don’t think a robot is ever going to be able to replace.” Vance has more recently expressed concerns about AI, telling Fox News’ Martha MacCallum last week that he worries specifically about it being used to surveil Americans or to advance invasions of privacy and political bias. In that same interview, Vance also illustrated the differences between him and DeSantis when it comes to regulating AI. DeSantis has pushed for state-level regulation, while Vance, along with the Trump administration more broadly, has supported the industry-backed idea of Congress passing a national regulatory bill. “I think that eventually you’re going to have some standard applied, whether it’s a federal standard or whether it’s one state standard dominating,” Vance told MacCallum when pressed for his thoughts on regulation. “I think, frankly, the worst possible outcome would be to have far left California dominate the entire AI regulatory map.” Representatives for Vance did not return requests for comment. An official close to the DeSantis administration said that the governor sees an opportunity with the growing number of average people who feel elbowed out and negatively impacted by AI’s growth. “Look at all these many trillions of dollars being spent on AI and data centers. They have no clue how it will ever benefit them,” the person said. “This stuff is more for enterprises than the individual person, and they see it as something that will jack up costs and replace them.” “For DeSantis’, it’s a populist play,” the person added. “And that’s perfect for him.” The former official  said there are, to some extent, broader geopolitical concerns to consider as the U.S. engages in an “arms race” with China over AI expansion — and the almost inevitable change to the modern world that fight will usher in.  “The reality is this is an arms race, this is a cold war arms race against China dumping tons of money into AI,” the person said. “If you are on Team Trump, the only way you dig out of debt right now is to radically enhance productivity and grow production.” “International investors I talk to are perplexed on how anyone in the U.S. could be anti-AI right now,” the person said. “That just makes things easier for China.” Across the country, people are heavily pushing back on efforts to put massive data centers in their cities and towns. Most national polling is favorable to building data centers, but the numbers crater when people are asked if they would like one in their backyard. “Nationally, when you talk about data centers, it polls at roughly 60%,” said an adviser who works with pro-AI groups. “But when you say to people in Loudoun County, if they want one in Loudoun County, the numbers are really, really bad,” the person said, referring to the Virginia county. “Nationally it all sounds good and gravy,” the adviser added. “But when you get to the local stuff, you’re f-----.”</div>",
            "link": "https://www.nbcnews.com/politics/2028-election/florida-gov-ron-desantis-ai-skepticism-contrast-vance-rcna258824",
            "pub_date": "2026-02-26 18:05:58",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Anthropic says U.S. military can use its AI systems for missile defense ",
            "description": "<div>In contract negotiations between senior Defense Department officials and leaders from AI giant Anthropic in December, the company agreed to allow the U.S. government to use its AI systems for missile and cyber defense purposes, a person familiar with the matter said, requesting anonymity to speak about private discussions. But that apparently did not satisfy the Pentagon. Following weeks of tension between the Defense Department and Anthropic over the company’s restrictions on how its products can be used by the military, Defense Secretary Pete Hegseth issued a stark ultimatum to company CEO Dario Amodei on Tuesday: Allow the AI technology to be used for all legal military purposes by this Friday or be forced to cooperate, a senior Pentagon official told NBC News. The ultimatum, detailed to NBC News by a senior Pentagon official, comes as Anthropic — a company that has heavily marketed its focus on AI safety — tries to maintain firm policies preventing its systems from being used for mass domestic surveillance or direct use in lethal autonomous weapons. The December contract changes would allow for its systems to be widely used for cyber and missile defense, according to the person familiar with the matter. An Anthropic spokesperson told NBC News in a statement that “Every iteration of our proposed contract language would enable our models to support missile defense and similar uses.”  But the company’s insistence on guardrails have continued to be a source of contention between Anthropic and the Defense Department. According to the senior Pentagon official, representatives from the department, including Undersecretary of Defense Emil Michael, recently discussed several hypothetical scenarios with Anthropic leadership about how the company’s products might be employed by the military.  As part of those discussions, the officials discussed how Anthropic’s systems might be used if an adversary launched an intercontinental ballistic missile at the U.S. According to the Pentagon source, the officials discussed whether Anthropic’s guardrails might somehow block a U.S. response to the launch. Anthropic officials said they could be called on to lift those restrictions, according to the official, but Pentagon leadership was not fully satisfied with Anthropic’s adjustments and did not want to be beholden to the private company.  According to an Anthropic spokesperson, any suggestion that CEO Amodei said the Pentagon would have to call the company in each missile defense operation is “patently false.”  In the latest escalation in negotiations, during Tuesday’s meeting Pentagon leaders said they could invoke the Defense Production Act to force Anthropic to comply with the Pentagon's rules, according to the senior Pentagon official. The Act allows the president to control domestic companies critical to national security in times of need. In Tuesday’s meeting, Pentagon leadership also invoked threats to instead label Anthropic as a “supply chain risk” and ban all defense business with the company if it does not align its terms of service for certain high-stakes uses with the Pentagon by Friday, the source said.  “Anthropic has until 5:01pm Friday to get on board with the Department of War,” the senior Pentagon official said of the ultimatum in a statement provided to NBC News, responding to questions about the meeting. “If they don’t get on board, the Secretary of War will ensure the Defense Production Act is invoked on Anthropic, compelling them to be used by the Pentagon.” “Additionally, the Secretary of War will also label Anthropic a supply chain risk,” the official said. Asked about Tuesday’s meeting, an Anthropic spokesperson said in a statement: “Dario expressed appreciation for the Department’s work and thanked the Secretary for his service. We continued good-faith conversations about our usage policy to ensure Anthropic can continue to support the government’s national security mission in line with what our models can reliably and responsibly do.” Hegseth complimented Anthropic’s products and said the Pentagon wanted to work with Anthropic, according to another person familiar with the meeting, who requested anonymity to speak candidly. The person confirmed that the department said it would terminate Anthropic’s work with the Pentagon by Friday if it did not agree to its terms. According to reports from The Wall Street Journal and Axios, Anthropic’s Claude systems were used during the operation to capture Venezuelan President Nicolás Maduro in January. It is unclear exactly how the systems were used. Hegseth sent a memo to senior Pentagon officers Jan. 9 announcing the Pentagon’s drive toward an “AI-first warfighting force.” He outlined a push to use AI models, like Anthropic’s, for all legitimate military purposes, “free from usage policy constraints” set by individual AI companies.  Anthropic is the only AI company whose products are actively used on classified networks, through its contract with Palantir, a data analytics company. A senior Pentagon official confirmed to NBC News that xAI reached a deal with the Pentagon on Monday to use its Grok chatbot system on classified networks, agreeing to allow its systems to be harnessed for “any lawful use” as Hegseth desired. Anthropic was one of four AI companies — the others were OpenAI, Google DeepMind and xAI — to get contracts worth up to $200 million in July to “prototype frontier AI capabilities that advance U.S. national security.”</div>",
            "link": "https://www.nbcnews.com/tech/security/anthropic-pentagon-us-military-can-use-ai-missile-defense-hegseth-rcna260534",
            "pub_date": "2026-02-26 02:00:58",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Discord pushes back age verification rollout following backlash",
            "description": "<div>Discord, the popular online communication platform for gamers, said Wednesday it will delay its global age verification rollout after receiving user criticism. Earlier this month, Discord announced a phased rollout for new and existing users that would implement video selfies to determine a person's age group. Users could also submit a form of identification to their vendor partners.  The rollout was supposed to begin in early March and would give underage users a \"teen-appropriate experience\" that included updated communication settings, content filtering and restricted access to age-gated spaces.  Criticism was immediate, with many users pointing to an October security breach of a third-party provider Discord used that exposed government ID photos for thousands of users, The Associated Press reported.  The platform’s chief technology officer responded to the backlash in an update Tuesday, saying that Discord \"missed the mark\" and that the rollout is being delayed to the second half of 2026.  \"Let me be upfront: we knew this rollout was going to be controversial. Any time you introduce something that touches identity and verification, people are going to have strong feelings. Rightfully so. In hindsight, we should have provided more detail about our intentions and how the process works,\" Discord CTO Stanislav Vishnevskiy wrote in a Wednesday blog post.  Vishnevskiy said the platform will not require face scans or ID uploads from everyone, and that over 90% of users will never need to verify their age to continue using it.  \"If you’re among the less than 10% of users who do need to verify, we’ll give you options, designed to tell us only your age and never your identity,\" Vishnevskiy said.  If a user chooses not to verify their age, they can keep their account, servers, friends list, messages, and voice chat, but won't be able to access age-restricted content or change certain safety settings.   Vishnevskiy also addressed last year's security breach, saying that Discord no longer works with that vendor.  \"We’ve made mistakes. I won’t pretend we haven’t. And I know that being a bigger company now means our mistakes have bigger consequences and erode trust faster. I don’t expect one blog post to fix that,\" Vishnevskiy said.  \"We’re listening. We’ll get this right. And when we ship, you’ll be able to see for yourselves,\" he added.</div>",
            "link": "https://www.nbcnews.com/tech/tech-news/discord-pushes-back-age-verification-rollout-backlash-rcna260604",
            "pub_date": "2026-02-25 23:16:32",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Plaintiff set to testify against tech companies in landmark social media addiction trial",
            "description": "<div>LOS ANGELES — The first plaintiff to take major social media companies to trial is expected to take the stand Wednesday to speak about the harms she says the tech platforms inflicted on her mental health as a child. The plaintiff, who is now 20 and is identified in court by her initials, K.G.M., is at the center of a bellwether case in Los Angeles County Superior Court that could set a legal precedent about whether social media platforms are responsible for causing mental health issues in children. The trial is the first in a consolidated group of cases brought against Instagram, YouTube, TikTok and Snap by more than 1,600 plaintiffs, including over 350 families and over 250 school districts. The plaintiffs accuse the tech companies of knowingly designing addictive products harmful to young users’ mental health. K.G.M., who was a minor at the time of the incidents outlined in her lawsuit, claims that her early use of social media led to addiction and worsened her mental health problems. Her lawsuit alleges that social media companies made deliberate design choices to make their platforms more addictive to children for purposes of profit. “Defendants know children are in a developmental stage that leaves them particularly vulnerable to the addictive effects of these features,” her lawsuit says. “Defendants target them anyway, in pursuit of additional profit.” Historically, social media platforms have largely been shielded by Section 230, a provision added to the Communications Act of 1934 that says internet companies are not liable for content users post. TikTok and Snap reached settlements with K.G.M. before the trial, but they remain defendants in a series of similar lawsuits expected to go to trial this year. The lawsuit highlights a variety of features that it claims the platforms use to “exploit children and adolescents,” including “an algorithmically-generated, endless feed to keep users scrolling,” rewards that encourage people to keep using the platform, and “incessant” notifications, as well as “inadequate” measures for age verification and parental control. “Disconnected ‘Likes’ have replaced the intimacy of adolescent friendships. Mindless scrolling has displaced the creativity of play and sport,” the lawsuit says. “While presented as ‘social,’ [the platforms] have in myriad ways promoted disconnection, disassociation, and a legion of resulting mental and physical harms.” So far during the trial, K.G.M.’s lawyers have called on Instagram head Adam Mosseri, Meta CEO Mark Zuckerberg and YouTube’s vice president of engineering, Cristos Goodrow, to testify before the jury. Mosseri argued in his defense of Instagram that although excessive use of the app could pose a problem, it’s “important to differentiate between clinical addiction and problematic use.” He added that while it is in Instagram’s business interests to attract as many users as possible, he believes “protecting minors in the long run is good for profit and business.” Zuckerberg echoed similar sentiments last week, saying Meta prioritizes building “a community that is sustainable” over boosting users’ screen time. “If you do something that’s not good for people, maybe they’ll spend more time [on Instagram] short term, but if they’re not happy with it, they’re not going to use it over time,” Zuckerberg said in his testimony. “I’m not trying to maximize the amount of time people spend every month.” Pressed about Meta’s age verification policies, Zuckerberg said that despite Instagram’s longtime policy prohibiting children under 13 from making accounts, he believes there are kids “who lie about their age in order to use the services.” Meta has developed measures over time to try to detect underage users, Zuckerberg said. But K.G.M.’s attorney, Mark Lanier, noted that the age verification features were not available when many children joined Instagram. K.G.M. got on the app at age 9. “I always wish we could have gotten there sooner,” Zuckerberg said of the safety tools Meta added in recent years. A spokesperson for Meta said in a statement that “the question for the jury in Los Angeles is whether Instagram was a substantial factor in the plaintiff’s mental health struggles. The evidence will show she faced many significant, difficult challenges well before she ever used social media.” K.G.M. was present during part of Zuckerberg’s testimony but did not speak. Goodrow, who took the stand Monday and Tuesday ahead of K.G.M.’s testimony, was pressed about YouTube’s self-proclaimed “big, hairy, audacious goal” of achieving 1 billion hours of daily watch time by the end of 2016. “YouTube is not designed to maximize time,” Goodrow said Monday. “Now we measure in time well spent.” YouTube’s CEO was scheduled to testify, as well, but was removed from the witness list last week. Lanier told NBC News that his team is “running out of time allowed by the court to put on our case.”</div>",
            "link": "https://www.nbcnews.com/tech/tech-news/social-media-addiction-trial-los-angeles-plaintiff-testimony-rcna260546",
            "pub_date": "2026-02-25 22:45:15",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "AI-linked fears roil some corners of Wall Street after years of hype and gains",
            "description": "<div>Stocks surged to records in large part because of hope — and hype — about artificial intelligence.  But in recent months, worries about aggressive spending on AI have rippled through Wall Street as investors question whether that spending will materialize into actual profits. And some industries wavered this week as anxieties about the technology intensified, underscoring how quickly sentiment has shifted since the start of the year.  Visa, Mastercard and IBM all fell sharply Monday, extending a broader bout of volatility across AI-linked names. Tuesday saw a modest bounce back across the markets as some software stocks also rebounded, thanks to new AI integrations announced by Anthropic. The benchmark S&P 500 index remains roughly flat for the year. Since November, AI-powered coding systems such as Anthropic’s Claude Code and OpenAI’s Codex have surged in capabilities and popularity among software developers. Using these tools, complex software packages and products can now be developed in minutes or days.  The most recent sell-off came after a grim and now-viral weekend substack post by Citrini Research warned of an eventual stock market crash, a sharp pullback in consumer spending and widespread white-collar layoffs by 2028 as a result of AI.  Payment companies such as Mastercard and American Express were hit particularly hard as traders contemplated a future with lower spending. Both companies were mentioned as potential victims of the AI rush in the Citrini post.  While Citrini has published analysis about AI for years, Sunday’s post stood out for its intricate, dramatic description of a world with widespread unemployment and reduced economic activity. On Monday, IBM suffered its worst, single-day drop since October 2000, during the height of the dot-com boom and bust. Part of the pressure followed Anthropic’s midday announcement that its Claude Code tool can be used to update a legacy computing language prized at IBM. Investors viewed the development as a potential threat to the kind of maintenance and modernization work that underpins IBM’s legacy business. Shares of Accenture and Cognizant Technology, two other consulting and professional service companies, fell in tandem.  The weakness in these individual stocks spilled over into the broader market. All three major indexes fell Monday, led by a more than 800 point drop in the Dow Jones Industrial Average. Five of the 11 S&P 500 sectors also closed in the red with financials and consumer discretionary leading the declines, down 3.3% and 2.2%, respectively.  It’s an irony not lost on Wall Street: The same force that’s fueled the tech sector’s explosive gains over the past two years is now driving investor hesitation. Mona Mahajan, head of investment strategy and asset allocation at Edward Jones, said investors have been quickly pulling money out of sectors viewed as vulnerable to AI disruption, including financial services, real estate, transportation and logistics. She noted that despite the sharp moves in stock prices, the underlying businesses themselves haven’t materially changed, underscoring how much of the reaction remains speculative. One standout in Monday’s sell-off was the consumer staples sector, home to companies such as Walmart and Coca-Cola, as investors put their money into stalwart businesses less likely to be disrupted by AI. While Anthropic and OpenAI have seen their valuations soar in the past year, investors covering industries ranging from logistics and trucking to legal services are waking up to the fact that traditional barriers to entry for competitors might soon be dissolved by AI’s coding power.  Melissa Otto, head of Visible Alpha research at S&P Global, told NBC News that growing AI-focused capital expenditure and recent advances in the field threatened many existing business models.  “I think it has started to call into question how exactly software companies are really going to compete and provide something superior in this environment,” Otto said, highlighting that companies with more complex workflows and troves of unique data stand a better chance of withstanding the coming AI storm. “Data that underlies generative AI that has predictive qualities and is proprietary, that can potentially really give firms an edge.” Beyond AI, markets are also contending with growing fears of war between the U.S. and Iran and the legal whiplash around President Donald Trump’s now-invalidated emergency tariffs. Still, some Wall Street analysts argue the latest trade developments could serve as a constructive distraction for U.S. markets in the near term. “Investors have been ruminating on a number of AI concerns in early 2026, with concerns about cash flow, capex levels, and whether a number of different industries will survive the AI era,” Lori Calvasina, head of U.S. equity strategy research at RBC Capital Markets, wrote in a note to clients Monday. “We generally have thought the existential concerns for industries other than software like wealth management and transportation/logistics have seemed overblown, and we think it would be healthy for the equity community to turn its attention to other topics for a while.” </div>",
            "link": "https://www.nbcnews.com/business/markets/ai-fears-stock-market-hype-gains-rcna260408",
            "pub_date": "2026-02-25 04:56:34",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Chinese AI companies 'distilled' Claude to improve own models, Anthropic says",
            "description": "<div>Three Chinese artificial intelligence companies used Claude to improperly obtain capabilities to improve their own models, the chatbot’s creator Anthropic said in a blog post Monday while also making a case for export controls on chips. The announcement follows a memo by OpenAI earlier this month, when the startup warned U.S. lawmakers that Chinese AI firm DeepSeek is targeting the ChatGPT maker and the nation’s leading AI companies to replicate models and use them for its own training. DeepSeek, Moonshot and MiniMax created more than 16 million interactions with Claude using roughly 24,000 fake accounts, in violation of Anthropic’s terms of service and regional access restrictions, the company said.  They used a technique called “distillation,” which involves training a less capable model on the outputs of a stronger one, Anthropic said. “These campaigns are growing in intensity and sophistication. The window to act is narrow, and the threat extends beyond any single company or region.” Anthropic warned that illicitly distilled models lacked necessary safeguards, creating significant national security risks. If these models are open-sourced, the risk multiplies as capabilities spread freely beyond any single government’s control. Anthropic, which raised $30 billion in its latest funding round and is now valued at $380 billion, said that distillation attacks support the case for export controls: Chip access restrictions reduce both direct model training capabilities and the extent of improper distillation. DeepSeek’s operation targeted reasoning capabilities across diverse tasks and the creation of censorship-safe alternatives to policy-sensitive queries, while Moonshot aimed at agentic reasoning and tool use, as well as coding and data analysis, Anthropic said. MiniMax targeted agentic coding, tool use and orchestration and Anthropic detected the campaign while it was still active — before MiniMax released the model it was training. “When we released a new model during MiniMax’s active campaign, they pivoted within 24 hours, redirecting nearly half their traffic to capture capabilities from our latest system,” the blog post said. DeepSeek, Moonshot and MiniMax did not immediately respond to requests for comment.</div>",
            "link": "https://www.nbcnews.com/world/asia/chinese-ai-companies-distilled-claude-improve-models-anthropic-says-rcna260386",
            "pub_date": "2026-02-24 18:36:51",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Facebook designed an app for teens called Bell but never launched it, court records reveal",
            "description": "<div>In 2018, as Facebook sought to expand its global footprint, the company considered launching a separate app for teens called Bell, which would have been built around their high schools, offering forums where students could discuss sports teams, school events or what they overheard in the hallway, a new court filing shows.   The company intended for Bell to become a central hub for teens within high schools across the United States and eventually across the world, where they could communicate with their classmates but not anyone outside their school. The strategy was to draw teens into the company’s ecosystem and then move them onto the regular Facebook platform once they graduated, according to a partially redacted April 2018 internal presentation, which was filed in federal court last week.  “High School communication is important to teens and important for us to win,” the presentation stated. Although the Bell app never launched, the internal plans demonstrate the importance that Facebook had placed on “winning” users before they turned 18, laying groundwork to keep them on the products over the long term. Do you have a story to share about technology in education? Contact reporter Tyler Kingkade A spokesperson for Meta, Facebook’s parent company, said the app was developed as an early exploratory idea, and it would have relied heavily on Facebook moderation teams to police the content. The spokesperson did not respond to a question about why the app never launched.  Plans for Bell were included among a large batch of exhibits filed late Friday by the plaintiffs as part of a sprawling lawsuit against the largest social media companies, including Meta. Hundreds of individual families, school districts and 33 state attorneys general accuse Meta, Google, ByteDance and Snap of designing addictive social media products and promoting them to minors, despite knowing about research showing harm to children’s mental health. “The social media addiction trials are providing a look behind the curtain and are proving that the status quo was even worse than we imagined,” said Sacha Haworth, executive director of the Tech Oversight Project, a nonprofit advocacy group pushing for more regulation of digital technology companies. “We simply have to do more to protect kids.” Meta and the other companies have broadly argued that there is no conclusively established link between social media use and mental health problems, and the platforms did not have a duty to warn the public about potential dangers. “We strongly disagree with these allegations and are confident the evidence will show our longstanding commitment to supporting young people,” Meta said in a statement. “For over a decade, we’ve listened to parents, worked with experts and law enforcement, and conducted in-depth research to understand the issues that matter most.”  Meta CEO Mark Zuckerberg argued last week in a Los Angeles courtroom that people stay on the company’s platforms because they find them helpful in communicating with peers. Meta has developed better age detection systems over the years, the company says, in an attempt to stop children under 13 from accessing its platforms. A Meta spokesperson pointed to features on its new Teen Accounts that are intended to give parents more control over their children’s social media use, and encourage young users to take breaks and pause notifications overnight. Meta has considered launching platforms aimed at children over the past decade. It paused plans to create a version of Instagram for children under age 13 in 2021 following pushback from parent safety groups. It also considered building a version of Facebook for children in 2017, but decided against it after parents reportedly provided negative feedback.  Facebook’s 2018 presentation on Bell shows how teen users would have been able to message anyone in their school, organize events on the platform and create class or club-based group chats similar to the apps Discord and Slack. Students also would have been able to post anonymous confessions, similar to the app YikYak, and Bell would have integrated with education technology products, such as Google Classroom. Once teens graduated from high school, according to the internal presentation, Bell would have provided “a smooth on-ramp” for them to import their information to Facebook. The data that Bell collected on students would later influence what showed up in their Facebook feeds.  The internal presentation cited surveys of high school students who identified their “must haves” in a social media app: communicating with classmates, watching videos and memes created by students and staying in the loop on happenings at their high school. Some of this already occurred on Facebook Groups and Messenger, and on Snapchat, but the company saw an opening to create a single app that “gathers everyone at school into one closed campus.” The company hoped that the Bell app would reach 80% of U.S. high schools by the end of 2020 and expand to Australia, Canada and European countries. Late last year, Australia enacted a ban on children under 16 from using social media.</div>",
            "link": "https://www.nbcnews.com/tech/social-media/facebook-designed-app-teens-bell-court-records-reveal-rcna260315",
            "pub_date": "2026-02-24 09:11:16",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Tech companies are making their robots cute to try and win over humans",
            "description": "<div>When the streets of Los Angeles flooded with rain last week, some of the city’s residents found themselves feeling sorry for a peculiar object: a food delivery robot floundering in water and debris. “She’s doing her best, you guys,” one social media user says in a video posted to Instagram showing a delivery robot struggling to drive onto a flooding curb. “Wait, I’m so sad. This is an empath’s worst nightmare.” In many major cities, the delivery robots taking over sidewalks bear facial expressions and names of their own. In turn, some observers have reacted with affection and sympathy for the machines as they trek along: They’re helping them navigate through debris, pushing crosswalk buttons for them, even wishing them luck on their journey. As AI-powered robots grow more common in households and public spaces, tech developers are racing to figure out how to make them appealing to humans. Lately, that's meant designing robots to have a cute, almost petlike appearance. “If you were a robot developer or designer, you would certainly not want your product to be threatening. You would want people to feel comfortable,” said Ellie Sanoubari, a robot designer and postdoctoral researcher focusing on human-robot interaction. “You would want to signal that it is friendly, that it is not going to harm anyone.” In the past, robots have typically been confined to factory environments where people needed technical expertise to operate them, Sanoubari said. Now, she predicts that a growing class of robots geared toward human interaction will become more prevalent in everyday spaces. That could lead to design choices such as larger heads, big eyes and the ability to make “cute” noises — all of which can evoke “deeply seated biological responses in us,” Sanoubari said. DoorDash, the largest food delivery service in the U.S., created its delivery robot Dot with that in mind. The autonomous vehicle, which launched in the fall, is built to navigate urban roads at speeds of up to 25 miles an hour. But it’s also designed in a way that fosters “human acceptance.” “As humans, we are social animals. We have dogs, we have cats, we have all kinds of pets,” said Ashu Rege, the vice president of autonomy for DoorDash. “And Dot and robots like Dot want to be part of that family, so to speak. I think they absolutely have some kind of character or persona.” He said the company built Dot to be round because studies have shown that humans tend to prefer rounded elements over boxy, square ones. Its big, circular eyes were another key design feature: Dot “looks” in the direction that it plans to steer, and it makes eye contact with pedestrians to signal for them to cross. The robot also makes sounds to announce its arrival, or simply to alert a nearby human to its presence on the sidewalk. Rege said he hopes such features help create acceptance and trust as people get used to Dot’s characteristics and learn to gauge its intent. DoorDash’s proprietary robot currently operates in the greater Phoenix area, with plans to expand. The company is not alone in trying to make its robots more humanlike. A California startup called Interaction Labs recruited Oscar-nominated “Toy Story” writer Alec Sokolow to help design its interactive lamp, called Ongo. The desk lamp takes the form of a wide-eyed robot that speaks in a cartoonish voice and moves like the Pixar lamp. Like a chatbot, Ongo learns about its human users over time and can act as a companion or an AI agent. But unlike a chatbot, it can also bounce up and down in excitement or physically peer over somebody’s shoulder. Sokolow, who leads Ongo’s creative design, said his team wanted to create a piece of physical tech that was “somewhere between a pet and a concierge.” “It’s like a character on ‘The Jetsons,’ if you know the old TV cartoon from the ’60s. It is definitely a desk lamp, but I also see it as a character,” he said. “I think the real thing that we’re trying to do is create a little personality.” But as AI agents take on a physical presence in robots, Sanoubari warned that the same risks of emotional dependency people face with chatbots could translate to robots too. Robotic AI-powered toys and petlike companions have already raised a flurry of concerns around data privacy, loose guardrails and inappropriate conversation topics for kids. “One of the things that we can do is to be very transparent about the machine nature of the technology, especially when we’re dealing with vulnerable populations like children, or when robots are being used for elderly care and a lot of these things,” Sanoubari said. She added that even robots that have no reason to be cute are often “cutesified” by their owners anyway, pointing to case studies of people naming and decorating their Roomba vacuum cleaners. “So humans are cute that way,” she said. “Humans develop their own attachment. They kind of assign meaning to things.” Memo, an anthropomorphic robot designed to handle household tasks like loading dishwashers and folding socks, has a look that’s reminiscent of Baymax, the beloved personal companion robot from Disney’s “Big Hero 6.” Fabian Fernandez-Han, the marketing lead for its California-based developer, Sunday Robotics, said the company wanted to give Memo a humanlike appearance without making it too realistic, which would create an “uncanny valley” effect that tends to creep people out. “I’ve heard some people say our design sort of looks Nintendo, or it kind of looks like a Lego human being, which is, I think, more akin to what we want,” Fernandez-Han said. “We want this robot to be perceived as robust enough that it’s not a toy, but it’s also cute enough that it’s never going to hurt you, and it’s somewhere in the middle. Nailing that is tricky.” As the robot prepares to enter beta testing later this year, Fernandez-Han noted that his team is experimenting with customization features that “add to the cuteness factor,” such as different-colored hats and other accessories. Brian Comiskey, the Consumer Technology Association’s senior director of innovation and trends, said many developers are trying to strike a balance between a robot’s responsiveness — its ability to quickly and reliably execute a task — and its physical cuteness. “By designing them with these cute, softer features, and even especially having eyes, a face and gestures that are similar to humans,” Comiskey said, “it allows humans, where we’re wired to read body language and faces and movements, to immediately start to attach to them a lot more quickly.” The CTA hosts the annual Consumer Electronics Show every January in Las Vegas. This year, the event featured more than 600 robotics exhibitors, including a robotic Labrador retriever named Jennie from the exhibitor Tombot, which designed it to resemble an emotional support animal for people. The CTA said it is tracking major growth in the consumer robotics industry, driven largely by advancements in AI. Right now, Comiskey noted, the software is powerful enough to drive serious acceleration in robot development. But the hardware is still catching up. “This is the intelligence decade, the 2020s. And I think for a long time we considered only the artificial intelligence and software portion of it,” he said. “I think, rightfully so, this back half of the decade will be defined by physical AI, which is the robotics portion of it.”</div>",
            "link": "https://www.nbcnews.com/tech/tech-news/tech-companies-cute-robot-designs-win-over-humans-rcna259818",
            "pub_date": "2026-02-22 23:06:47",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "India’s AI summit draws global leaders, big pledges and some chaos",
            "description": "<div>As the United States and China battle to dominate artificial intelligence, this week India attempted to highlight that there are other pathways to navigate the silicon surge. Billed as the first high-level AI gathering to be held in the Global South, the India AI Impact Summit has given the world's most populous country a stage to promote itself as a global AI player, broadening the AI conversation to include countries in Latin America, Africa and beyond. “Long term, it’s good for the world that AI is not just viewed as a race between the U.S. and China, and I think that India is right now the player that most confidently says, ‘We reject this dynamic,’” said Jakob Mökander, director of science and technology policy at the Tony Blair Institute for Global Change. As the event's \"impact\" branding suggests, the summit highlighted how countries can adopt and adapt increasingly powerful AI systems to their own needs and industries.  \"Every country will want to chart their own AI destiny,\" Michael Kratsios, director of the White House Office of Science and Technology Policy and the leader of the U.S. delegation at the summit, told NBC News. \"They each have unique characteristics about their culture, their language, their traditions, the way that they want to use AI.\"  As part of the event, Kratsios announced a series of initiatives to increase America's global engagement on AI, including an AI-focused Peace Corps program and new World Bank funding for countries to buy AI systems. The five-day summit in New Delhi, hosted by Prime Minister Narendra Modi, opened with its share of hiccups. With over 250,000 registered attendees, the summit was plagued early this week with complaints of overcrowding, long lines, visa issues and traffic disruptions, and there were some major no-shows, including a last-minute cancellation from Nvidia CEO Jensen Huang. Microsoft founder Bill Gates, who has faced questions over his ties to convicted sex offender Jeffrey Epstein, also pulled out hours before his keynote address on Thursday, saying in a statement that he wanted “to ensure the focus remains on the AI Summit’s key priorities.” Later in the day, U.S. rivals Sam Altman of OpenAI and Dario Amodei of Anthropic held hands with the people on either side of them but not with each other in what was supposed to be a show of unity with Indian Prime Minister Narendra Modi and other tech leaders. Earlier in the week, an Indian university was reportedly asked to leave the summit after a staff member passed off a robotic dog developed by Chinese company Unitree as one the university had developed. “The Modi government has made a laughing stock of India globally, with regard to AI,” the opposition Congress party said in a post on X, noting that India’s information technology minister had shared the erroneous report and then deleted it. But the summit still brought some wins for India, two of whose biggest conglomerates, Reliance and Adani, pledged a combined $210 billion in investment in domestic AI and data infrastructure (compared with the more than $630 billion that U.S. tech giants are expected to spend this year). OpenAI signed a partnership deal with the Mumbai-based Tata Group, while Anthropic announced one with Infosys and opened an office in its home city of Bangalore. “The solutions presented here — in agriculture, security, assistance for persons with disabilities, and addressing the needs of multilingual populations — are powerful examples of Made in India strength and India’s innovative capabilities,” Modi said in a speech Thursday. Attendees said that while this year’s summit was accessible to far more people than previous ones, they noted there were fewer of the top government and business leaders who actually make policy. The summit was attended by at least 20 heads of state and government, including French President Emmanuel Macron and Brazilian President Luiz Inácio Lula da Silva, along with tech executives including Google Chief Executive Sundar Pichai and Microsoft President Brad Smith. The world’s two biggest AI powers, the U.S. and China, did not send heads of state. For China, which has a contentious but warming relationship with India, the summit coincided with the Lunar New Year, the country’s biggest holiday.  As the highest-ranking U.S. official at the event, the White House's Kratsios emphasized the need for countries to eschew strict technocratic oversight of AI, echoing remarks from Vice President J.D. Vance at last year's gathering in Paris.  \"AI governance must focus on the particular needs and interests of particular people, and so it must be local,\" Kratsios said in a speech Friday. \"AI adoption cannot lead to a brighter future if it is subject to bureaucracies and centralized control.\" As part of his speech Friday, Kratsios announced the National Champions Initiative, which aims to help AI-related companies from partner nations create closer links with American AI ventures.  \"When we say that we're trying to export the American AI stack, it doesn't mean that it's 100% American content and nothing else is in there,\" Kratsios told NBC News. \"There are so many countries around the world that have great national AI champions themselves and that excel at certain layers in the AI stack.\" The Trump administration is highly interested in outcompeting China, which has long courted international partners on AI. On Friday, India formally joined Pax Silica, a U.S.-led international coalition launched in December aimed at building a resilient supply chain for critical minerals. The U.S.-China competition has countries such as India wary of being caught in the middle. Sriram Krishnan, senior White House policy adviser on AI, drew some backlash with his comments this week that the American AI stack should be the “bedrock” its allies build on. Critics said India should build its own foundational AI models to avoid being too dependent on the U.S. Though India is far behind the U.S. and China, which together control about 85% of global AI computing power, its digital public infrastructure — such as internet connectivity, digital payments and digital ID — is “better than most of the developed world,” Mökander, from the Tony Blair Institute, said. “They are quite proud about it, that it’s sort of a third way between China’s open-source and closed-source U.S. AI,” he said. The global AI summit series has evolved significantly since the first one was held in Bletchley Park, England, in 2023, attended by a small group of government and business leaders. While the first summit was focused mainly on the existential risks of frontier AI, this year’s event “takes a very expansive view of what safety means,” said Amlan Mohanty, a fellow at Carnegie India and adviser to the government on AI policy issues. “It’s no longer only about malicious use, or cybersecurity risks or national security risks,” he said. “It’s about, how can we ensure that we have sufficient data around economic transformation, jobs, impact on labor transitions to be able to make useful policy changes?” Mohanty said that thinking was reflected in two voluntary commitments made at the summit: one on using data to assess the economic impact of AI, and another on improving the performance of AI models across different languages and cultural contexts.  “It is critical to understand what works, what doesn’t, and who benefits so AI applications can be designed to maximize social benefits and mitigate unintended harms,\" said Iqbal Dhaliwal, global executive director of the Abdul Latif Jameel Poverty Action Lab at MIT.  “The majority of the world’s population lives in the Global South,” Dhaliwal said in emailed comments, noting that India represents a sixth of the global population. “Hosting the Summit here has allowed the conversation to center the issues and use cases that will affect these billions of people.”</div>",
            "link": "https://www.nbcnews.com/world/asia/indias-ai-summit-draws-global-leaders-big-pledges-chaos-rcna259855",
            "pub_date": "2026-02-21 04:21:58",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        }
    ]
}