{
    "data": [
        {
            "title": "AI chip made for Chinese company draws scrutiny over potential U.S. export violations",
            "description": "<div>Key components produced by a leading Taiwanese chipmaker were found in a powerful AI chip from a Chinese company, according to an initial report from a semiconductor research firm, which experts say could violate U.S. export controls. U.S. officials have been briefed on the research, according to a source familiar with the issue who requested anonymity to discuss private matters. The research details a breakdown of an AI chip produced for the Chinese company Enflame. After it took the Enflame processor apart, the research firm TechInsights said on its website, it found that key chip components had been manufactured by Taiwan Semiconductor Manufacturing Co. (TSMC). Over the past several years, the U.S. has imposed tight restrictions on what AI-related products can be sold to Chinese companies or companies operating in China, a move intended to maintain America’s lead in AI development. Chips provide the computing power necessary to create and run AI systems, and their limited supply has acted as a bottleneck for China’s AI industry. The trade rules, established by the Commerce Department, determine where and how a chip can be exported based on such metrics as the number of computing calculations it can perform per second and its computing power relative to its size. The Enflame S60 chip would be powerful enough to be subject to those export restrictions, TechInsights appeared to conclude, based on its initial analysis. The export control classification potentially makes the chip’s sale to Chinese AI companies illegal from late 2022 onward, experts say. According to several Chinese newspapers, at least one of which is state-affiliated, Enflame S60 chips were manufactured in 2024. TechInsights says it classifies export control details, also called ECCNs, based on information from manufacturers or third-party suppliers or on “information available at the time of analysis.” In the same disclaimer, TechInsights notes that its inferences and conclusions are not final regulatory decisions.  Shortly after NBC News sent its request for comment to TechInsights, the publicly viewable classification was changed to “TBD” before it was removed from the website entirely. “The technical analysis of the Enflame S60 are ongoing and not complete,” TechInsights spokesperson Graham Butler wrote in response to a request for comment. “We will temporarily place an ECCN based on the preliminary report findings. These are subject to change as reports are finalised.” TechInsights declined to provide the full report to NBC News. TSMC told NBC News that the classification originally applied by TechInsights was incorrect.  Since 2022, the U.S. has asserted that its export controls apply to certain AI-related “items made with the use of U.S. technology, software, or tools” destined for China, even if the items themselves are manufactured outside the U.S. That determination includes essentially all AI chips, given that American technology is used in the machines that manufacture chips.  Under the U.S. export rules, which have been expanded to apply to chips that are destined for countries beyond China to address smuggling fears, TSMC has been banned from shipping cutting-edge chips meant for AI purposes to Chinese companies without specific export licenses.  TSMC fabricates the majority of the world’s AI chips because of its expertise with the unique and intricate manufacturing techniques required to create semiconductor-dense chips.  “We have reviewed a certain third-party analysis report claiming a particular chip manufactured by TSMC is classified as a controlled AI chip,” a TSMC spokesperson wrote in reply to a request for comment about the new research regarding the Enflame S60.  “Based on its technical features and applications, this chip does not meet the criteria for classification as a controlled AI chip and the report has since been corrected, and the incorrect claim has been removed. TSMC is a law-abiding company, and we are committed to complying with all applicable rules and regulations, including applicable export controls.” Asked about TSMC’s claim that TechInsights’ classification for the Enflame chip was erroneous, a TechInsights spokesperson simply reiterated that the technical analysis was “ongoing and not complete.”  Enflame did not reply to requests for comment. TSMC has previously been accused of violating export controls related to manufacturing a chip found in AI processors from the leading Chinese tech company Huawei, according to Reuters.  In April, Reuters reported that the Commerce Department opened an investigation into TSMC over how its chips made for a company called Sophgo wound up in Huawei’s processors, which was already prohibited by export controls from receiving TSMC-made AI chips, and said TSMC could face a penalty of $1 billion or more. At the time, a TSMC spokesperson said that the company was committed to complying with the law and that it was cooperating with the Commerce Department.  When asked about the existence and status of the Huawei investigation, Commerce Department spokesperson Lauren Weber Holley wrote in an email: “The Department of Commerce does not comment on active enforcement matters or confirm or deny the existence of any pending investigations.” Asked about any potential investigation regarding the Enflame chip, Holley wrote: “The Department of Commerce does not comment on active enforcement matters or confirm or deny the existence of any pending investigations.”  Measures introduced in November 2023 further restricted the types of chips and countries to which semiconductor manufacturing companies using American technology can export chips meant for AI purposes. The Enflame S60 chip, manufactured by TSMC, is sufficiently powerful to make it subject to these tighter regulations, according to details previously viewable on TechInsight’s website. Jacob Feldgoise, a senior data research analyst at Georgetown University’s Center for Security and Emerging Technology who had access to separate data about the chip, said he was inclined to trust TechInsights’ initial determination that the chip is powerful enough to be subject to the rules. Because of intense global AI competition and fears that Chinese companies might divert advanced chips to the Chinese military, the Commerce Department rarely grants such licenses. According to a senior Commerce Department official, Enflame did not receive a license in 2023, 2024, or 2025. “If this is a large AI chip designed for data center use and produced at TSMC, that’s likely an export control violation similar to the Huawei-Sophgo incident,” said Lennart Heim, a leading semiconductor expert, specifying that he had not seen the chip and would need more details to reach a firm conclusion. The Commerce Department did not comment on the existence or status of the Huawei-related investigation or confirm whether that incident was an export control violation.  “Enflame is basically Tencent’s outsourced AI chip designer. It’s not some random independent startup — it’s closely tied to one of China’s largest tech companies,” Heim told NBC News in written comments. “This is probably a significantly smaller incident than Huawei-Sophgo in terms of how many chips were produced — but it shows the pattern of chips getting through TSMC that shouldn’t have.” According to the reports from Chinese newspapers, Enflame has deployed tens of thousands of S60 chips in data centers and similar infrastructure critical for AI development and usage. In recent months, the Trump administration has approved the sale of high-powered chips far exceeding the performance capabilities of the Enflame S60 chip to China. However, the State Department has reportedly slowed the approval process as it pushes for tighter rules on how the chips, like Nvidia’s H200 series, are used given national security considerations.</div>",
            "link": "https://www.nbcnews.com/tech/tech-news/ai-chip-tsmc-enflame-techinsights-rcna259342",
            "pub_date": "2026-02-27 23:25:53",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "South Korea set to finally get a fully functioning Google Maps",
            "description": "<div>SEOUL, South Korea — South Korea will soon no longer be one of the few countries where Google Maps doesn’t work properly, after its security-conscious government reversed a two-decade stance to approve the export of high-precision map data to overseas servers. The approval was made “on the condition that strict security requirements are met,” the Ministry of Land, Infrastructure and Transport said in a statement. Those conditions include blurring military and other sensitive security-related facilities, as well as restricting longitude and latitude coordinates for South Korean territory on products such as Google Maps and Google Earth, it said. The decision is expected to hurt Naver and Kakao — local internet giants that currently dominate the country’s market for digital map services. But it will appease Washington, which has urged Seoul to tackle what it says is discrimination against U.S. tech companies. “We welcome today’s decision and look forward to our ongoing collaboration with local officials to bring a fully functioning Google Maps to Korea,” Google Vice President Cris Turner said in a statement. South Korea, still technically at war with North Korea, had shot down Google’s previous bids in 2007 and 2016 to be allowed to export the data, citing the risks that information about sensitive military and security facilities could be exposed. Google Maps was not banned per se in South Korea and other countries have gone further. It is prohibited in mainland China, North Korea, Syria and Vietnam, for example. The data in question is 1:5000 scale data, where 1 centimeter on a map represents 50 meters in actual distance. Google has argued it needs to export the data to provide real-time navigation information worldwide. This includes people researching South Korean destinations from overseas. The conditions stipulate that Google must process map data on locally based servers and is only allowed to export data related to navigation and direction services that have been pre-approved by the government. The South Korean government also reserves the right to request revisions to maps, and Google must set up a security incident prevention framework to respond to emergency issues. Choi Jin-mu, a geography professor at Kyung Hee University, said the decision raised serious questions about market control and national security. “Google can now come in, slash usage fees, and take the market. If Naver and Kakao are weakened or pushed out and Google later raises prices, that becomes a monopoly. Then, even companies that rely on map services — logistics firms, for example — become dependent, and in the long run, even government GIS (geographic information) systems could end up dependent on Google or Apple. That’s the biggest concern.” Naver’s stock ended 2.3% down after the decision on Friday, though Kakao gained 1.5%. AMCHAM Chairman James Kim welcomed the decision, saying it “sends a positive signal about Korea’s commitment to innovation, open markets, and ensuring a level playing field for global companies operating in Korea.”</div>",
            "link": "https://www.nbcnews.com/world/asia/south-korea-set-finally-get-fully-functioning-google-maps-rcna260924",
            "pub_date": "2026-02-27 17:31:03",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Plaintiff in landmark social media trial testifies that apps affected her 'self-worth'",
            "description": "<div>LOS ANGELES — In a highly-anticipated testimony Thursday, the plaintiff in a landmark social media addiction trial said using apps like Instagram and YouTube when she was a child fueled her depression and anxiety, leading her to withdraw from her family. Now 20 years old, the plaintiff — identified in court documents by her initials, K.G.M. — detailed how her almost nonstop use of social media “really affected my self-worth.”  “They made me give up a lot of hobbies and old interests, and they prevented me from making friends ... [and] caused me to compare myself to other people,” she told jurors in Los Angeles County Superior Court.  “I just felt like I wanted to be on it all the time,” she said. “If I wasn’t on it, I was going to miss out on something.” K.G.M.’s trial is the first in a consolidated group of cases brought against Instagram, YouTube, TikTok and Snap by more than 1,600 plaintiffs, including over 350 families and over 250 school districts. The plaintiffs accuse the tech companies of knowingly designing addictive products harmful to young users’ mental health.  Historically, social media platforms have largely been shielded by Section 230, a provision added to the Communications Act of 1934 that says internet companies aren’t liable for the content users post. TikTok and Snap reached settlements with K.G.M. before the trial, but they remain defendants in a series of similar lawsuits expected to go to trial this year. K.G.M.’s bellwether case could set a legal precedent for whether social media platforms are liable for mental health issues in children. If the jury’s verdict favors K.G.M., the companies could face damages to be determined by the jury and forced to change the designs of their platforms. The verdict could also set the tone for whether they choose to fight or settle the oncoming cases. Meta, which owns Instagram and Facebook, and Google, which owns YouTube, have denied that their apps are purposefully harmful and addictive for young users.  In court, K.G.M. elaborated on the claims that the companies made deliberate design choices to make their platforms more addictive to children for purposes of profit. Her complaint highlights a variety of features that it argues the platforms use to “exploit children and adolescents,” including “an algorithmically-generated, endless feed to keep users scrolling,” rewards that encourage people to keep using the platform and “incessant” notifications, as well as “inadequate” measures for age verification and parental control. K.G.M. said she created her Instagram account at age 9, before the app asked new users to enter their birthdays. Before that, she said, she also lied about her age to create a YouTube account without having read through any of the platform’s legal fine print.  As she was growing up, K.G.M. told jurors, being without her phone often sent her “into a panic.” “Without it, I felt like a huge part of me was missing,” she said. “If I didn’t have it, I would be missing out on something. I couldn’t see who was liking my stuff.” Though she and her mother often argued about her phone use, she couldn’t put it down. She said she would get “a rush” every time she got a notification about her social media posts or profiles and subsequently felt so compelled to check them that she’d sneak away to the bathroom or put off sleep at night.  “When I got a bunch of likes, I was really happy,” she said. “If I didn’t get a lot of likes, I would feel I shouldn’t have posted it, I was ugly.” With YouTube, K.G.M. said, she started “at a young age” and “spent all my time on it.”  “I would watch it in class,” she said. “Any time I tried to set limits for myself, it just didn’t work, and I just couldn’t get off.” K.G.M. said that because of her social media use, she still struggles with body dysmorphia. She said she began experiencing it after she was exposed to social media filters, which often overlay effects like skin smoothing or makeup.  Meta introduced beauty filters to Instagram stories in 2017. In 2019, it significantly expanded its slate of augmented reality filters, allowing users to make and publish their own.  K.G.M. said she now tries to avoid filters “because I know I’ll feel worse if I use them.” Meta has pushed back against claims that the design of social media platforms is responsible for K.G.M.’s mental health challenges as a child, arguing in a brief filed Wednesday that she faced other issues at home that contributed to her mental state. Its filing pointed to “numerous examples of ‘emotional abuse and neglect by [Plaintiff’s] mother, including prolonged periods of the silent treatment, frequent name-calling (e.g., ‘dumb,’ ‘stupid’), and mocking of her voice,’ and ‘physical abuse, including hitting the plaintiff.’” (In court, K.G.M. disputed Meta’s argument that her family contributed to her mental health struggles, telling jurors she didn’t experience “abuse or neglect or anything like that.”) Pressed about social media addiction, Instagram head Adam Mosseri, who testified earlier this month, said, “I think it’s important to differentiate between clinical addiction and problematic use.” Mosseri also touched on filters, saying the platform eventually decided to prohibit “effects promoting plastic surgery.” YouTube’s vice president of engineering, Cristos Goodrow, who took the stand Monday and Tuesday, also emphasized that the video platform is “not designed to maximize time.”  On Wednesday, Victoria Burke, who was K.G.M.’s therapist when she was 13, testified that she doesn’t think social media was the sole driver of her former client’s mental health issues but that she does suspect it played a part. “I believe it was a contributing factor, not a causation factor,” she said.</div>",
            "link": "https://www.nbcnews.com/tech/tech-news/social-media-addiction-trial-plaintiff-testifies-depression-anxiety-rcna260851",
            "pub_date": "2026-02-27 08:51:26",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "AI giant Nvidia made $120 billion in profit last year. Investors are still spooked.",
            "description": "<div>Nvidia — the most valuable company in the world thanks to its place atop the AI food chain — generated a staggering $120 billion in profits last year. This includes an eye-popping $43 billion during the three-month period ending in January, one of the strongest quarters of any business ever recorded.  Investors didn’t seem to care.  Shares of Nvidia fell more than 5% Thursday, following the results. With a total market value of roughly $4.5 trillion, the company’s one-day loss amounted to roughly $256 billion worth of market capital.  Nvidia’s stock price decline is part of a broader phenomenon dubbed the  AI “scare trade,” that is percolating in certain corners of the stock market. This bearish play threatens the very driver that has powered broader, double-digit gains across the benchmark S&P 500 for the past two years.  And while the stock market might look broad, its gains are increasingly concentrated in just a handful of mega-cap names, including Nvidia. In other words, the entire market’s performance is heavily tied to the performance of these select companies.  Nvidia says its growth story is very much still intact.  “We have now seen the inflection of agentic AI and the usefulness of agents across the world and enterprises everywhere,” Nvidia CEO Jensen Huang said during the company’s quarterly earnings call Wednesday, referring to AI chatbots like OpenAI’s ChatGPT and Anthropic’s Claude.  “You’re seeing incredible compute demand because of it,” he said. “In this new world of AI, compute is revenues.”   Compute refers to the processing power that is needed to train and operate AI models. Nvidia’s chips, each of which is around 30 inches square, underpin the massive data centers needed to run AI chatbots and agents.  Nvidia’s dominance in the AI chip race also means that more companies than ever are dependent upon its products, at a time when AI is evolving faster than even its early adopters say they could have imagined. Over the past five years, this has sparked a massive investor rush to buy a piece of Nvidia, whose shares have surged nearly 1,300% since the start of 2021. Driven by a mixture of FOMO and faith in AI’s growth-at-any-cost business model, these investors and others like them have piled into virtually any company with even a tangential relationship to the AI industry.  All this time, Nvidia has led the charge. But so far this year, Nvidia’s share price is barely positive. Some firms, including HSBC, have argued that in order to justify another leg higher in the company’s stock price, Nvidia needs a “new narrative,” such as a meaningful expansion in AI demand or pricing power, to justify another leg higher in the stock. But more broadly, the AI scare trade visited upon Nvidia Thursday underscores a growing unease around the future of AI.  After a multi-year boom for public and private companies alike, AI is now facing tougher scrutiny. Questions persist around whether or not the AI boom is starting to look more like a bubble. Likewise, investors are uncertain whether AI can generate the kind of near-term returns necessary to justify the massive investments — and soaring share prices —  coursing through the tech world. “Artificial intelligence stands to become one of the most consequential technologies in generations, if not in the history of humankind, with enormous implications for the economy,” Moody’s economist Mark Zandi wrote in a new report Wednesday. “However, the specifics of how it will shape the future remain highly uncertain and are the subject of immense debate.” That debate includes growing concerns over how AI agents will affect vulnerable industries like cybersecurity and software — and potentially upend traditional business models that have worked for decades.  Shares of software companies like ServiceNow and Synopsis have fallen sharply amid those fears, declining roughly 20% and 15% over the past month, respectively. Salesforce is down nearly 25% this year. So far this year, companies in the software industry have been the largest drag on the S&P 500. AI has “started to call into question how exactly software companies are really going to compete and provide something superior in this environment,” Melissa Otto, head of Visible Alpha research at S&P Global, told NBC News. Nvidia’s Huang attempted to push back on this narrative in an interview with CNBC on Wednesday.  The “markets got it wrong” when it comes to the AI-driven panic around software, he said. Huang argued that AI will enhance productivity and expand what software can do, rather than kill the whole industry.  Huang’s attempt to assuage investor fears didn’t move the needle much, though. On Thursday, the tech-heavy Nasdaq fell nearly 1.5% on the back of Nvidia’s slide. Software giants like Synopsys dropped 5% while shares of Microsoft and Alphabet also traded lower.  Beyond software, investors are contending with other existential anxieties. Many of them were captured this week in an essay posted on Substack by a small research firm called Citrini Research. The post warned that AI adoption would lead to a stock market crash, a sharp pullback in consumer spending and widespread white-collar layoffs by 2028. The report painted a vivid picture of an economic doomsday scenario caused by AI, effectively animating investors’ vague, simmering fears. Payments giants like Mastercard and American Express were hit particularly hard after the post named them as potential casualties in a lower-spending, AI-disrupted economy. Shares of the two payment giants rebounded slightly on Thursday.  Many Wall Street analysts say it’s too soon to panic.  “While we take concerns about the AI trade and private markets and other matters seriously, we think it’s premature to assume that’s the kind of risk we face today,” Lori Calvasina, head of U.S. equity strategy research at RBC Capital Markets, wrote in a client note earlier this month.  Kristy Akullian, Blackrock’s head of iShares investment strategy for the Americas, added in a separate note Thursday that the recent sell-off “is predicated on still uncertain existential risk,” rather than any immediate changes to company earnings or business fundamentals.  Nonetheless, this existential risk is one that investors are taking more seriously now than they did six months ago. </div>",
            "link": "https://www.nbcnews.com/business/markets/nvidia-made-120-billion-profit-last-year-investors-are-worried-rcna260839",
            "pub_date": "2026-02-27 06:10:48",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "What the polls say about how Americans are using AI",
            "description": "<div>Americans are increasingly encountering and using artificial intelligence technologies like Anthropic’s Claude, OpenAI’s ChatGPT and Google’s Gemini. Most surveys now suggest more than two-thirds of the population is interacting with these technologies. However, a defining question is how far along we are in a transition from experimentation with AI to integration into daily personal and professional life. How many Americans are using it regularly? For what purpose? And what value are they extracting? One compelling way to track this adoption of AI is via Gallup polling, which found that 12% of Americans now report using AI daily at work. While the number seems modest, it represents a threefold increase in just over a year, from 4% in mid-2024. Employees in the knowledge-work sector are leading this change. In the technology industry, 31% of workers reported daily usage. That’s compared to 19% of finance industry workers and 16% of employees in the professional service industry. Gallup also identified the ability to work remotely as an indicator of AI adoption. Daily usage among remote-capable jobs sits at 19%, more than double the 7% daily adoption rate in roles requiring an on-site or physical presence. For historical perspective, Pew Research Center tracked internet adoption in the early aughts. In 2000, 14% went online daily for job-related research. In 2001, the number jumped up to 19%. While AI usage by white-collar workers is more pronounced than among blue-collar workers, pessimism regarding AI’s impact on the labor market cuts across the typical dividing line for social and cultural issues.  An Economist/YouGov poll found 63% of American adults thought that advances in AI would lead to an overall decrease in jobs. There was little difference in opinion based on education: 67% of those with college degrees and 61% of those without degrees shared this concern. This pessimism does appear to be greater than concerns about computers in the workplace at the turn of the century. In 1999, a NPR/Kaiser/Harvard Technology survey found that 32% thought computers would lead to a decrease in jobs, while 43% thought they would lead to an increase and 23% thought computers wouldn’t make a difference. That means today’s anxiety about AI is nearly twice as high as the computer anxiety of the late ’90s, as overall attitudes toward AI seem to resemble the mix of skepticism and curiosity seen during the early rise of the internet. As use of AI spreads, another question is how people are actually using it. OpenAI released a study last year that estimated 30% of ChatGPT usage was work-related and 70% was personal. The study categorized ChatGPT usage into three main behaviors. There was “asking” (49%), or prompting the AI for advice or information on a specific topic. “Doing” (40%) involved practical tasks such as drafting text or writing code. And “expressing” — chatting or playing — accounted for 11% of usage. Writing and seeking guidance are among the leading practical uses of ChatGPT, according to OpenAI. When specifically looking at the workplace, the study found technical help and writing become even more dominant uses. Still, a majority of respondents (53%) in a recent Fox News poll said AI had not yet made a significant difference in their lives, while 26% said it had personally helped them and 20% believed it had caused them harm. With daily AI use in the workplace tripling in a year, it's possible those numbers could shift even more rapidly in the months ahead, as the technology moves from the periphery and toward the center of American life.</div>",
            "link": "https://www.nbcnews.com/politics/politics-news/polls-say-americans-are-using-ai-more-work-personal-rcna260422",
            "pub_date": "2026-02-27 01:41:26",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Instagram will alert parents to teens' repeated suicidal or self-harm searches",
            "description": "<div>Instagram will begin notifying parents when their teenagers repeatedly attempt to search for suicide or self-harm content, the company said Thursday. The new feature will alert parents enrolled in the parental supervision tool to repeated searches in a short time period and offer expert resources about how to talk to their teens about the issue. The function will start to roll out in the coming weeks in Australia, Canada, the United Kingdom and the United States, with more countries to come later this year.  “The vast majority of teens do not try to search for suicide and self-harm content on Instagram, and when they do, our policy is to block these searches, instead directing them to resources and helplines that can offer support,” the company said in its announcement. “These alerts are designed to make sure parents are aware if their teen is repeatedly trying to search for this content, and to give them the resources they need to support their teen.” The move comes as Instagram’s parent company, Meta, and other social media platforms face ongoing scrutiny over the safety of their products, particularly for young people.  In Los Angeles, a consolidated group of cases with more than 1,600 plaintiffs, including more than 350 families and over 250 school districts, are accusing Instagram, YouTube, TikTok and Snap of deliberately designing platforms to be addictive to young users. Last week, Meta CEO Mark Zuckerberg said in court that Instagram is meant to build “a community that is sustainable” and not designed to addict young users. TikTok and Snap settled ahead of the trial. Meta, and in particular Instagram, have taken some steps to address concerns around the use of its platforms by teens. In 2024, Instagram introduced accounts specifically for teens meant to restrict who can contact them. In October, the company said it would overhaul its approach to teens' accounts, limiting their access to certain content in an attempt to make the experience closer to viewing PG-13 movies.  Instagram already blocks content related to suicide or self-harm from reaching teens’ accounts. However, families of teens who died by suicide allege in their lawsuits that Instagram is responsible for multiple sextortion scams targeting teens, NBC News previously reported. Meta spokesperson Sophie Vogel told NBC News that teens can also talk to Instagram’s existing artificial intelligence tool later this year to seek support, and parents will also be notified of conversations related to suicide or self-harm. If you or someone you know is in crisis, call or text 988 or go to 988lifeline.org to reach the Suicide & Crisis Lifeline. You can also call the network, previously known as the National Suicide Prevention Lifeline, at 800-273-8255 or visit SpeakingOfSuicide.com/resources.</div>",
            "link": "https://www.nbcnews.com/tech/social-media/instagram-will-alert-parents-teens-repeated-suicidal-self-harm-searche-rcna260789",
            "pub_date": "2026-02-27 00:50:55",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Florida Gov. Ron DeSantis leans into AI skepticism, seeking a contrast with Vance",
            "description": "<div>Florida Gov. Ron DeSantis isn’t sold on the massive expansion of AI.  And that belief might be his way back to national political relevance. The Republican governor is appealing to a growing number of people who have concerns that AI’s rapid build-up, fueled in part by taxpayer dollars, could displace jobs, increase energy costs and hurt the environment. DeSantis’ positions stand in direct contrast to the embrace of the AI industry by President Donald Trump and the two likeliest potential candidates to snag his 2028 presidential endorsement: Vice President JD Vance and Secretary of State Marco Rubio. “We don’t want to see them building a massive data center and then sending you the bill,” DeSantis said this month when asked about AI companies. “Data centers take up the power equivalent of a half a million-person city. We feel very, very strongly about protecting the consumer.” For DeSantis, the embrace of AI skepticism is rooted in both personal policy preference and a 2028-focused political calculation as the term-limited governor plots out his political future, according to eight sources, most of whom have either worked in his administration or for his past campaigns at both the state and national levels. Many of them requested anonymity to speak candidly. “It’s kind of a no-brainer, right? You’ve got JD Vance and Marco Rubio, the top two contenders for 2028 big time in the pro-AI lane,” a longtime DeSantis adviser said. “The infrastructure is lining up behind JD and to some extent Marco. So, DeSantis’ challenge is to stay relevant.” Taryn Fenske, a DeSantis political aide, said the governor is a skeptic because of AI’s potential societal dangers. “The governor is an AI skeptic because chatbots are convincing children to commit suicide,” she said.  DeSantis also leads a state the AI industry is likely to target, making it a focus of the broader fight between AI skeptics and proponents.  NBC News reported this month that Leading the Future, a pro-AI super PAC, is spending $5 million on TV ads boosting Republican Rep. Byron Donalds' Florida gubernatorial campaign. Donalds is the only state-level politician the group has spent money to help, and officials there said it's an indication AI companies will look to expand their footprint there in the future. Recent polling suggests that AI could be a pertinent issue in the upcoming midterm elections and the 2028 presidential race. A poll conducted this month by The Economist and YouGov found that 63% of the U.S. citizens surveyed — including 60% who voted for Trump in 2024 — believe that advances in AI will reduce the number of jobs available in the country. A plurality of respondents, 33%, said that AI would have a “more negative than positive” impact on the U.S. economy. And a Morning Consult poll in November found that a 41% plurality of registered voters favored banning the construction of data centers near their homes; 36% opposed such a ban, while 22% said they didn’t know or had no opinion. DeSantis and the Trump administration have already been at odds on the issue.  Trump’s AI czar David Sacks and other administration allies have directly lobbied against DeSantis’ push to get Florida’s GOP-dominated Legislature to implement state-level regulations on AI and the massive data centers needed to accommodate the industry’s boom. Those bills remain alive, but as Florida’s legislative session comes to a close, their passage has become increasingly unlikely. “There are some people … who almost relish in the fact that they think this just displaces human beings and then, ultimately, you’re going to have AI run society, and that you’re not going to be able to control it,” DeSantis said at an AI roundtable earlier this month. “Count me out on that.” DeSantis and Trump themselves have publicly buried the hatchet, including golfing together this month, after a brutal 2024 GOP presidential primary. But as the jockeying begins to become the first post-Trump Republican nominee for president, it does not mean the notoriously politically sharp-elbowed DeSantis will not look to use the fight over AI regulation as a political cudgel against Trump allies such as Vance and Rubio. “You know the story of the scorpion and the frog? Ron DeSantis is looking for his moment to stab the White House on something, and that might very well be AI,” said a longtime DeSantis political adviser who currently represents AI industry clients.  “And you know why he’s going to do it?” the person added. “Because he’s Ron DeSantis. It’s what he does.” During his State of the Union address Tuesday night, Trump said his administration had struck a deal with major tech companies to require them to pay for more of the energy costs associated with building massive data centers.  “We’re telling the major tech companies that they have the obligation to provide for their own power needs — they can build their own power plants as part of their factory, so that no one’s prices will go up — and in many cases, prices of electricity will go down for the community,” the president said. For now, DeSantis’ AI strategy would likely to some degree center on Vance, who in most public polling has been the overwhelming leader for the 2028 Republican presidential nomination, even as Rubio has picked up momentum in recent weeks. Vance, with his background in Silicon Valley venture capital and relationships with leading Big Tech figures, is known as one of the Republican Party’s biggest AI champions.  At last year’s Artificial Intelligence Action Summit in Paris, the vice president warned that excessive regulation “could kill a transformative industry” and pledged the Trump  administration’s support for “pro-growth AI policies.”  At the same time, Vance has attempted to reassure those on both sides of the debate.  Last fall, in an interview with Fox News’ Sean Hannity, Vance compared a rise in AI-related jobs to the arrival decades ago of cash-dispensing ATMs. Human bank tellers, Vance noted, still exist. He also linked his pro-AI stance to his anti-immigration views. Using home construction as an example, Vance described robots as complementary to “blue-collar” workers and immigrant laborers as an outright threat to replace them on job sites. “No robot can replace a great blue-collar construction worker,” Vance said. “You see some of the houses, some of the things they do, the trim that they’re able to do. There’s an art there that I don’t think a robot is ever going to be able to replace.” Vance has more recently expressed concerns about AI, telling Fox News’ Martha MacCallum last week that he worries specifically about it being used to surveil Americans or to advance invasions of privacy and political bias. In that same interview, Vance also illustrated the differences between him and DeSantis when it comes to regulating AI. DeSantis has pushed for state-level regulation, while Vance, along with the Trump administration more broadly, has supported the industry-backed idea of Congress passing a national regulatory bill. “I think that eventually you’re going to have some standard applied, whether it’s a federal standard or whether it’s one state standard dominating,” Vance told MacCallum when pressed for his thoughts on regulation. “I think, frankly, the worst possible outcome would be to have far left California dominate the entire AI regulatory map.” Representatives for Vance did not return requests for comment. An official close to the DeSantis administration said that the governor sees an opportunity with the growing number of average people who feel elbowed out and negatively impacted by AI’s growth. “Look at all these many trillions of dollars being spent on AI and data centers. They have no clue how it will ever benefit them,” the person said. “This stuff is more for enterprises than the individual person, and they see it as something that will jack up costs and replace them.” “For DeSantis’, it’s a populist play,” the person added. “And that’s perfect for him.” The former official  said there are, to some extent, broader geopolitical concerns to consider as the U.S. engages in an “arms race” with China over AI expansion — and the almost inevitable change to the modern world that fight will usher in.  “The reality is this is an arms race, this is a cold war arms race against China dumping tons of money into AI,” the person said. “If you are on Team Trump, the only way you dig out of debt right now is to radically enhance productivity and grow production.” “International investors I talk to are perplexed on how anyone in the U.S. could be anti-AI right now,” the person said. “That just makes things easier for China.” Across the country, people are heavily pushing back on efforts to put massive data centers in their cities and towns. Most national polling is favorable to building data centers, but the numbers crater when people are asked if they would like one in their backyard. “Nationally, when you talk about data centers, it polls at roughly 60%,” said an adviser who works with pro-AI groups. “But when you say to people in Loudoun County, if they want one in Loudoun County, the numbers are really, really bad,” the person said, referring to the Virginia county. “Nationally it all sounds good and gravy,” the adviser added. “But when you get to the local stuff, you’re f-----.”</div>",
            "link": "https://www.nbcnews.com/politics/2028-election/florida-gov-ron-desantis-ai-skepticism-contrast-vance-rcna258824",
            "pub_date": "2026-02-26 18:05:58",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Anthropic says U.S. military can use its AI systems for missile defense ",
            "description": "<div>In contract negotiations between senior Defense Department officials and leaders from AI giant Anthropic in December, the company agreed to allow the U.S. government to use its AI systems for missile and cyber defense purposes, a person familiar with the matter said, requesting anonymity to speak about private discussions. But that apparently did not satisfy the Pentagon. Following weeks of tension between the Defense Department and Anthropic over the company’s restrictions on how its products can be used by the military, Defense Secretary Pete Hegseth issued a stark ultimatum to company CEO Dario Amodei on Tuesday: Allow the AI technology to be used for all legal military purposes by this Friday or be forced to cooperate, a senior Pentagon official told NBC News. The ultimatum, detailed to NBC News by a senior Pentagon official, comes as Anthropic — a company that has heavily marketed its focus on AI safety — tries to maintain firm policies preventing its systems from being used for mass domestic surveillance or direct use in lethal autonomous weapons. The December contract changes would allow for its systems to be widely used for cyber and missile defense, according to the person familiar with the matter. An Anthropic spokesperson told NBC News in a statement that “Every iteration of our proposed contract language would enable our models to support missile defense and similar uses.”  But the company’s insistence on guardrails have continued to be a source of contention between Anthropic and the Defense Department. According to the senior Pentagon official, representatives from the department, including Undersecretary of Defense Emil Michael, recently discussed several hypothetical scenarios with Anthropic leadership about how the company’s products might be employed by the military.  As part of those discussions, the officials discussed how Anthropic’s systems might be used if an adversary launched an intercontinental ballistic missile at the U.S. According to the Pentagon source, the officials discussed whether Anthropic’s guardrails might somehow block a U.S. response to the launch. Anthropic officials said they could be called on to lift those restrictions, according to the official, but Pentagon leadership was not fully satisfied with Anthropic’s adjustments and did not want to be beholden to the private company.  According to an Anthropic spokesperson, any suggestion that CEO Amodei said the Pentagon would have to call the company in each missile defense operation is “patently false.”  In the latest escalation in negotiations, during Tuesday’s meeting Pentagon leaders said they could invoke the Defense Production Act to force Anthropic to comply with the Pentagon's rules, according to the senior Pentagon official. The Act allows the president to control domestic companies critical to national security in times of need. In Tuesday’s meeting, Pentagon leadership also invoked threats to instead label Anthropic as a “supply chain risk” and ban all defense business with the company if it does not align its terms of service for certain high-stakes uses with the Pentagon by Friday, the source said.  “Anthropic has until 5:01pm Friday to get on board with the Department of War,” the senior Pentagon official said of the ultimatum in a statement provided to NBC News, responding to questions about the meeting. “If they don’t get on board, the Secretary of War will ensure the Defense Production Act is invoked on Anthropic, compelling them to be used by the Pentagon.” “Additionally, the Secretary of War will also label Anthropic a supply chain risk,” the official said. Asked about Tuesday’s meeting, an Anthropic spokesperson said in a statement: “Dario expressed appreciation for the Department’s work and thanked the Secretary for his service. We continued good-faith conversations about our usage policy to ensure Anthropic can continue to support the government’s national security mission in line with what our models can reliably and responsibly do.” Hegseth complimented Anthropic’s products and said the Pentagon wanted to work with Anthropic, according to another person familiar with the meeting, who requested anonymity to speak candidly. The person confirmed that the department said it would terminate Anthropic’s work with the Pentagon by Friday if it did not agree to its terms. According to reports from The Wall Street Journal and Axios, Anthropic’s Claude systems were used during the operation to capture Venezuelan President Nicolás Maduro in January. It is unclear exactly how the systems were used. Hegseth sent a memo to senior Pentagon officers Jan. 9 announcing the Pentagon’s drive toward an “AI-first warfighting force.” He outlined a push to use AI models, like Anthropic’s, for all legitimate military purposes, “free from usage policy constraints” set by individual AI companies.  Anthropic is the only AI company whose products are actively used on classified networks, through its contract with Palantir, a data analytics company. A senior Pentagon official confirmed to NBC News that xAI reached a deal with the Pentagon on Monday to use its Grok chatbot system on classified networks, agreeing to allow its systems to be harnessed for “any lawful use” as Hegseth desired. Anthropic was one of four AI companies — the others were OpenAI, Google DeepMind and xAI — to get contracts worth up to $200 million in July to “prototype frontier AI capabilities that advance U.S. national security.”</div>",
            "link": "https://www.nbcnews.com/tech/security/anthropic-pentagon-us-military-can-use-ai-missile-defense-hegseth-rcna260534",
            "pub_date": "2026-02-26 02:00:58",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Discord pushes back age verification rollout following backlash",
            "description": "<div>Discord, the popular online communication platform for gamers, said Wednesday it will delay its global age verification rollout after receiving user criticism. Earlier this month, Discord announced a phased rollout for new and existing users that would implement video selfies to determine a person's age group. Users could also submit a form of identification to their vendor partners.  The rollout was supposed to begin in early March and would give underage users a \"teen-appropriate experience\" that included updated communication settings, content filtering and restricted access to age-gated spaces.  Criticism was immediate, with many users pointing to an October security breach of a third-party provider Discord used that exposed government ID photos for thousands of users, The Associated Press reported.  The platform’s chief technology officer responded to the backlash in an update Tuesday, saying that Discord \"missed the mark\" and that the rollout is being delayed to the second half of 2026.  \"Let me be upfront: we knew this rollout was going to be controversial. Any time you introduce something that touches identity and verification, people are going to have strong feelings. Rightfully so. In hindsight, we should have provided more detail about our intentions and how the process works,\" Discord CTO Stanislav Vishnevskiy wrote in a Wednesday blog post.  Vishnevskiy said the platform will not require face scans or ID uploads from everyone, and that over 90% of users will never need to verify their age to continue using it.  \"If you’re among the less than 10% of users who do need to verify, we’ll give you options, designed to tell us only your age and never your identity,\" Vishnevskiy said.  If a user chooses not to verify their age, they can keep their account, servers, friends list, messages, and voice chat, but won't be able to access age-restricted content or change certain safety settings.   Vishnevskiy also addressed last year's security breach, saying that Discord no longer works with that vendor.  \"We’ve made mistakes. I won’t pretend we haven’t. And I know that being a bigger company now means our mistakes have bigger consequences and erode trust faster. I don’t expect one blog post to fix that,\" Vishnevskiy said.  \"We’re listening. We’ll get this right. And when we ship, you’ll be able to see for yourselves,\" he added.</div>",
            "link": "https://www.nbcnews.com/tech/tech-news/discord-pushes-back-age-verification-rollout-backlash-rcna260604",
            "pub_date": "2026-02-25 23:16:32",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Plaintiff set to testify against tech companies in landmark social media addiction trial",
            "description": "<div>LOS ANGELES — The first plaintiff to take major social media companies to trial is expected to take the stand Wednesday to speak about the harms she says the tech platforms inflicted on her mental health as a child. The plaintiff, who is now 20 and is identified in court by her initials, K.G.M., is at the center of a bellwether case in Los Angeles County Superior Court that could set a legal precedent about whether social media platforms are responsible for causing mental health issues in children. The trial is the first in a consolidated group of cases brought against Instagram, YouTube, TikTok and Snap by more than 1,600 plaintiffs, including over 350 families and over 250 school districts. The plaintiffs accuse the tech companies of knowingly designing addictive products harmful to young users’ mental health. K.G.M., who was a minor at the time of the incidents outlined in her lawsuit, claims that her early use of social media led to addiction and worsened her mental health problems. Her lawsuit alleges that social media companies made deliberate design choices to make their platforms more addictive to children for purposes of profit. “Defendants know children are in a developmental stage that leaves them particularly vulnerable to the addictive effects of these features,” her lawsuit says. “Defendants target them anyway, in pursuit of additional profit.” Historically, social media platforms have largely been shielded by Section 230, a provision added to the Communications Act of 1934 that says internet companies are not liable for content users post. TikTok and Snap reached settlements with K.G.M. before the trial, but they remain defendants in a series of similar lawsuits expected to go to trial this year. The lawsuit highlights a variety of features that it claims the platforms use to “exploit children and adolescents,” including “an algorithmically-generated, endless feed to keep users scrolling,” rewards that encourage people to keep using the platform, and “incessant” notifications, as well as “inadequate” measures for age verification and parental control. “Disconnected ‘Likes’ have replaced the intimacy of adolescent friendships. Mindless scrolling has displaced the creativity of play and sport,” the lawsuit says. “While presented as ‘social,’ [the platforms] have in myriad ways promoted disconnection, disassociation, and a legion of resulting mental and physical harms.” So far during the trial, K.G.M.’s lawyers have called on Instagram head Adam Mosseri, Meta CEO Mark Zuckerberg and YouTube’s vice president of engineering, Cristos Goodrow, to testify before the jury. Mosseri argued in his defense of Instagram that although excessive use of the app could pose a problem, it’s “important to differentiate between clinical addiction and problematic use.” He added that while it is in Instagram’s business interests to attract as many users as possible, he believes “protecting minors in the long run is good for profit and business.” Zuckerberg echoed similar sentiments last week, saying Meta prioritizes building “a community that is sustainable” over boosting users’ screen time. “If you do something that’s not good for people, maybe they’ll spend more time [on Instagram] short term, but if they’re not happy with it, they’re not going to use it over time,” Zuckerberg said in his testimony. “I’m not trying to maximize the amount of time people spend every month.” Pressed about Meta’s age verification policies, Zuckerberg said that despite Instagram’s longtime policy prohibiting children under 13 from making accounts, he believes there are kids “who lie about their age in order to use the services.” Meta has developed measures over time to try to detect underage users, Zuckerberg said. But K.G.M.’s attorney, Mark Lanier, noted that the age verification features were not available when many children joined Instagram. K.G.M. got on the app at age 9. “I always wish we could have gotten there sooner,” Zuckerberg said of the safety tools Meta added in recent years. A spokesperson for Meta said in a statement that “the question for the jury in Los Angeles is whether Instagram was a substantial factor in the plaintiff’s mental health struggles. The evidence will show she faced many significant, difficult challenges well before she ever used social media.” K.G.M. was present during part of Zuckerberg’s testimony but did not speak. Goodrow, who took the stand Monday and Tuesday ahead of K.G.M.’s testimony, was pressed about YouTube’s self-proclaimed “big, hairy, audacious goal” of achieving 1 billion hours of daily watch time by the end of 2016. “YouTube is not designed to maximize time,” Goodrow said Monday. “Now we measure in time well spent.” YouTube’s CEO was scheduled to testify, as well, but was removed from the witness list last week. Lanier told NBC News that his team is “running out of time allowed by the court to put on our case.”</div>",
            "link": "https://www.nbcnews.com/tech/tech-news/social-media-addiction-trial-los-angeles-plaintiff-testimony-rcna260546",
            "pub_date": "2026-02-25 22:45:15",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        }
    ]
}