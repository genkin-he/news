{
    "data": [
        {
            "title": "AI giant Nvidia made $120 billion in profit last year. Investors are still spooked.",
            "description": "<div>Nvidia — the most valuable company in the world thanks to its place atop the AI food chain — generated a staggering $120 billion in profits last year. This includes an eye-popping $43 billion during the three-month period ending in January, one of the strongest quarters of any business ever recorded.  Investors didn’t seem to care.  Shares of Nvidia fell more than 5% Thursday, following the results. With a total market value of roughly $4.5 trillion, the company’s one-day loss amounted to roughly $256 billion worth of market capital.  Nvidia’s stock price decline is part of a broader phenomenon dubbed the  AI “scare trade,” that is percolating in certain corners of the stock market. This bearish play threatens the very driver that has powered broader, double-digit gains across the benchmark S&P 500 for the past two years.  And while the stock market might look broad, its gains are increasingly concentrated in just a handful of mega-cap names, including Nvidia. In other words, the entire market’s performance is heavily tied to the performance of these select companies.  Nvidia says its growth story is very much still intact.  “We have now seen the inflection of agentic AI and the usefulness of agents across the world and enterprises everywhere,” Nvidia CEO Jensen Huang said during the company’s quarterly earnings call Wednesday, referring to AI chatbots like OpenAI’s ChatGPT and Anthropic’s Claude.  “You’re seeing incredible compute demand because of it,” he said. “In this new world of AI, compute is revenues.”   Compute refers to the processing power that is needed to train and operate AI models. Nvidia’s chips, each of which is around 30 inches square, underpin the massive data centers needed to run AI chatbots and agents.  Nvidia’s dominance in the AI chip race also means that more companies than ever are dependent upon its products, at a time when AI is evolving faster than even its early adopters say they could have imagined. Over the past five years, this has sparked a massive investor rush to buy a piece of Nvidia, whose shares have surged nearly 1,300% since the start of 2021. Driven by a mixture of FOMO and faith in AI’s growth-at-any-cost business model, these investors and others like them have piled into virtually any company with even a tangential relationship to the AI industry.  All this time, Nvidia has led the charge. But so far this year, Nvidia’s share price is barely positive. Some firms, including HSBC, have argued that in order to justify another leg higher in the company’s stock price, Nvidia needs a “new narrative,” such as a meaningful expansion in AI demand or pricing power, to justify another leg higher in the stock. But more broadly, the AI scare trade visited upon Nvidia Thursday underscores a growing unease around the future of AI.  After a multi-year boom for public and private companies alike, AI is now facing tougher scrutiny. Questions persist around whether or not the AI boom is starting to look more like a bubble. Likewise, investors are uncertain whether AI can generate the kind of near-term returns necessary to justify the massive investments — and soaring share prices —  coursing through the tech world. “Artificial intelligence stands to become one of the most consequential technologies in generations, if not in the history of humankind, with enormous implications for the economy,” Moody’s economist Mark Zandi wrote in a new report Wednesday. “However, the specifics of how it will shape the future remain highly uncertain and are the subject of immense debate.” That debate includes growing concerns over how AI agents will affect vulnerable industries like cybersecurity and software — and potentially upend traditional business models that have worked for decades.  Shares of software companies like ServiceNow and Synopsis have fallen sharply amid those fears, declining roughly 20% and 15% over the past month, respectively. Salesforce is down nearly 25% this year. So far this year, companies in the software industry have been the largest drag on the S&P 500. AI has “started to call into question how exactly software companies are really going to compete and provide something superior in this environment,” Melissa Otto, head of Visible Alpha research at S&P Global, told NBC News. Nvidia’s Huang attempted to push back on this narrative in an interview with CNBC on Wednesday.  The “markets got it wrong” when it comes to the AI-driven panic around software, he said. Huang argued that AI will enhance productivity and expand what software can do, rather than kill the whole industry.  Huang’s attempt to assuage investor fears didn’t move the needle much, though. On Thursday, the tech-heavy Nasdaq fell nearly 1.5% on the back of Nvidia’s slide. Software giants like Synopsys dropped 5% while shares of Microsoft and Alphabet also traded lower.  Beyond software, investors are contending with other existential anxieties. Many of them were captured this week in an essay posted on Substack by a small research firm called Citrini Research. The post warned that AI adoption would lead to a stock market crash, a sharp pullback in consumer spending and widespread white-collar layoffs by 2028. The report painted a vivid picture of an economic doomsday scenario caused by AI, effectively animating investors’ vague, simmering fears. Payments giants like Mastercard and American Express were hit particularly hard after the post named them as potential casualties in a lower-spending, AI-disrupted economy. Shares of the two payment giants rebounded slightly on Thursday.  Many Wall Street analysts say it’s too soon to panic.  “While we take concerns about the AI trade and private markets and other matters seriously, we think it’s premature to assume that’s the kind of risk we face today,” Lori Calvasina, head of U.S. equity strategy research at RBC Capital Markets, wrote in a client note earlier this month.  Kristy Akullian, Blackrock’s head of iShares investment strategy for the Americas, added in a separate note Thursday that the recent sell-off “is predicated on still uncertain existential risk,” rather than any immediate changes to company earnings or business fundamentals.  Nonetheless, this existential risk is one that investors are taking more seriously now than they did six months ago. </div>",
            "link": "https://www.nbcnews.com/business/markets/nvidia-made-120-billion-profit-last-year-investors-are-worried-rcna260839",
            "pub_date": "2026-02-27 06:10:48",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "What the polls say about how Americans are using AI",
            "description": "<div>Americans are increasingly encountering and using artificial intelligence technologies like Anthropic’s Claude, OpenAI’s ChatGPT and Google’s Gemini. Most surveys now suggest more than two-thirds of the population is interacting with these technologies. However, a defining question is how far along we are in a transition from experimentation with AI to integration into daily personal and professional life. How many Americans are using it regularly? For what purpose? And what value are they extracting? One compelling way to track this adoption of AI is via Gallup polling, which found that 12% of Americans now report using AI daily at work. While the number seems modest, it represents a threefold increase in just over a year, from 4% in mid-2024. Employees in the knowledge-work sector are leading this change. In the technology industry, 31% of workers reported daily usage. That’s compared to 19% of finance industry workers and 16% of employees in the professional service industry. Gallup also identified the ability to work remotely as an indicator of AI adoption. Daily usage among remote-capable jobs sits at 19%, more than double the 7% daily adoption rate in roles requiring an on-site or physical presence. For historical perspective, Pew Research Center tracked internet adoption in the early aughts. In 2000, 14% went online daily for job-related research. In 2001, the number jumped up to 19%. While AI usage by white-collar workers is more pronounced than among blue-collar workers, pessimism regarding AI’s impact on the labor market cuts across the typical dividing line for social and cultural issues.  An Economist/YouGov poll found 63% of American adults thought that advances in AI would lead to an overall decrease in jobs. There was little difference in opinion based on education: 67% of those with college degrees and 61% of those without degrees shared this concern. This pessimism does appear to be greater than concerns about computers in the workplace at the turn of the century. In 1999, a NPR/Kaiser/Harvard Technology survey found that 32% thought computers would lead to a decrease in jobs, while 43% thought they would lead to an increase and 23% thought computers wouldn’t make a difference. That means today’s anxiety about AI is nearly twice as high as the computer anxiety of the late ’90s, as overall attitudes toward AI seem to resemble the mix of skepticism and curiosity seen during the early rise of the internet. As use of AI spreads, another question is how people are actually using it. OpenAI released a study last year that estimated 30% of ChatGPT usage was work-related and 70% was personal. The study categorized ChatGPT usage into three main behaviors. There was “asking” (49%), or prompting the AI for advice or information on a specific topic. “Doing” (40%) involved practical tasks such as drafting text or writing code. And “expressing” — chatting or playing — accounted for 11% of usage. Writing and seeking guidance are among the leading practical uses of ChatGPT, according to OpenAI. When specifically looking at the workplace, the study found technical help and writing become even more dominant uses. Still, a majority of respondents (53%) in a recent Fox News poll said AI had not yet made a significant difference in their lives, while 26% said it had personally helped them and 20% believed it had caused them harm. With daily AI use in the workplace tripling in a year, it's possible those numbers could shift even more rapidly in the months ahead, as the technology moves from the periphery and toward the center of American life.</div>",
            "link": "https://www.nbcnews.com/politics/politics-news/polls-say-americans-are-using-ai-more-work-personal-rcna260422",
            "pub_date": "2026-02-27 01:41:26",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Instagram will alert parents to teens' repeated suicidal or self-harm searches",
            "description": "<div>Instagram will begin notifying parents when their teenagers repeatedly attempt to search for suicide or self-harm content, the company said Thursday. The new feature will alert parents enrolled in the parental supervision tool to repeated searches in a short time period and offer expert resources about how to talk to their teens about the issue. The function will start to roll out in the coming weeks in Australia, Canada, the United Kingdom and the United States, with more countries to come later this year.  “The vast majority of teens do not try to search for suicide and self-harm content on Instagram, and when they do, our policy is to block these searches, instead directing them to resources and helplines that can offer support,” the company said in its announcement. “These alerts are designed to make sure parents are aware if their teen is repeatedly trying to search for this content, and to give them the resources they need to support their teen.” The move comes as Instagram’s parent company, Meta, and other social media platforms face ongoing scrutiny over the safety of their products, particularly for young people.  In Los Angeles, a consolidated group of cases with more than 1,600 plaintiffs, including more than 350 families and over 250 school districts, are accusing Instagram, YouTube, TikTok and Snap of deliberately designing platforms to be addictive to young users. Last week, Meta CEO Mark Zuckerberg said in court that Instagram is meant to build “a community that is sustainable” and not designed to addict young users. TikTok and Snap settled ahead of the trial. Meta, and in particular Instagram, have taken some steps to address concerns around the use of its platforms by teens. In 2024, Instagram introduced accounts specifically for teens meant to restrict who can contact them. In October, the company said it would overhaul its approach to teens' accounts, limiting their access to certain content in an attempt to make the experience closer to viewing PG-13 movies.  Instagram already blocks content related to suicide or self-harm from reaching teens’ accounts. However, families of teens who died by suicide allege in their lawsuits that Instagram is responsible for multiple sextortion scams targeting teens, NBC News previously reported. Meta spokesperson Sophie Vogel told NBC News that teens can also talk to Instagram’s existing artificial intelligence tool later this year to seek support, and parents will also be notified of conversations related to suicide or self-harm. If you or someone you know is in crisis, call or text 988 or go to 988lifeline.org to reach the Suicide & Crisis Lifeline. You can also call the network, previously known as the National Suicide Prevention Lifeline, at 800-273-8255 or visit SpeakingOfSuicide.com/resources.</div>",
            "link": "https://www.nbcnews.com/tech/social-media/instagram-will-alert-parents-teens-repeated-suicidal-self-harm-searche-rcna260789",
            "pub_date": "2026-02-27 00:50:55",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Florida Gov. Ron DeSantis leans into AI skepticism, seeking a contrast with Vance",
            "description": "<div>Florida Gov. Ron DeSantis isn’t sold on the massive expansion of AI.  And that belief might be his way back to national political relevance. The Republican governor is appealing to a growing number of people who have concerns that AI’s rapid build-up, fueled in part by taxpayer dollars, could displace jobs, increase energy costs and hurt the environment. DeSantis’ positions stand in direct contrast to the embrace of the AI industry by President Donald Trump and the two likeliest potential candidates to snag his 2028 presidential endorsement: Vice President JD Vance and Secretary of State Marco Rubio. “We don’t want to see them building a massive data center and then sending you the bill,” DeSantis said this month when asked about AI companies. “Data centers take up the power equivalent of a half a million-person city. We feel very, very strongly about protecting the consumer.” For DeSantis, the embrace of AI skepticism is rooted in both personal policy preference and a 2028-focused political calculation as the term-limited governor plots out his political future, according to eight sources, most of whom have either worked in his administration or for his past campaigns at both the state and national levels. Many of them requested anonymity to speak candidly. “It’s kind of a no-brainer, right? You’ve got JD Vance and Marco Rubio, the top two contenders for 2028 big time in the pro-AI lane,” a longtime DeSantis adviser said. “The infrastructure is lining up behind JD and to some extent Marco. So, DeSantis’ challenge is to stay relevant.” Taryn Fenske, a DeSantis political aide, said the governor is a skeptic because of AI’s potential societal dangers. “The governor is an AI skeptic because chatbots are convincing children to commit suicide,” she said.  DeSantis also leads a state the AI industry is likely to target, making it a focus of the broader fight between AI skeptics and proponents.  NBC News reported this month that Leading the Future, a pro-AI super PAC, is spending $5 million on TV ads boosting Republican Rep. Byron Donalds' Florida gubernatorial campaign. Donalds is the only state-level politician the group has spent money to help, and officials there said it's an indication AI companies will look to expand their footprint there in the future. Recent polling suggests that AI could be a pertinent issue in the upcoming midterm elections and the 2028 presidential race. A poll conducted this month by The Economist and YouGov found that 63% of the U.S. citizens surveyed — including 60% who voted for Trump in 2024 — believe that advances in AI will reduce the number of jobs available in the country. A plurality of respondents, 33%, said that AI would have a “more negative than positive” impact on the U.S. economy. And a Morning Consult poll in November found that a 41% plurality of registered voters favored banning the construction of data centers near their homes; 36% opposed such a ban, while 22% said they didn’t know or had no opinion. DeSantis and the Trump administration have already been at odds on the issue.  Trump’s AI czar David Sacks and other administration allies have directly lobbied against DeSantis’ push to get Florida’s GOP-dominated Legislature to implement state-level regulations on AI and the massive data centers needed to accommodate the industry’s boom. Those bills remain alive, but as Florida’s legislative session comes to a close, their passage has become increasingly unlikely. “There are some people … who almost relish in the fact that they think this just displaces human beings and then, ultimately, you’re going to have AI run society, and that you’re not going to be able to control it,” DeSantis said at an AI roundtable earlier this month. “Count me out on that.” DeSantis and Trump themselves have publicly buried the hatchet, including golfing together this month, after a brutal 2024 GOP presidential primary. But as the jockeying begins to become the first post-Trump Republican nominee for president, it does not mean the notoriously politically sharp-elbowed DeSantis will not look to use the fight over AI regulation as a political cudgel against Trump allies such as Vance and Rubio. “You know the story of the scorpion and the frog? Ron DeSantis is looking for his moment to stab the White House on something, and that might very well be AI,” said a longtime DeSantis political adviser who currently represents AI industry clients.  “And you know why he’s going to do it?” the person added. “Because he’s Ron DeSantis. It’s what he does.” During his State of the Union address Tuesday night, Trump said his administration had struck a deal with major tech companies to require them to pay for more of the energy costs associated with building massive data centers.  “We’re telling the major tech companies that they have the obligation to provide for their own power needs — they can build their own power plants as part of their factory, so that no one’s prices will go up — and in many cases, prices of electricity will go down for the community,” the president said. For now, DeSantis’ AI strategy would likely to some degree center on Vance, who in most public polling has been the overwhelming leader for the 2028 Republican presidential nomination, even as Rubio has picked up momentum in recent weeks. Vance, with his background in Silicon Valley venture capital and relationships with leading Big Tech figures, is known as one of the Republican Party’s biggest AI champions.  At last year’s Artificial Intelligence Action Summit in Paris, the vice president warned that excessive regulation “could kill a transformative industry” and pledged the Trump  administration’s support for “pro-growth AI policies.”  At the same time, Vance has attempted to reassure those on both sides of the debate.  Last fall, in an interview with Fox News’ Sean Hannity, Vance compared a rise in AI-related jobs to the arrival decades ago of cash-dispensing ATMs. Human bank tellers, Vance noted, still exist. He also linked his pro-AI stance to his anti-immigration views. Using home construction as an example, Vance described robots as complementary to “blue-collar” workers and immigrant laborers as an outright threat to replace them on job sites. “No robot can replace a great blue-collar construction worker,” Vance said. “You see some of the houses, some of the things they do, the trim that they’re able to do. There’s an art there that I don’t think a robot is ever going to be able to replace.” Vance has more recently expressed concerns about AI, telling Fox News’ Martha MacCallum last week that he worries specifically about it being used to surveil Americans or to advance invasions of privacy and political bias. In that same interview, Vance also illustrated the differences between him and DeSantis when it comes to regulating AI. DeSantis has pushed for state-level regulation, while Vance, along with the Trump administration more broadly, has supported the industry-backed idea of Congress passing a national regulatory bill. “I think that eventually you’re going to have some standard applied, whether it’s a federal standard or whether it’s one state standard dominating,” Vance told MacCallum when pressed for his thoughts on regulation. “I think, frankly, the worst possible outcome would be to have far left California dominate the entire AI regulatory map.” Representatives for Vance did not return requests for comment. An official close to the DeSantis administration said that the governor sees an opportunity with the growing number of average people who feel elbowed out and negatively impacted by AI’s growth. “Look at all these many trillions of dollars being spent on AI and data centers. They have no clue how it will ever benefit them,” the person said. “This stuff is more for enterprises than the individual person, and they see it as something that will jack up costs and replace them.” “For DeSantis’, it’s a populist play,” the person added. “And that’s perfect for him.” The former official  said there are, to some extent, broader geopolitical concerns to consider as the U.S. engages in an “arms race” with China over AI expansion — and the almost inevitable change to the modern world that fight will usher in.  “The reality is this is an arms race, this is a cold war arms race against China dumping tons of money into AI,” the person said. “If you are on Team Trump, the only way you dig out of debt right now is to radically enhance productivity and grow production.” “International investors I talk to are perplexed on how anyone in the U.S. could be anti-AI right now,” the person said. “That just makes things easier for China.” Across the country, people are heavily pushing back on efforts to put massive data centers in their cities and towns. Most national polling is favorable to building data centers, but the numbers crater when people are asked if they would like one in their backyard. “Nationally, when you talk about data centers, it polls at roughly 60%,” said an adviser who works with pro-AI groups. “But when you say to people in Loudoun County, if they want one in Loudoun County, the numbers are really, really bad,” the person said, referring to the Virginia county. “Nationally it all sounds good and gravy,” the adviser added. “But when you get to the local stuff, you’re f-----.”</div>",
            "link": "https://www.nbcnews.com/politics/2028-election/florida-gov-ron-desantis-ai-skepticism-contrast-vance-rcna258824",
            "pub_date": "2026-02-26 18:05:58",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Anthropic says U.S. military can use its AI systems for missile defense ",
            "description": "<div>In contract negotiations between senior Defense Department officials and leaders from AI giant Anthropic in December, the company agreed to allow the U.S. government to use its AI systems for missile and cyber defense purposes, a person familiar with the matter said, requesting anonymity to speak about private discussions. But that apparently did not satisfy the Pentagon. Following weeks of tension between the Defense Department and Anthropic over the company’s restrictions on how its products can be used by the military, Defense Secretary Pete Hegseth issued a stark ultimatum to company CEO Dario Amodei on Tuesday: Allow the AI technology to be used for all legal military purposes by this Friday or be forced to cooperate, a senior Pentagon official told NBC News. The ultimatum, detailed to NBC News by a senior Pentagon official, comes as Anthropic — a company that has heavily marketed its focus on AI safety — tries to maintain firm policies preventing its systems from being used for mass domestic surveillance or direct use in lethal autonomous weapons. The December contract changes would allow for its systems to be widely used for cyber and missile defense, according to the person familiar with the matter. An Anthropic spokesperson told NBC News in a statement that “Every iteration of our proposed contract language would enable our models to support missile defense and similar uses.”  But the company’s insistence on guardrails have continued to be a source of contention between Anthropic and the Defense Department. According to the senior Pentagon official, representatives from the department, including Undersecretary of Defense Emil Michael, recently discussed several hypothetical scenarios with Anthropic leadership about how the company’s products might be employed by the military.  As part of those discussions, the officials discussed how Anthropic’s systems might be used if an adversary launched an intercontinental ballistic missile at the U.S. According to the Pentagon source, the officials discussed whether Anthropic’s guardrails might somehow block a U.S. response to the launch. Anthropic officials said they could be called on to lift those restrictions, according to the official, but Pentagon leadership was not fully satisfied with Anthropic’s adjustments and did not want to be beholden to the private company.  According to an Anthropic spokesperson, any suggestion that CEO Amodei said the Pentagon would have to call the company in each missile defense operation is “patently false.”  In the latest escalation in negotiations, during Tuesday’s meeting Pentagon leaders said they could invoke the Defense Production Act to force Anthropic to comply with the Pentagon's rules, according to the senior Pentagon official. The Act allows the president to control domestic companies critical to national security in times of need. In Tuesday’s meeting, Pentagon leadership also invoked threats to instead label Anthropic as a “supply chain risk” and ban all defense business with the company if it does not align its terms of service for certain high-stakes uses with the Pentagon by Friday, the source said.  “Anthropic has until 5:01pm Friday to get on board with the Department of War,” the senior Pentagon official said of the ultimatum in a statement provided to NBC News, responding to questions about the meeting. “If they don’t get on board, the Secretary of War will ensure the Defense Production Act is invoked on Anthropic, compelling them to be used by the Pentagon.” “Additionally, the Secretary of War will also label Anthropic a supply chain risk,” the official said. Asked about Tuesday’s meeting, an Anthropic spokesperson said in a statement: “Dario expressed appreciation for the Department’s work and thanked the Secretary for his service. We continued good-faith conversations about our usage policy to ensure Anthropic can continue to support the government’s national security mission in line with what our models can reliably and responsibly do.” Hegseth complimented Anthropic’s products and said the Pentagon wanted to work with Anthropic, according to another person familiar with the meeting, who requested anonymity to speak candidly. The person confirmed that the department said it would terminate Anthropic’s work with the Pentagon by Friday if it did not agree to its terms. According to reports from The Wall Street Journal and Axios, Anthropic’s Claude systems were used during the operation to capture Venezuelan President Nicolás Maduro in January. It is unclear exactly how the systems were used. Hegseth sent a memo to senior Pentagon officers Jan. 9 announcing the Pentagon’s drive toward an “AI-first warfighting force.” He outlined a push to use AI models, like Anthropic’s, for all legitimate military purposes, “free from usage policy constraints” set by individual AI companies.  Anthropic is the only AI company whose products are actively used on classified networks, through its contract with Palantir, a data analytics company. A senior Pentagon official confirmed to NBC News that xAI reached a deal with the Pentagon on Monday to use its Grok chatbot system on classified networks, agreeing to allow its systems to be harnessed for “any lawful use” as Hegseth desired. Anthropic was one of four AI companies — the others were OpenAI, Google DeepMind and xAI — to get contracts worth up to $200 million in July to “prototype frontier AI capabilities that advance U.S. national security.”</div>",
            "link": "https://www.nbcnews.com/tech/security/anthropic-pentagon-us-military-can-use-ai-missile-defense-hegseth-rcna260534",
            "pub_date": "2026-02-26 02:00:58",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Discord pushes back age verification rollout following backlash",
            "description": "<div>Discord, the popular online communication platform for gamers, said Wednesday it will delay its global age verification rollout after receiving user criticism. Earlier this month, Discord announced a phased rollout for new and existing users that would implement video selfies to determine a person's age group. Users could also submit a form of identification to their vendor partners.  The rollout was supposed to begin in early March and would give underage users a \"teen-appropriate experience\" that included updated communication settings, content filtering and restricted access to age-gated spaces.  Criticism was immediate, with many users pointing to an October security breach of a third-party provider Discord used that exposed government ID photos for thousands of users, The Associated Press reported.  The platform’s chief technology officer responded to the backlash in an update Tuesday, saying that Discord \"missed the mark\" and that the rollout is being delayed to the second half of 2026.  \"Let me be upfront: we knew this rollout was going to be controversial. Any time you introduce something that touches identity and verification, people are going to have strong feelings. Rightfully so. In hindsight, we should have provided more detail about our intentions and how the process works,\" Discord CTO Stanislav Vishnevskiy wrote in a Wednesday blog post.  Vishnevskiy said the platform will not require face scans or ID uploads from everyone, and that over 90% of users will never need to verify their age to continue using it.  \"If you’re among the less than 10% of users who do need to verify, we’ll give you options, designed to tell us only your age and never your identity,\" Vishnevskiy said.  If a user chooses not to verify their age, they can keep their account, servers, friends list, messages, and voice chat, but won't be able to access age-restricted content or change certain safety settings.   Vishnevskiy also addressed last year's security breach, saying that Discord no longer works with that vendor.  \"We’ve made mistakes. I won’t pretend we haven’t. And I know that being a bigger company now means our mistakes have bigger consequences and erode trust faster. I don’t expect one blog post to fix that,\" Vishnevskiy said.  \"We’re listening. We’ll get this right. And when we ship, you’ll be able to see for yourselves,\" he added.</div>",
            "link": "https://www.nbcnews.com/tech/tech-news/discord-pushes-back-age-verification-rollout-backlash-rcna260604",
            "pub_date": "2026-02-25 23:16:32",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Plaintiff set to testify against tech companies in landmark social media addiction trial",
            "description": "<div>LOS ANGELES — The first plaintiff to take major social media companies to trial is expected to take the stand Wednesday to speak about the harms she says the tech platforms inflicted on her mental health as a child. The plaintiff, who is now 20 and is identified in court by her initials, K.G.M., is at the center of a bellwether case in Los Angeles County Superior Court that could set a legal precedent about whether social media platforms are responsible for causing mental health issues in children. The trial is the first in a consolidated group of cases brought against Instagram, YouTube, TikTok and Snap by more than 1,600 plaintiffs, including over 350 families and over 250 school districts. The plaintiffs accuse the tech companies of knowingly designing addictive products harmful to young users’ mental health. K.G.M., who was a minor at the time of the incidents outlined in her lawsuit, claims that her early use of social media led to addiction and worsened her mental health problems. Her lawsuit alleges that social media companies made deliberate design choices to make their platforms more addictive to children for purposes of profit. “Defendants know children are in a developmental stage that leaves them particularly vulnerable to the addictive effects of these features,” her lawsuit says. “Defendants target them anyway, in pursuit of additional profit.” Historically, social media platforms have largely been shielded by Section 230, a provision added to the Communications Act of 1934 that says internet companies are not liable for content users post. TikTok and Snap reached settlements with K.G.M. before the trial, but they remain defendants in a series of similar lawsuits expected to go to trial this year. The lawsuit highlights a variety of features that it claims the platforms use to “exploit children and adolescents,” including “an algorithmically-generated, endless feed to keep users scrolling,” rewards that encourage people to keep using the platform, and “incessant” notifications, as well as “inadequate” measures for age verification and parental control. “Disconnected ‘Likes’ have replaced the intimacy of adolescent friendships. Mindless scrolling has displaced the creativity of play and sport,” the lawsuit says. “While presented as ‘social,’ [the platforms] have in myriad ways promoted disconnection, disassociation, and a legion of resulting mental and physical harms.” So far during the trial, K.G.M.’s lawyers have called on Instagram head Adam Mosseri, Meta CEO Mark Zuckerberg and YouTube’s vice president of engineering, Cristos Goodrow, to testify before the jury. Mosseri argued in his defense of Instagram that although excessive use of the app could pose a problem, it’s “important to differentiate between clinical addiction and problematic use.” He added that while it is in Instagram’s business interests to attract as many users as possible, he believes “protecting minors in the long run is good for profit and business.” Zuckerberg echoed similar sentiments last week, saying Meta prioritizes building “a community that is sustainable” over boosting users’ screen time. “If you do something that’s not good for people, maybe they’ll spend more time [on Instagram] short term, but if they’re not happy with it, they’re not going to use it over time,” Zuckerberg said in his testimony. “I’m not trying to maximize the amount of time people spend every month.” Pressed about Meta’s age verification policies, Zuckerberg said that despite Instagram’s longtime policy prohibiting children under 13 from making accounts, he believes there are kids “who lie about their age in order to use the services.” Meta has developed measures over time to try to detect underage users, Zuckerberg said. But K.G.M.’s attorney, Mark Lanier, noted that the age verification features were not available when many children joined Instagram. K.G.M. got on the app at age 9. “I always wish we could have gotten there sooner,” Zuckerberg said of the safety tools Meta added in recent years. A spokesperson for Meta said in a statement that “the question for the jury in Los Angeles is whether Instagram was a substantial factor in the plaintiff’s mental health struggles. The evidence will show she faced many significant, difficult challenges well before she ever used social media.” K.G.M. was present during part of Zuckerberg’s testimony but did not speak. Goodrow, who took the stand Monday and Tuesday ahead of K.G.M.’s testimony, was pressed about YouTube’s self-proclaimed “big, hairy, audacious goal” of achieving 1 billion hours of daily watch time by the end of 2016. “YouTube is not designed to maximize time,” Goodrow said Monday. “Now we measure in time well spent.” YouTube’s CEO was scheduled to testify, as well, but was removed from the witness list last week. Lanier told NBC News that his team is “running out of time allowed by the court to put on our case.”</div>",
            "link": "https://www.nbcnews.com/tech/tech-news/social-media-addiction-trial-los-angeles-plaintiff-testimony-rcna260546",
            "pub_date": "2026-02-25 22:45:15",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "AI-linked fears roil some corners of Wall Street after years of hype and gains",
            "description": "<div>Stocks surged to records in large part because of hope — and hype — about artificial intelligence.  But in recent months, worries about aggressive spending on AI have rippled through Wall Street as investors question whether that spending will materialize into actual profits. And some industries wavered this week as anxieties about the technology intensified, underscoring how quickly sentiment has shifted since the start of the year.  Visa, Mastercard and IBM all fell sharply Monday, extending a broader bout of volatility across AI-linked names. Tuesday saw a modest bounce back across the markets as some software stocks also rebounded, thanks to new AI integrations announced by Anthropic. The benchmark S&P 500 index remains roughly flat for the year. Since November, AI-powered coding systems such as Anthropic’s Claude Code and OpenAI’s Codex have surged in capabilities and popularity among software developers. Using these tools, complex software packages and products can now be developed in minutes or days.  The most recent sell-off came after a grim and now-viral weekend substack post by Citrini Research warned of an eventual stock market crash, a sharp pullback in consumer spending and widespread white-collar layoffs by 2028 as a result of AI.  Payment companies such as Mastercard and American Express were hit particularly hard as traders contemplated a future with lower spending. Both companies were mentioned as potential victims of the AI rush in the Citrini post.  While Citrini has published analysis about AI for years, Sunday’s post stood out for its intricate, dramatic description of a world with widespread unemployment and reduced economic activity. On Monday, IBM suffered its worst, single-day drop since October 2000, during the height of the dot-com boom and bust. Part of the pressure followed Anthropic’s midday announcement that its Claude Code tool can be used to update a legacy computing language prized at IBM. Investors viewed the development as a potential threat to the kind of maintenance and modernization work that underpins IBM’s legacy business. Shares of Accenture and Cognizant Technology, two other consulting and professional service companies, fell in tandem.  The weakness in these individual stocks spilled over into the broader market. All three major indexes fell Monday, led by a more than 800 point drop in the Dow Jones Industrial Average. Five of the 11 S&P 500 sectors also closed in the red with financials and consumer discretionary leading the declines, down 3.3% and 2.2%, respectively.  It’s an irony not lost on Wall Street: The same force that’s fueled the tech sector’s explosive gains over the past two years is now driving investor hesitation. Mona Mahajan, head of investment strategy and asset allocation at Edward Jones, said investors have been quickly pulling money out of sectors viewed as vulnerable to AI disruption, including financial services, real estate, transportation and logistics. She noted that despite the sharp moves in stock prices, the underlying businesses themselves haven’t materially changed, underscoring how much of the reaction remains speculative. One standout in Monday’s sell-off was the consumer staples sector, home to companies such as Walmart and Coca-Cola, as investors put their money into stalwart businesses less likely to be disrupted by AI. While Anthropic and OpenAI have seen their valuations soar in the past year, investors covering industries ranging from logistics and trucking to legal services are waking up to the fact that traditional barriers to entry for competitors might soon be dissolved by AI’s coding power.  Melissa Otto, head of Visible Alpha research at S&P Global, told NBC News that growing AI-focused capital expenditure and recent advances in the field threatened many existing business models.  “I think it has started to call into question how exactly software companies are really going to compete and provide something superior in this environment,” Otto said, highlighting that companies with more complex workflows and troves of unique data stand a better chance of withstanding the coming AI storm. “Data that underlies generative AI that has predictive qualities and is proprietary, that can potentially really give firms an edge.” Beyond AI, markets are also contending with growing fears of war between the U.S. and Iran and the legal whiplash around President Donald Trump’s now-invalidated emergency tariffs. Still, some Wall Street analysts argue the latest trade developments could serve as a constructive distraction for U.S. markets in the near term. “Investors have been ruminating on a number of AI concerns in early 2026, with concerns about cash flow, capex levels, and whether a number of different industries will survive the AI era,” Lori Calvasina, head of U.S. equity strategy research at RBC Capital Markets, wrote in a note to clients Monday. “We generally have thought the existential concerns for industries other than software like wealth management and transportation/logistics have seemed overblown, and we think it would be healthy for the equity community to turn its attention to other topics for a while.” </div>",
            "link": "https://www.nbcnews.com/business/markets/ai-fears-stock-market-hype-gains-rcna260408",
            "pub_date": "2026-02-25 04:56:34",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Chinese AI companies 'distilled' Claude to improve own models, Anthropic says",
            "description": "<div>Three Chinese artificial intelligence companies used Claude to improperly obtain capabilities to improve their own models, the chatbot’s creator Anthropic said in a blog post Monday while also making a case for export controls on chips. The announcement follows a memo by OpenAI earlier this month, when the startup warned U.S. lawmakers that Chinese AI firm DeepSeek is targeting the ChatGPT maker and the nation’s leading AI companies to replicate models and use them for its own training. DeepSeek, Moonshot and MiniMax created more than 16 million interactions with Claude using roughly 24,000 fake accounts, in violation of Anthropic’s terms of service and regional access restrictions, the company said.  They used a technique called “distillation,” which involves training a less capable model on the outputs of a stronger one, Anthropic said. “These campaigns are growing in intensity and sophistication. The window to act is narrow, and the threat extends beyond any single company or region.” Anthropic warned that illicitly distilled models lacked necessary safeguards, creating significant national security risks. If these models are open-sourced, the risk multiplies as capabilities spread freely beyond any single government’s control. Anthropic, which raised $30 billion in its latest funding round and is now valued at $380 billion, said that distillation attacks support the case for export controls: Chip access restrictions reduce both direct model training capabilities and the extent of improper distillation. DeepSeek’s operation targeted reasoning capabilities across diverse tasks and the creation of censorship-safe alternatives to policy-sensitive queries, while Moonshot aimed at agentic reasoning and tool use, as well as coding and data analysis, Anthropic said. MiniMax targeted agentic coding, tool use and orchestration and Anthropic detected the campaign while it was still active — before MiniMax released the model it was training. “When we released a new model during MiniMax’s active campaign, they pivoted within 24 hours, redirecting nearly half their traffic to capture capabilities from our latest system,” the blog post said. DeepSeek, Moonshot and MiniMax did not immediately respond to requests for comment.</div>",
            "link": "https://www.nbcnews.com/world/asia/chinese-ai-companies-distilled-claude-improve-models-anthropic-says-rcna260386",
            "pub_date": "2026-02-24 18:36:51",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        },
        {
            "title": "Facebook designed an app for teens called Bell but never launched it, court records reveal",
            "description": "<div>In 2018, as Facebook sought to expand its global footprint, the company considered launching a separate app for teens called Bell, which would have been built around their high schools, offering forums where students could discuss sports teams, school events or what they overheard in the hallway, a new court filing shows.   The company intended for Bell to become a central hub for teens within high schools across the United States and eventually across the world, where they could communicate with their classmates but not anyone outside their school. The strategy was to draw teens into the company’s ecosystem and then move them onto the regular Facebook platform once they graduated, according to a partially redacted April 2018 internal presentation, which was filed in federal court last week.  “High School communication is important to teens and important for us to win,” the presentation stated. Although the Bell app never launched, the internal plans demonstrate the importance that Facebook had placed on “winning” users before they turned 18, laying groundwork to keep them on the products over the long term. Do you have a story to share about technology in education? Contact reporter Tyler Kingkade A spokesperson for Meta, Facebook’s parent company, said the app was developed as an early exploratory idea, and it would have relied heavily on Facebook moderation teams to police the content. The spokesperson did not respond to a question about why the app never launched.  Plans for Bell were included among a large batch of exhibits filed late Friday by the plaintiffs as part of a sprawling lawsuit against the largest social media companies, including Meta. Hundreds of individual families, school districts and 33 state attorneys general accuse Meta, Google, ByteDance and Snap of designing addictive social media products and promoting them to minors, despite knowing about research showing harm to children’s mental health. “The social media addiction trials are providing a look behind the curtain and are proving that the status quo was even worse than we imagined,” said Sacha Haworth, executive director of the Tech Oversight Project, a nonprofit advocacy group pushing for more regulation of digital technology companies. “We simply have to do more to protect kids.” Meta and the other companies have broadly argued that there is no conclusively established link between social media use and mental health problems, and the platforms did not have a duty to warn the public about potential dangers. “We strongly disagree with these allegations and are confident the evidence will show our longstanding commitment to supporting young people,” Meta said in a statement. “For over a decade, we’ve listened to parents, worked with experts and law enforcement, and conducted in-depth research to understand the issues that matter most.”  Meta CEO Mark Zuckerberg argued last week in a Los Angeles courtroom that people stay on the company’s platforms because they find them helpful in communicating with peers. Meta has developed better age detection systems over the years, the company says, in an attempt to stop children under 13 from accessing its platforms. A Meta spokesperson pointed to features on its new Teen Accounts that are intended to give parents more control over their children’s social media use, and encourage young users to take breaks and pause notifications overnight. Meta has considered launching platforms aimed at children over the past decade. It paused plans to create a version of Instagram for children under age 13 in 2021 following pushback from parent safety groups. It also considered building a version of Facebook for children in 2017, but decided against it after parents reportedly provided negative feedback.  Facebook’s 2018 presentation on Bell shows how teen users would have been able to message anyone in their school, organize events on the platform and create class or club-based group chats similar to the apps Discord and Slack. Students also would have been able to post anonymous confessions, similar to the app YikYak, and Bell would have integrated with education technology products, such as Google Classroom. Once teens graduated from high school, according to the internal presentation, Bell would have provided “a smooth on-ramp” for them to import their information to Facebook. The data that Bell collected on students would later influence what showed up in their Facebook feeds.  The internal presentation cited surveys of high school students who identified their “must haves” in a social media app: communicating with classmates, watching videos and memes created by students and staying in the loop on happenings at their high school. Some of this already occurred on Facebook Groups and Messenger, and on Snapchat, but the company saw an opening to create a single app that “gathers everyone at school into one closed campus.” The company hoped that the Bell app would reach 80% of U.S. high schools by the end of 2020 and expand to Australia, Canada and European countries. Late last year, Australia enacted a ban on children under 16 from using social media.</div>",
            "link": "https://www.nbcnews.com/tech/social-media/facebook-designed-app-teens-bell-court-records-reveal-rcna260315",
            "pub_date": "2026-02-24 09:11:16",
            "source": "nbcnews",
            "kind": 1,
            "language": "en"
        }
    ]
}